{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep2eK29S_5qq"
   },
   "source": [
    "\n",
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90wz8D6UuSs9",
    "outputId": "dd983309-3b41-4363-eec7-483d11cfc237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.13.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.5.3\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JlLkEWbmiyYj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "THXTiqZQmCO0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m M_star \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubhalo/SubhaloMassType\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1e10\u001b[39m\n\u001b[1;32m      2\u001b[0m pos  \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubhalo/SubhaloPos\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e3\u001b[39m\n\u001b[1;32m      3\u001b[0m vel \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubhalo/SubhaloVel\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "M_star = f['Subhalo/SubhaloMassType'][:,4]*1e10\n",
    "pos  = f['Subhalo/SubhaloPos'][:]/1e3\n",
    "vel = f['Subhalo/SubhaloVel'][:]\n",
    "met = f['Subhalo/SubhaloStarMetallicity'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dgJ7wxu6sLUM"
   },
   "outputs": [],
   "source": [
    "def load_and_filter_data(file, mass_threshold=2e8):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        positions = f['Subhalo/SubhaloPos'][:]/1e3  # Convert to Mpc/h\n",
    "        vel = f['Subhalo/SubhaloVel'][:]\n",
    "        metallicities = f['Subhalo/SubhaloStarMetallicity'][:]\n",
    "        masses = f['Subhalo/SubhaloMassType'][:,4]*1e10  # Stellar mass\n",
    "        radii = f['Subhalo/SubhaloHalfmassRadType'][:,4]/1e3\n",
    "        omega_m = f['Header'].attrs['Omega0']\n",
    "    \n",
    "    # Filter galaxies based on the stellar mass threshold\n",
    "    mask = masses > mass_threshold\n",
    "    positions = positions[mask]\n",
    "    vel = vel[mask]\n",
    "    metallicities = metallicities[mask]\n",
    "    masses = masses[mask]\n",
    "    radii = radii[mask]\n",
    "    \n",
    "    return positions, vel, metallicities, masses, radii, omega_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "--oaAn3kDLtU"
   },
   "outputs": [],
   "source": [
    "def apply_periodic_boundary_conditions(positions, box_size):\n",
    "    # Wrap positions to the box size\n",
    "    positions = positions % box_size\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B7VnGquYDPRe"
   },
   "outputs": [],
   "source": [
    "def minimum_image_distance(pos1, pos2, box_size):\n",
    "    # Calculate the minimum image distance between two points\n",
    "    delta = np.abs(pos1 - pos2)\n",
    "    delta = np.where(delta > 0.5 * box_size, box_size - delta, delta)\n",
    "    return np.sqrt((delta ** 2).sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lKOyScsvstrg"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "def distance(point1, point2):\n",
    "    return np.linalg.norm(point1 - point2)\n",
    "\n",
    "def create_edges_knn(points, k=6):\n",
    "    edges = []\n",
    "    edge_value = []\n",
    "\n",
    "    # Create a KDTree for efficient nearest neighbor search\n",
    "    point_tree = KDTree(points)\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        # Query the k nearest neighbors for each point\n",
    "        _, neighbors = point_tree.query(points[i], k=k+1)\n",
    "\n",
    "        for j in neighbors[1:]:  # Skip the first neighbor because it's the point itself\n",
    "            # Add an edge between the point and its neighbor\n",
    "            edges.append([i, j])\n",
    "\n",
    "            # Compute the distance between the points as the edge value\n",
    "            edge_value.append(distance(points[i], points[j]))\n",
    "\n",
    "    return [edges, edge_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ztRfbIqROn7N"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def min_distance(positions, box_size = 25):\n",
    "    min_distance = np.inf\n",
    "    max_distance = 0\n",
    "\n",
    "    # Iterate over all pairs of galaxies\n",
    "    for i in range(len(positions)):\n",
    "        for j in range(i + 1, len(positions)):\n",
    "            dist = minimum_image_distance(positions[i], positions[j], box_size)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Minimum distance: {min_distance} Mpc/h\")\n",
    "    print(f\"Maximum distance: {max_distance} Mpc/h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l40oy2_KRiWJ"
   },
   "outputs": [],
   "source": [
    "def minimum_image_distance_vectorized(positions, box_size = 25):\n",
    "    num_galaxies = positions.shape[0]\n",
    "\n",
    "    # Compute pairwise differences in each dimension\n",
    "    diff = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]\n",
    "\n",
    "    # Apply periodic boundary conditions\n",
    "    diff = np.abs(diff)\n",
    "    diff = np.where(diff > 0.5 * box_size, box_size - diff, diff)\n",
    "\n",
    "    # Compute the Euclidean distance\n",
    "    dist = np.sqrt(np.sum(diff ** 2, axis=-1))\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zPQB3MDzDcid"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.spatial import cKDTree\n",
    "def create_edges_knn_pbc(points, box_size = 25, k=6):\n",
    "    tree = KDTree(points, boxsize=box_size)\n",
    "\n",
    "    edges = []\n",
    "    edge_values = []\n",
    "    '''\n",
    "    distances = minimum_image_distance_vectorized(points, box_size)\n",
    "    # Mask the diagonal (self-distances which are zero)\n",
    "    np.fill_diagonal(distances, np.inf)\n",
    "    # Get the minimum and maximum distances\n",
    "    min_distance = np.min(distances)\n",
    "    max_distance = np.max(np.triu(distances, k=1))\n",
    "    print(min_distance, max_distance)\n",
    "    '''\n",
    "\n",
    "    min_distance = np.inf\n",
    "    max_distance = 0\n",
    "    large_distance_count = 0\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        distances, neighbors = tree.query(points[i], k=k+1)\n",
    "        for j, tree_dist in zip(neighbors[1:], distances[1:]):\n",
    "            if j != i and j < len(points):\n",
    "                actual_distance = minimum_image_distance(points[i], points[j], box_size)\n",
    "                edges.append([i, j])\n",
    "                edge_values.append(actual_distance)\n",
    "                min_distance = min(min_distance, actual_distance)\n",
    "                max_distance = max(max_distance, actual_distance)\n",
    "    return np.array(edges), np.array(edge_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m5XiGu3wtkRA"
   },
   "outputs": [],
   "source": [
    "def create_points(positions, masses, vel, met, radii):\n",
    "    point_features = []\n",
    "    for i, pos in enumerate(positions):\n",
    "        point_features.append(list(pos) + [masses[i]] + [met[i]] + [radii[i]])\n",
    "    return point_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3CE9AtwhuAKs"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "def create_graph(file_path, k_val=6):\n",
    "    positions, velocity, metallicities, masses, radii, omega_m = load_and_filter_data(file_path)\n",
    "    edges, edge_values = create_edges_knn_pbc(positions, 25, k_val)\n",
    "    point_values = create_points(positions, masses, velocity, metallicities, radii)\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    point_values = torch.tensor(point_values, dtype=torch.float)\n",
    "    edge_value = torch.tensor(edge_values, dtype=torch.float)\n",
    "\n",
    "    return [point_values, edge_index, edge_value, omega_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mhxwN1mIub7M"
   },
   "outputs": [],
   "source": [
    "def turn_data(graph):\n",
    "    graph_data = Data(x=graph[0], edge_index=graph[1], edge_attr=graph[2], y = graph[3])\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_PXmi7cFLFqy"
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "def create_data(file_path, k_val=6):\n",
    "    graph = create_graph(file_path, k_val)\n",
    "    data = turn_data(graph)\n",
    "    data = T.ToUndirected()(data)\n",
    "    #data = T.NormalizeFeatures()(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "V9zLYGSy_cz4"
   },
   "outputs": [],
   "source": [
    "def calculate_normalization_params(data_list):\n",
    "    # Concatenate all node features and edge attributes\n",
    "    all_x = torch.cat([data.x for data in data_list], dim=0)\n",
    "    all_edge_attr = torch.cat([data.edge_attr for data in data_list], dim=0)\n",
    "\n",
    "    # Calculate mean and std for node features and edge attributes\n",
    "    x_mean, x_std = all_x.mean(dim=0), all_x.std(dim=0)\n",
    "    edge_attr_mean, edge_attr_std = all_edge_attr.mean(dim=0), all_edge_attr.std(dim=0)\n",
    "\n",
    "    return (x_mean, x_std), (edge_attr_mean, edge_attr_std)\n",
    "\n",
    "def normalize_dataset(data_list, x_params, edge_attr_params):\n",
    "    x_mean, x_std = x_params\n",
    "    edge_attr_mean, edge_attr_std = edge_attr_params\n",
    "\n",
    "    normalized_data_list = []\n",
    "    for data in data_list:\n",
    "        normalized_x = (data.x - x_mean) / (x_std + 1e-8)\n",
    "        normalized_edge_attr = (data.edge_attr - edge_attr_mean) / (edge_attr_std + 1e-8)\n",
    "\n",
    "        # Create a new Data object with normalized features and original y value\n",
    "        normalized_data = Data(x=normalized_x,\n",
    "                               edge_index=data.edge_index,\n",
    "                               edge_attr=normalized_edge_attr,\n",
    "                               y=data.y)  # Preserve the original y value\n",
    "\n",
    "        normalized_data_list.append(normalized_data)\n",
    "\n",
    "    return normalized_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoAJQ_raMZ3c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import DataLoader\n",
    "import h5py\n",
    "directory = '/scratch/gpfs/hk4638/FinalData/NewData'\n",
    "def load_all_graphs(directory, k_val=10, box_size=25):\n",
    "    file_list = os.listdir(directory)\n",
    "    data_list = []\n",
    "    for file_name in tqdm(file_list):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        graph_data = create_data(file_path, k_val)\n",
    "        data_list.append(graph_data)\n",
    "    return data_list\n",
    "\n",
    "# Load all graphs\n",
    "data_list = load_all_graphs(directory)\n",
    "\n",
    "# Calculate normalization parameters based on all graphs\n",
    "x_params, edge_attr_params = calculate_normalization_params(data_list)\n",
    "\n",
    "# Normalize the dataset using the calculated parameters\n",
    "normalized_data_list = normalize_dataset(data_list, x_params, edge_attr_params)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def numpy_to_torch_float(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return torch.from_numpy(data).float()\n",
    "    elif isinstance(data, (np.float64, np.float32)):\n",
    "        return torch.tensor(data, dtype=torch.float32)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.float()\n",
    "    else:\n",
    "        return data\n",
    "        \n",
    "def ensure_torch_float32(data):\n",
    "    data.x = numpy_to_torch_float(data.x)\n",
    "    data.edge_attr = numpy_to_torch_float(data.edge_attr)\n",
    "    if hasattr(data, 'y'):\n",
    "        data.y = numpy_to_torch_float(data.y)\n",
    "    return data\n",
    "    \n",
    "normalized_data_list = [ensure_torch_float32(data) for data in normalized_data_list]\n",
    "\n",
    "# Calculate the lengths for the 70-15-15 split\n",
    "total_len = len(normalized_data_list)\n",
    "train_len = int(0.7 * total_len)\n",
    "val_len = int(0.15 * total_len)\n",
    "test_len = total_len - train_len - val_len  # Ensures all data is used\n",
    "\n",
    "# Perform the split\n",
    "train_data, val_data, test_data = random_split(normalized_data_list, [train_len, val_len, test_len])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogenous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_filter_data(file, mass_threshold=2e8):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        positions = f['Subhalo/SubhaloPos'][:]/1e3  # Convert to Mpc/h\n",
    "        metallicities = f['Subhalo/SubhaloStarMetallicity'][:]\n",
    "        masses = f['Subhalo/SubhaloMassType'][:,4]*1e10  # Stellar mass\n",
    "        radii = f['Subhalo/SubhaloHalfmassRadType'][:,4]/1e3\n",
    "        omega_m = f['Header'].attrs['Omega0']\n",
    "    # Filter galaxies based on the stellar mass threshold\n",
    "    mask = masses > mass_threshold\n",
    "    return positions[mask], metallicities[mask], masses[mask], radii[mask], omega_m\n",
    "\n",
    "def split_galaxies(positions, masses, metallicities, radii, split_ratios):\n",
    "    total_galaxies = len(positions)\n",
    "    indices = np.random.permutation(total_galaxies)\n",
    "    split_points = [int(ratio * total_galaxies) for ratio in np.cumsum(split_ratios)]\n",
    "    \n",
    "    a_indices = indices[:split_points[0]]\n",
    "    b_indices = indices[split_points[0]:split_points[1]]\n",
    "    c_indices = indices[split_points[1]:]\n",
    "    \n",
    "    return {\n",
    "        'A': (positions[a_indices], masses[a_indices], metallicities[a_indices], radii[a_indices]),\n",
    "        'B': (positions[b_indices], masses[b_indices], metallicities[b_indices], radii[b_indices]),\n",
    "        'C': (positions[c_indices], masses[c_indices], metallicities[c_indices], radii[c_indices])\n",
    "    }\n",
    "\n",
    "def minimum_image_distance(pos1, pos2, box_size):\n",
    "    delta = np.abs(pos1 - pos2)\n",
    "    delta = np.where(delta > 0.5 * box_size, box_size - delta, delta)\n",
    "    return np.sqrt((delta ** 2).sum(axis=-1))\n",
    "\n",
    "def create_edges_hetero(node_data, box_size=25, k=6):\n",
    "    all_positions = np.concatenate([node_data[t][0] for t in ['A', 'B', 'C']])\n",
    "    tree = cKDTree(all_positions, boxsize=box_size)\n",
    "    \n",
    "    edge_dict = {}\n",
    "    node_types = ['A', 'B', 'C']\n",
    "    start_idx = {'A': 0, 'B': len(node_data['A'][0]), 'C': len(node_data['A'][0]) + len(node_data['B'][0])}\n",
    "    \n",
    "    for src_type in node_types:\n",
    "        src_positions = node_data[src_type][0]\n",
    "        for dst_type in node_types:\n",
    "            dst_positions = node_data[dst_type][0]\n",
    "            edges = []\n",
    "            edge_attrs = []\n",
    "            \n",
    "            for i, pos in enumerate(src_positions):\n",
    "                distances, neighbors = tree.query(pos, k=k+1)\n",
    "                for j, dist in zip(neighbors[1:], distances[1:]):\n",
    "                    if start_idx[dst_type] <= j < start_idx[dst_type] + len(dst_positions):\n",
    "                        edges.append([i, j - start_idx[dst_type]])\n",
    "                        if src_type != 'B' and dst_type != 'B':\n",
    "                            actual_distance = minimum_image_distance(pos, all_positions[j], box_size)\n",
    "                            edge_attrs.append([actual_distance])\n",
    "                        else:\n",
    "                            edge_attrs.append([0.0])  # Placeholder for B connections\n",
    "            \n",
    "            if edges:\n",
    "                edge_dict[(src_type, 'to', dst_type)] = (np.array(edges).T, np.array(edge_attrs))\n",
    "    \n",
    "    return edge_dict\n",
    "\n",
    "def create_heterogeneous_graph(file_path, split_ratios, k_val=6):\n",
    "    positions, metallicities, masses, radii, omega_m = load_and_filter_data(file_path)\n",
    "    node_data = split_galaxies(positions, masses, metallicities, radii, split_ratios)\n",
    "    edge_dict = create_edges_hetero(node_data, k=k_val)\n",
    "    \n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Add node features\n",
    "    data['A'].x = torch.tensor(np.column_stack(node_data['A'][0:1]), dtype=torch.float) # only position for A\n",
    "    data['B'].x = torch.tensor(np.column_stack(node_data['B'][1:]), dtype=torch.float)  # Exclude positions for B\n",
    "    data['C'].x = torch.tensor(np.column_stack(node_data['C']), dtype=torch.float)\n",
    "    \n",
    "    # Add edges\n",
    "    for edge_type, (edge_index, edge_attr) in edge_dict.items():\n",
    "        data[edge_type].edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        if edge_type[0] != 'B' and edge_type[2] != 'B':\n",
    "            data[edge_type].edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Add global target\n",
    "    data.y = torch.tensor([omega_m], dtype=torch.float)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_dataset(file_paths, split_ratios, k_val=6):\n",
    "    dataset = []\n",
    "    for file_path in tqdm(file_paths, desc=\"Creating dataset\"):\n",
    "        graph = create_heterogeneous_graph(file_path, split_ratios, k_val)\n",
    "        graph = T.ToUndirected()(graph)\n",
    "        dataset.append(graph)\n",
    "    return dataset\n",
    "\n",
    "def calculate_normalization_params(dataset):\n",
    "    node_features = {node_type: [] for node_type in ['A', 'B', 'C']}\n",
    "    edge_features = {edge_type: [] for edge_type in [('A', 'to', 'A'), ('A', 'to', 'C'), ('C', 'to', 'C')]}\n",
    "    \n",
    "    for data in dataset:\n",
    "        for node_type in ['A', 'B', 'C']:\n",
    "            node_features[node_type].append(data[node_type].x)\n",
    "        \n",
    "        for edge_type in [('A', 'to', 'A'), ('A', 'to', 'C'), ('C', 'to', 'C')]:\n",
    "            if edge_type in data.edge_types and hasattr(data[edge_type], 'edge_attr'):\n",
    "                edge_features[edge_type].append(data[edge_type].edge_attr)\n",
    "    \n",
    "    norm_params = {}\n",
    "    for node_type, features in node_features.items():\n",
    "        if features:\n",
    "            features = torch.cat(features, dim=0)\n",
    "            norm_params[f'{node_type}_mean'] = features.mean(dim=0)\n",
    "            norm_params[f'{node_type}_std'] = features.std(dim=0)\n",
    "    \n",
    "    for edge_type, features in edge_features.items():\n",
    "        if features:\n",
    "            features = torch.cat(features, dim=0)\n",
    "            norm_params[f'{edge_type}_mean'] = features.mean(dim=0)\n",
    "            norm_params[f'{edge_type}_std'] = features.std(dim=0)\n",
    "    \n",
    "    return norm_params\n",
    "\n",
    "def normalize_graph(graph, norm_params):\n",
    "    for node_type in ['A', 'B', 'C']:\n",
    "        mean = norm_params[f'{node_type}_mean']\n",
    "        std = norm_params[f'{node_type}_std']\n",
    "        graph[node_type].x = (graph[node_type].x - mean) / (std + 1e-8)\n",
    "    \n",
    "    for edge_type in [('A', 'to', 'A'), ('A', 'to', 'C'), ('C', 'to', 'C')]:\n",
    "        if edge_type in graph.edge_types and hasattr(graph[edge_type], 'edge_attr'):\n",
    "            mean = norm_params[f'{edge_type}_mean']\n",
    "            std = norm_params[f'{edge_type}_std']\n",
    "            graph[edge_type].edge_attr = (graph[edge_type].edge_attr - mean) / (std + 1e-8)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    norm_params = calculate_normalization_params(dataset)\n",
    "    normalized_dataset = [normalize_graph(graph, norm_params) for graph in dataset]\n",
    "    return normalized_dataset, norm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading graphs: 100%|██████████| 1000/1000 [02:44<00:00,  6.09it/s]\n",
      "/tmp/ipykernel_3473201/3858287339.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float32)\n",
      "/tmp/ipykernel_3473201/3858287339.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[edge_type].edge_attr = torch.tensor(data[edge_type].edge_attr, dtype=torch.float32)\n",
      "/tmp/ipykernel_3473201/3858287339.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data.y = torch.tensor(data.y, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of graphs: 1000\n",
      "Number of training graphs: 700\n",
      "Number of validation graphs: 150\n",
      "Number of test graphs: 150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming the previously defined functions are available:\n",
    "# create_heterogeneous_graph, calculate_normalization_params, normalize_graph\n",
    "\n",
    "directory = '/scratch/gpfs/hk4638/FinalData/NewData'\n",
    "\n",
    "def load_all_heterogeneous_graphs(directory, split_ratios, k_val=10, box_size=25):\n",
    "    file_list = os.listdir(directory)\n",
    "    data_list = []\n",
    "    for file_name in tqdm(file_list, desc=\"Loading graphs\"):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        graph_data = create_heterogeneous_graph(file_path, split_ratios, k_val)\n",
    "        data_list.append(graph_data)\n",
    "    return data_list\n",
    "\n",
    "# Load all heterogeneous graphs\n",
    "split_ratios = [0.3, 0.3, 0.4]  # 30% A, 30% B, 40% C\n",
    "data_list = load_all_heterogeneous_graphs(directory, split_ratios)\n",
    "\n",
    "# Calculate normalization parameters based on all graphs\n",
    "norm_params = calculate_normalization_params(data_list)\n",
    "\n",
    "# Normalize the dataset using the calculated parameters\n",
    "normalized_data_list = [normalize_graph(graph, norm_params) for graph in data_list]\n",
    "\n",
    "def ensure_torch_float32(data):\n",
    "    for node_type in data.node_types:\n",
    "        data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float32)\n",
    "    for edge_type in data.edge_types:\n",
    "        if hasattr(data[edge_type], 'edge_attr'):\n",
    "            data[edge_type].edge_attr = torch.tensor(data[edge_type].edge_attr, dtype=torch.float32)\n",
    "    data.y = torch.tensor(data.y, dtype=torch.float32)\n",
    "    return data\n",
    "\n",
    "normalized_data_list = [ensure_torch_float32(data) for data in normalized_data_list]\n",
    "\n",
    "# Calculate the lengths for the 70-15-15 split\n",
    "total_len = len(normalized_data_list)\n",
    "train_len = int(0.7 * total_len)\n",
    "val_len = int(0.15 * total_len)\n",
    "test_len = total_len - train_len - val_len  # Ensures all data is used\n",
    "\n",
    "# Perform the split\n",
    "train_data, val_data, test_data = random_split(normalized_data_list, [train_len, val_len, test_len])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(f\"Total number of graphs: {total_len}\")\n",
    "print(f\"Number of training graphs: {train_len}\")\n",
    "print(f\"Number of validation graphs: {val_len}\")\n",
    "print(f\"Number of test graphs: {test_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD3eA9nTAAch"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, global_mean_pool\n",
    "\n",
    "class ComplexGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers=4, heads=4, dropout_rate=0.1):\n",
    "        super(ComplexGAT, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "        #self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "            #GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "            for _ in range(num_layers - 2)\n",
    "        ])\n",
    "\n",
    "        self.conv_last = GATv2Conv(hidden_channels * heads, hidden_channels, heads=1, concat=False, edge_dim=1)\n",
    "        #self.conv_last = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, edge_dim=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, 1)  # Output a single value for omega_m\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index, edge_attr=edge_attr))\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.conv_last(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mksges42ACLq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GATv2Conv, global_mean_pool\n",
    "\n",
    "class ComplexGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers=4, heads=4, dropout_rate=0.1):\n",
    "        super(ComplexGAT, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # First GAT layer\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "        self.ln1 = nn.LayerNorm(hidden_channels * heads)  # LayerNorm after first layer\n",
    "\n",
    "        # Intermediate GAT layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
    "            for _ in range(num_layers - 2)\n",
    "        ])\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(hidden_channels * heads) for _ in range(num_layers - 2)])\n",
    "\n",
    "        # Final GAT layer\n",
    "        self.conv_last = GATv2Conv(hidden_channels * heads, hidden_channels, heads=1, concat=False, edge_dim=1)\n",
    "        self.ln_last = nn.LayerNorm(hidden_channels)  # LayerNorm after last layer\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, 1)  # Output a single value for omega_m\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # First GAT layer with LayerNorm\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.ln1(x)  # Apply LayerNorm\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Intermediate GAT layers with LayerNorm\n",
    "        for conv, ln in zip(self.convs, self.lns):\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            x = ln(x)  # Apply LayerNorm\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Final GAT layer with LayerNorm\n",
    "        x = self.conv_last(x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.ln_last(x)  # Apply LayerNorm\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MetaLayer, global_add_pool, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "\n",
    "class MetaLayerGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=2, num_layers=4, dropout_rate=0.1, activation='ReLU', skip_connection=True, self_loops=True):\n",
    "        super(MetaLayerGNN, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.skip_connection = skip_connection\n",
    "        self.self_loops = self_loops\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'ELU':\n",
    "            self.activation = nn.ELU()\n",
    "        elif activation == 'GELU':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels)\n",
    "        )\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            edge_model = EdgeModel(hidden_channels, hidden_channels, hidden_channels, hidden_channels, self.activation)\n",
    "            node_model = NodeModel(hidden_channels, hidden_channels, hidden_channels, hidden_channels, self.activation)\n",
    "            global_model = GlobalModel(hidden_channels, hidden_channels, hidden_channels, hidden_channels, self.activation)\n",
    "            self.layers.append(MetaLayer(edge_model, node_model, global_model))\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 4, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        u = torch.zeros(batch.max().item() + 1, self.hidden_channels, device=x.device)\n",
    "\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr.unsqueeze(1))\n",
    "\n",
    "        if self.self_loops:\n",
    "            edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=x.size(0))\n",
    "        else:\n",
    "            edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_prev, edge_attr_prev, u_prev = x, edge_attr, u\n",
    "            x, edge_attr, u = layer(x, edge_index, edge_attr, u, batch)\n",
    "            if self.skip_connection:\n",
    "                x = x + x_prev\n",
    "                edge_attr = edge_attr + edge_attr_prev\n",
    "                u = u + u_prev\n",
    "            x = self.activation(x)\n",
    "            edge_attr = self.activation(edge_attr)\n",
    "            u = self.activation(u)\n",
    "\n",
    "        add_pool = global_add_pool(x, batch)\n",
    "        mean_pool = global_mean_pool(x, batch)\n",
    "        max_pool = global_max_pool(x, batch)\n",
    "\n",
    "        out = torch.cat([add_pool, mean_pool, max_pool, u], dim=1)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "\n",
    "class EdgeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, edge_out, activation):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in*2 + edge_in, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, edge_out),\n",
    "            nn.LayerNorm(edge_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        out = torch.cat([src, dest, edge_attr], dim=1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, node_out, activation):\n",
    "        super().__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + edge_in*3, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, node_out),\n",
    "            nn.LayerNorm(node_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out1 = torch.zeros(x.size(0), edge_attr.size(1), device=x.device).index_add_(0, col, edge_attr)\n",
    "        out2 = torch.zeros(x.size(0), edge_attr.size(1), device=x.device).index_add_(0, col, edge_attr)\n",
    "        out2 = torch.max(out2, out1)\n",
    "        out3 = out1 / (torch.zeros(x.size(0), 1, device=x.device).index_add_(0, col, torch.ones_like(edge_attr[:, :1])) + 1e-8)\n",
    "        out = torch.cat([x, out1, out2, out3], dim=1)\n",
    "        return self.node_mlp(out)\n",
    "\n",
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, global_out, activation):\n",
    "        super().__init__()\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + edge_in + hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, global_out),\n",
    "            nn.LayerNorm(global_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([\n",
    "            global_mean_pool(x, batch),\n",
    "            global_mean_pool(edge_attr, batch[row]),\n",
    "            u\n",
    "        ], dim=1)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MetaLayer, global_add_pool, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "\n",
    "class EnhancedMetaLayerGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=1, num_layers=4, dropout_rate=0.1, \n",
    "                 activation='ReLU', skip_connection=True, self_loops=True, num_heads=4, device='cuda'):\n",
    "        super(EnhancedMetaLayerGNN, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.skip_connection = skip_connection\n",
    "        self.self_loops = self_loops\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        # Activation function dictionary\n",
    "        self.activation_dict = nn.ModuleDict({\n",
    "            'ReLU': nn.ReLU(),\n",
    "            'LeakyReLU': nn.LeakyReLU(),\n",
    "            'ELU': nn.ELU(),\n",
    "            'GELU': nn.GELU()\n",
    "        })\n",
    "        \n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation_dict[activation]\n",
    "        ).to(device)\n",
    "        \n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation_dict[activation]\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initial global features\n",
    "        self.global_encoder = nn.Sequential(\n",
    "            nn.Linear(1, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation_dict[activation]\n",
    "        ).to(device)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            layer_hidden = hidden_channels // 2 if i % 2 == 0 else hidden_channels\n",
    "            edge_model = EnhancedEdgeModel(hidden_channels, hidden_channels, layer_hidden, hidden_channels, activation, num_heads, device)\n",
    "            node_model = EnhancedNodeModel(hidden_channels, hidden_channels, layer_hidden, hidden_channels, activation, num_heads, device)\n",
    "            global_model = EnhancedGlobalModel(hidden_channels, hidden_channels, layer_hidden, hidden_channels, activation, device)\n",
    "            self.layers.append(MetaLayer(edge_model, node_model, global_model).to(device))\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 4, hidden_channels * 2),\n",
    "            nn.LayerNorm(hidden_channels * 2),\n",
    "            self.activation_dict[activation],\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation_dict[activation],\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x.to(self.device), data.edge_index.to(self.device), data.edge_attr.to(self.device), data.batch.to(self.device)\n",
    "        \n",
    "        # Initialize global features based on graph size\n",
    "        u = self.global_encoder(torch.log(torch.bincount(batch).float().unsqueeze(-1).to(self.device)))\n",
    "\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr.unsqueeze(1))\n",
    "\n",
    "        if self.self_loops:\n",
    "            edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=x.size(0))\n",
    "        else:\n",
    "            edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x_prev, edge_attr_prev, u_prev = x, edge_attr, u\n",
    "            x, edge_attr, u = layer(x, edge_index, edge_attr, u, batch)\n",
    "            if self.skip_connection:\n",
    "                x = x + x_prev\n",
    "                edge_attr = edge_attr + edge_attr_prev\n",
    "                u = u + u_prev\n",
    "            x = self.activation_dict['ReLU' if i % 2 == 0 else 'GELU'](x)\n",
    "            edge_attr = self.activation_dict['LeakyReLU' if i % 2 == 0 else 'ELU'](edge_attr)\n",
    "            u = self.activation_dict['ReLU' if i % 2 == 0 else 'GELU'](u)\n",
    "\n",
    "        # Learnable pooling\n",
    "        add_pool = global_add_pool(x, batch)\n",
    "        mean_pool = global_mean_pool(x, batch)\n",
    "        max_pool = global_max_pool(x, batch)\n",
    "\n",
    "        out = torch.cat([add_pool, mean_pool, max_pool, u], dim=1)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "\n",
    "class EnhancedEdgeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, edge_out, activation, num_heads, device):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in*2 + edge_in, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, edge_out * num_heads),\n",
    "            nn.LayerNorm(edge_out * num_heads)\n",
    "        ).to(device)\n",
    "        self.attention = nn.Parameter(torch.Tensor(1, num_heads, edge_out)).to(device)\n",
    "        self.activation = activation\n",
    "        self.num_heads = num_heads\n",
    "        self.edge_out = edge_out\n",
    "        self.device = device\n",
    "        nn.init.xavier_uniform_(self.attention)\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        out = torch.cat([src, dest, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out).view(-1, self.num_heads, self.edge_out)\n",
    "        out = F.softmax(out * self.attention, dim=1).sum(dim=1)\n",
    "        return out\n",
    "\n",
    "class EnhancedNodeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, node_out, activation, num_heads, device):\n",
    "        super().__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + edge_in*3, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, node_out * num_heads),\n",
    "            nn.LayerNorm(node_out * num_heads)\n",
    "        ).to(device)\n",
    "        self.attention = nn.Parameter(torch.Tensor(1, num_heads, node_out)).to(device)\n",
    "        self.activation = activation\n",
    "        self.num_heads = num_heads\n",
    "        self.node_out = node_out\n",
    "        self.device = device\n",
    "        nn.init.xavier_uniform_(self.attention)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out1 = torch.zeros_like(x).index_add_(0, col, edge_attr)\n",
    "        out2 = torch.zeros_like(x).index_add_(0, col, edge_attr)\n",
    "        out2 = torch.max(out2, out1)\n",
    "        count = torch.zeros(x.size(0), 1, device=self.device).index_add_(0, col, torch.ones_like(edge_attr[:, :1]))\n",
    "        out3 = out1 / (count + 1e-8)\n",
    "        out = torch.cat([x, out1, out2, out3], dim=1)\n",
    "        out = self.node_mlp(out).view(-1, self.num_heads, self.node_out)\n",
    "        out = F.softmax(out * self.attention, dim=1).sum(dim=1)\n",
    "        return out\n",
    "\n",
    "class EnhancedGlobalModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, global_out, activation, device):\n",
    "        super().__init__()\n",
    "        self.node_in = node_in\n",
    "        self.edge_in = edge_in\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.global_out = global_out\n",
    "        self.activation = activation\n",
    "        self.device = device\n",
    "\n",
    "        # Dynamic MLP will be created in forward pass\n",
    "        self.global_mlp = None\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([\n",
    "            global_mean_pool(x, batch),\n",
    "            global_max_pool(x, batch),\n",
    "            global_add_pool(x, batch),\n",
    "            global_mean_pool(edge_attr, batch[row]),\n",
    "            u\n",
    "        ], dim=1)\n",
    "\n",
    "        # Create MLP dynamically based on input size\n",
    "        if self.global_mlp is None or self.global_mlp[0].in_features != out.shape[1]:\n",
    "            self.global_mlp = nn.Sequential(\n",
    "                nn.Linear(out.shape[1], self.hidden_channels * 2),\n",
    "                nn.LayerNorm(self.hidden_channels * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_channels * 2, self.hidden_channels),\n",
    "                nn.LayerNorm(self.hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.hidden_channels, self.global_out),\n",
    "                nn.LayerNorm(self.global_out)\n",
    "            ).to(self.device)\n",
    "\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, global_mean_pool\n",
    "\n",
    "class HeteroComplexGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels_dict, hidden_channels, num_layers=4, heads=4, dropout_rate=0.1):\n",
    "        super(HeteroComplexGAT, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Input to same dimentions\n",
    "        self.lin_dict = nn.ModuleDict({\n",
    "            node_type: nn.Linear(in_channels_dict[node_type], hidden_channels * heads)\n",
    "            for node_type in in_channels_dict\n",
    "        })\n",
    "\n",
    "        \n",
    "        edge_types = [\n",
    "            ('A', 'to', 'A'), ('A', 'to', 'C'), ('C', 'to', 'A'), ('B', 'to', 'B'), ('B', 'to', 'C'), ('C', 'to', 'B'), ('C', 'to', 'C')\n",
    "        ]\n",
    "\n",
    "        # Initialize the first layer using HeteroConv with GATConv or SAGEConv depending on edge types\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: (GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, add_self_loops=False, edge_dim=1) \n",
    "                        if edge_type[0] != 'B' and edge_type[2] != 'B' \n",
    "                        else SAGEConv(hidden_channels * heads, hidden_channels * heads))\n",
    "            for edge_type in edge_types\n",
    "        })\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers - 2):\n",
    "            conv = HeteroConv({\n",
    "                edge_type: (GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, add_self_loops=False, edge_dim=1)\n",
    "                            if edge_type[0] != 'B' and edge_type[2] != 'B'\n",
    "                            else SAGEConv(hidden_channels * heads, hidden_channels * heads))\n",
    "                for edge_type in edge_types\n",
    "            })\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.conv_last = HeteroConv({\n",
    "            edge_type: (GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, add_self_loops=False, edge_dim=1)\n",
    "                        if edge_type[0] != 'B' and edge_type[2] != 'B'\n",
    "                        else SAGEConv(hidden_channels * heads, hidden_channels))\n",
    "            for edge_type in edge_types\n",
    "        })\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_channels * len(in_channels_dict), hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, 1)  # Output a single value for omega_m\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict, edge_index_dict, edge_attr_dict = data.x_dict, data.edge_index_dict, data.edge_attr_dict\n",
    "\n",
    "        # Apply initial linear transformation to standardize input dimensions\n",
    "        x_dict = {key: self.lin_dict[key](x) for key, x in x_dict.items()}\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict, edge_attr_dict)\n",
    "        x_dict = {key: nn.functional.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: nn.functional.dropout(x, p=self.dropout_rate, training=self.training) for key, x in x_dict.items()}\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict, edge_attr_dict)\n",
    "            x_dict = {key: nn.functional.relu(x) for key, x in x_dict.items()}\n",
    "            x_dict = {key: nn.functional.dropout(x, p=self.dropout_rate, training=self.training) for key, x in x_dict.items()}\n",
    "\n",
    "        x_dict = self.conv_last(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = torch.cat([global_mean_pool(x, data[node_type].batch) for node_type, x in x_dict.items()], dim=1)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MetaLayer, global_mean_pool\n",
    "from torch_geometric.utils import add_self_loops, remove_self_loops\n",
    "\n",
    "class HeteroMetaLayerGNN(nn.Module):\n",
    "    def __init__(self, in_channels_dict, hidden_channels, out_channels=1, num_layers=4, dropout_rate=0.1, activation='ReLU', skip_connection=True, self_loops=True):\n",
    "        super(HeteroMetaLayerGNN, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.skip_connection = skip_connection\n",
    "        self.self_loops = self_loops\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.device = torch.device('cpu')\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'ELU':\n",
    "            self.activation = nn.ELU()\n",
    "        elif activation == 'GELU':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Node encoders\n",
    "        self.node_encoders = nn.ModuleDict({\n",
    "            node_type: nn.Linear(in_channels_dict[node_type], hidden_channels)\n",
    "            for node_type in in_channels_dict\n",
    "        })\n",
    "\n",
    "        # Edge types\n",
    "        self.edge_types = [\n",
    "            ('A', 'to', 'A'), ('A', 'to', 'B'), ('A', 'to', 'C'),\n",
    "            ('B', 'to', 'A'), ('B', 'to', 'B'), ('B', 'to', 'C'),\n",
    "            ('C', 'to', 'A'), ('C', 'to', 'B'), ('C', 'to', 'C')\n",
    "        ]\n",
    "\n",
    "        # MetaLayers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            edge_model = EdgeModel(hidden_channels, 1, hidden_channels, hidden_channels, self.activation)\n",
    "            node_model = NodeModel(hidden_channels, hidden_channels, hidden_channels, hidden_channels, self.activation)\n",
    "            global_model = GlobalModel(hidden_channels, hidden_channels, hidden_channels, hidden_channels, self.activation)\n",
    "            self.layers.append(MetaLayer(edge_model, node_model, global_model))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * len(in_channels_dict), hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        try:\n",
    "            x_dict = {node_type: self.node_encoders[node_type](data[node_type].x.float()) for node_type in data.node_types}\n",
    "            edge_index_dict = {edge_type: data[edge_type].edge_index for edge_type in data.edge_types}\n",
    "            edge_attr_dict = {edge_type: data[edge_type].edge_attr for edge_type in data.edge_types if hasattr(data[edge_type], 'edge_attr')}\n",
    "            \n",
    "            batch_dict = {node_type: data[node_type].batch if hasattr(data[node_type], 'batch') else None for node_type in data.node_types}\n",
    "    \n",
    "            # Create a mapping for node indices\n",
    "            node_mapper = {}\n",
    "            start_idx = 0\n",
    "            for node_type in data.node_types:\n",
    "                num_nodes = x_dict[node_type].size(0)\n",
    "                node_mapper[node_type] = (start_idx, start_idx + num_nodes)\n",
    "                start_idx += num_nodes\n",
    "    \n",
    "            for layer in self.layers:\n",
    "                x_dict_new, edge_attr_dict_new = {}, {}\n",
    "    \n",
    "                for edge_type in self.edge_types:\n",
    "                    if edge_type not in edge_index_dict or edge_index_dict[edge_type].size(1) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    src, _, dst = edge_type\n",
    "                    x_src, x_dst = x_dict[src], x_dict[dst]\n",
    "                    edge_index = edge_index_dict[edge_type]\n",
    "                    edge_attr = edge_attr_dict.get(edge_type, None)\n",
    "                    \n",
    "                    # Adjust edge indices\n",
    "                    src_start, src_end = node_mapper[src]\n",
    "                    dst_start, dst_end = node_mapper[dst]\n",
    "                    edge_index_adjusted = edge_index.clone()\n",
    "                    edge_index_adjusted[0] -= src_start\n",
    "                    edge_index_adjusted[1] -= dst_start\n",
    "    \n",
    "                    # Filter valid edges\n",
    "                    mask = (edge_index_adjusted[0] >= 0) & (edge_index_adjusted[0] < x_src.size(0)) & \\\n",
    "                           (edge_index_adjusted[1] >= 0) & (edge_index_adjusted[1] < x_dst.size(0))\n",
    "                    edge_index_adjusted = edge_index_adjusted[:, mask]\n",
    "                    \n",
    "                    if edge_attr is not None and edge_attr.size(0) == mask.size(0):\n",
    "                        edge_attr = edge_attr[mask]\n",
    "    \n",
    "                    if edge_index_adjusted.size(1) > 0:\n",
    "                        x_new, edge_attr_new, _ = layer(x_src, edge_index_adjusted, edge_attr, None, batch_dict[src])\n",
    "                        \n",
    "                        if src not in x_dict_new:\n",
    "                            x_dict_new[src] = x_new\n",
    "                        else:\n",
    "                            x_dict_new[src] += x_new\n",
    "                        \n",
    "                        edge_attr_dict_new[edge_type] = edge_attr_new\n",
    "                \n",
    "                # Apply skip connections and activation\n",
    "                for node_type in x_dict:\n",
    "                    if node_type in x_dict_new:\n",
    "                        if self.skip_connection:\n",
    "                            x_dict[node_type] = self.activation(x_dict_new[node_type] + x_dict[node_type])\n",
    "                        else:\n",
    "                            x_dict[node_type] = self.activation(x_dict_new[node_type])\n",
    "                        x_dict[node_type] = F.dropout(x_dict[node_type], p=self.dropout_rate, training=self.training)\n",
    "                \n",
    "                edge_attr_dict = edge_attr_dict_new\n",
    "    \n",
    "            # Global pooling\n",
    "            pooled_x = torch.cat([global_mean_pool(x, batch_dict[node_type]) for node_type, x in x_dict.items()], dim=1)\n",
    "    \n",
    "            # Final output\n",
    "            out = self.output_layer(pooled_x)\n",
    "    \n",
    "            return out.squeeze()\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {str(e)}\")\n",
    "            print(f\"Error occurred at line {e.__traceback__.tb_lineno}\")\n",
    "            raise\n",
    "\n",
    "class EdgeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, edge_out, activation):\n",
    "        super().__init__()\n",
    "        # Store node_in as a class attribute\n",
    "        self.node_in = node_in\n",
    "        self.edge_in = edge_in\n",
    "        self.input_size = node_in * 2 + edge_in  # This should match the concatenated features size\n",
    "        \n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_size, hidden_channels),  # Match this size to the expected concatenated dimension\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, edge_out),\n",
    "            nn.LayerNorm(edge_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        # Ensure src and dest are 2D\n",
    "        src = src.unsqueeze(-1) if src.dim() == 1 else src\n",
    "        dest = dest.unsqueeze(-1) if dest.dim() == 1 else dest\n",
    "        \n",
    "        # Handle edge_attr\n",
    "        if edge_attr is None or edge_attr.numel() == 0:\n",
    "            edge_attr = torch.zeros(src.size(0), 1, device=src.device)\n",
    "        else:\n",
    "            # Check and correct edge_attr size\n",
    "            if edge_attr.size(1) != 1 and edge_attr.size(1) != self.input_size - self.node_in * 2:\n",
    "                edge_attr = edge_attr[:, :1]  # Reshape or slice to match expected size\n",
    "        out = torch.cat([src, dest, edge_attr], dim=1)\n",
    "\n",
    "        # Ensure the first layer in edge_mlp matches the size of the concatenated tensor\n",
    "        if out.size(1) != self.input_size:\n",
    "            raise RuntimeError(f\"Unexpected input size {out.size(1)}, expected {self.input_size}\")\n",
    "        \n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, node_out, activation):\n",
    "        super().__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + edge_in, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, node_out),\n",
    "            nn.LayerNorm(node_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x, edge_attr.mean(dim=0).repeat(x.size(0), 1)], dim=1)\n",
    "        return self.node_mlp(out)\n",
    "\n",
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, node_in, edge_in, hidden_channels, global_out, activation):\n",
    "        super().__init__()\n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in + edge_in, hidden_channels),\n",
    "            nn.LayerNorm(hidden_channels),\n",
    "            activation,\n",
    "            nn.Linear(hidden_channels, global_out),\n",
    "            nn.LayerNorm(global_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # Pool node features\n",
    "        pooled_x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Pool edge attributes if available\n",
    "        if edge_attr is not None and edge_attr.size(0) > 0:\n",
    "            pooled_edge_attr = global_mean_pool(edge_attr, batch[edge_index[0]])\n",
    "            \n",
    "            # Adjust dimensions to match pooled_x\n",
    "            if pooled_edge_attr.size(0) < pooled_x.size(0):\n",
    "                pooled_edge_attr = F.pad(pooled_edge_attr, (0, 0, 0, pooled_x.size(0) - pooled_edge_attr.size(0)))\n",
    "            elif pooled_edge_attr.size(0) > pooled_x.size(0):\n",
    "                pooled_edge_attr = pooled_edge_attr[:pooled_x.size(0), :]\n",
    "        else:\n",
    "            # If edge_attr is None or empty, create a zero tensor\n",
    "            pooled_edge_attr = torch.zeros_like(pooled_x)\n",
    "\n",
    "        out = torch.cat([pooled_x, pooled_edge_attr], dim=1)\n",
    "        return self.global_mlp(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCosmoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCosmoLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Split the predictions into mean and standard deviation\n",
    "        mean_pred = pred[:, 0]\n",
    "        std_pred = pred[:, 1]\n",
    "\n",
    "        # Ensure the standard deviation is positive\n",
    "        std_pred = F.softplus(std_pred)\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse_loss = torch.sum((mean_pred - target) ** 2)\n",
    "\n",
    "        # Likelihood-free inference (LFI) loss\n",
    "        lfi_loss = torch.sum(((mean_pred - target) ** 2 - std_pred ** 2) ** 2)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = torch.log(mse_loss) + torch.log(lfi_loss)\n",
    "\n",
    "        print(mse_loss, lfi_loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_5UGNXhNAggG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the custom loss function\n",
    "criterion = CustomCosmoLoss()\n",
    "\n",
    "# Data augmentation setup (if used)\n",
    "def augment_data(data, jitter_strength=0.029416683718052974, dropout_prob=0.07431779460223045):\n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.x.shape) > dropout_prob\n",
    "        data.x = data.x * mask.float()\n",
    "    \n",
    "    data.x += torch.randn_like(data.x) * jitter_strength\n",
    "    \n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.edge_index.shape[1]) > dropout_prob\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=128,  # Given batch_size\n",
    "    shuffle=True, \n",
    "    collate_fn=augment_data \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data, \n",
    "    batch_size=128  # Use the same batch_size for validation\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=100, load_best_model=False):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if load_best_model:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load('/scratch/gpfs/hk4638/astrid_optimization/model_54.pth'))\n",
    "            print(\"Loaded model from best_model.pth\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"best_model.pth not found. Starting with a new model.\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_mses = []\n",
    "    val_r2s = []\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            epoch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            for data in epoch_pbar:\n",
    "                data = data.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                \n",
    "                # Calculate the loss using the CustomCosmoLoss function\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.5888545489072394)  # Apply given grad_clip\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                epoch_pbar.set_postfix({'Train Loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_predictions = []\n",
    "            val_true = []\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    data = data.to(device)\n",
    "                    out = model(data)\n",
    "                    val_loss += criterion(out, data.y).item()\n",
    "                    \n",
    "                    # Use the mean predictions for calculating MSE and R²\n",
    "                    mean_pred = out[:, 0].cpu().numpy()\n",
    "                    val_predictions.extend(mean_pred)\n",
    "                    val_true.extend(data.y.cpu().numpy())\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            val_mse = mean_squared_error(val_true, val_predictions)\n",
    "            val_r2 = r2_score(val_true, val_predictions)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_mses.append(val_mse)\n",
    "            val_r2s.append(val_r2)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}')\n",
    "            print(f'  Val Loss: {val_loss:.4f}')\n",
    "            print(f'  Val MSE: {val_mse:.4f}')\n",
    "            print(f'  Val R2: {val_r2:.4f}')\n",
    "            \n",
    "            # Save the model after each epoch\n",
    "            torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "            print(f'  Model saved as model_epoch_{epoch+1}.pth')\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(best_model, 'best_model.pth')\n",
    "                print('  New best model saved as best_model.pth!')\n",
    "            \n",
    "            print()  # Add an empty line for better readability between epochs\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted. Saving current model state...\")\n",
    "        torch.save(model.state_dict(), 'interrupted_model.pth')\n",
    "        print(\"Model saved as interrupted_model.pth\")\n",
    "        if best_model is not None:\n",
    "            model.load_state_dict(best_model)\n",
    "        return model, train_losses, val_losses, val_mses, val_r2s\n",
    "    \n",
    "    # Load the best model if training completed without interruption\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, train_losses, val_losses, val_mses, val_r2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "5mxL3nt4Avrt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.0832, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.7849, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.95it/s, Train Loss=6.3774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.1580, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.0736, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.02it/s, Train Loss=5.9460]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.9588, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.9067, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  50%|█████     | 3/6 [00:01<00:01,  1.96it/s, Train Loss=6.1016]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.8348, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.9846, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s, Train Loss=6.3501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.6715, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.0164, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.5455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2616, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1727, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 1/1000:\n",
      "  Train Loss: 5.9196\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_1.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.8437, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.0390, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.94it/s, Train Loss=5.8500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.8434, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.8867, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.93it/s, Train Loss=6.1614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1257, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.2948, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  50%|█████     | 3/6 [00:01<00:01,  1.98it/s, Train Loss=5.9590]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1169, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.3692, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=5.6087]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6819, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3658, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.6860]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.2974, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.2718, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 2/1000:\n",
      "  Train Loss: 5.8361\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_2.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.0010, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.6192, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.18it/s, Train Loss=5.2451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.7192, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.7522, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.32it/s, Train Loss=6.4494]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.0211, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.4306, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  50%|█████     | 3/6 [00:01<00:01,  2.14it/s, Train Loss=5.9994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.0043, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.6253, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.1830]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.9554, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.4575, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 6/6 [00:02<00:00,  2.23it/s, Train Loss=4.9116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8644, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.5635, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 3/1000:\n",
      "  Train Loss: 5.7614\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_3.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.6461, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.6468, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.92it/s, Train Loss=5.2389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.8719, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.6192, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.87it/s, Train Loss=5.9026]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.9272, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.1333, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  50%|█████     | 3/6 [00:01<00:01,  1.88it/s, Train Loss=5.9399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.0816, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7884, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.95it/s, Train Loss=5.8710]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.6855, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.9673, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=2.9290]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5724, device='cuda:0', grad_fn=<SumBackward0>) tensor(2.4707, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 4/1000:\n",
      "  Train Loss: 5.3240\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_4.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1111, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.1844, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.12it/s, Train Loss=5.8713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.1761, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0627, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.24it/s, Train Loss=5.6473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.9318, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.5117, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  50%|█████     | 3/6 [00:01<00:01,  2.10it/s, Train Loss=6.2947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5324, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.9400, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.08it/s, Train Loss=5.8757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.2974, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0560, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.4952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0943, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.3564, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 5/1000:\n",
      "  Train Loss: 5.6490\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_5.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.9539, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2529, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.10it/s, Train Loss=5.7863]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.1318, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.4686, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.11it/s, Train Loss=6.1384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.5714, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.3775, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=5.4399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.1028, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8084, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  67%|██████▋   | 4/6 [00:01<00:01,  1.99it/s, Train Loss=5.8497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.4202, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.4558, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 6/6 [00:02<00:00,  2.19it/s, Train Loss=4.4200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.1794, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.3053, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 6/1000:\n",
      "  Train Loss: 5.6145\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_6.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.2215, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.4385, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.02it/s, Train Loss=6.1454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.4300, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.4311, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.15it/s, Train Loss=5.8518]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.2634, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.0223, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  50%|█████     | 3/6 [00:01<00:01,  2.16it/s, Train Loss=5.9082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4052, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0987, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.05it/s, Train Loss=5.6590]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4070, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.2308, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 6/6 [00:02<00:00,  2.23it/s, Train Loss=4.4638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.3922, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.6403, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 7/1000:\n",
      "  Train Loss: 5.6167\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_7.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.5471, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0361, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.15it/s, Train Loss=5.6417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.0898, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.6418, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.05it/s, Train Loss=6.0491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9078, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.4943, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  50%|█████     | 3/6 [00:01<00:01,  1.97it/s, Train Loss=5.6432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.9100, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.5247, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=5.8567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.3157, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.3142, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.9042]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1486, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.4500, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 8/1000:\n",
      "  Train Loss: 5.4560\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_8.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.0879, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.4942, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.98it/s, Train Loss=6.1446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.0657, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.7141, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.97it/s, Train Loss=6.2828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.4073, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5166, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  50%|█████     | 3/6 [00:01<00:01,  2.01it/s, Train Loss=5.5478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.1959, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.3640, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.07it/s, Train Loss=6.1723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.3199, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.2151, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, Train Loss=3.7980]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6290, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.1973, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 9/1000:\n",
      "  Train Loss: 5.7306\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_9.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3217, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.2614, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.89it/s, Train Loss=6.3547]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.4236, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8490, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.87it/s, Train Loss=5.8275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.1373, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.1955, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  50%|█████     | 3/6 [00:01<00:01,  1.90it/s, Train Loss=5.8382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.9257, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0290, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  67%|██████▋   | 4/6 [00:02<00:00,  2.00it/s, Train Loss=5.6350]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2729, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.0706, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.3137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6398, device='cuda:0', grad_fn=<SumBackward0>) tensor(2.5833, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 10/1000:\n",
      "  Train Loss: 5.4860\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_10.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.3411, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.5137, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.93it/s, Train Loss=6.4130]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.7328, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.4248, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.89it/s, Train Loss=6.5174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.9616, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.9140, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  50%|█████     | 3/6 [00:01<00:01,  2.00it/s, Train Loss=6.5517]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6078, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.7371, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=5.5940]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3389, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5195, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 6/6 [00:02<00:00,  2.23it/s, Train Loss=5.0094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.5524, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.6332, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 11/1000:\n",
      "  Train Loss: 5.9696\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_11.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5701, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8939, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.98it/s, Train Loss=5.8735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3901, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.6530, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.92it/s, Train Loss=6.0988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.7094, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.2924, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  50%|█████     | 3/6 [00:01<00:01,  1.97it/s, Train Loss=5.9815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.7942, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.6356, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  67%|██████▋   | 4/6 [00:02<00:00,  2.02it/s, Train Loss=5.5156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.7535, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.5050, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.6435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.5980, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.2475, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 12/1000:\n",
      "  Train Loss: 5.7441\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_12.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.9847, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8717, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.16it/s, Train Loss=5.7178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.8957, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.9941, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.06it/s, Train Loss=6.4579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.2878, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8068, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  50%|█████     | 3/6 [00:01<00:01,  2.00it/s, Train Loss=5.6866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.5357, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.9337, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.05it/s, Train Loss=6.4233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.2915, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2747, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.4393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.3781, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.8920, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 13/1000:\n",
      "  Train Loss: 5.7361\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_13.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3507, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.2948, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.94it/s, Train Loss=6.2632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6439, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.1500, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.92it/s, Train Loss=6.1354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3888, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.6600, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=6.0657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.7407, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.4393, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.4332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6187, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8885, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|██████████| 6/6 [00:02<00:00,  2.19it/s, Train Loss=4.2846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.3372, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.8822, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 14/1000:\n",
      "  Train Loss: 5.8428\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_14.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.3568, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.9548, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.11it/s, Train Loss=5.7901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.4392, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.0873, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.22it/s, Train Loss=5.4817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7261, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.0352, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000:  50%|█████     | 3/6 [00:01<00:01,  2.09it/s, Train Loss=5.8799]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.7812, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.5431, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000:  67%|██████▋   | 4/6 [00:01<00:01,  1.98it/s, Train Loss=6.0328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2453, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.2269, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=5.4148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3376, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.2538, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 15/1000:\n",
      "  Train Loss: 5.8750\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_15.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9751, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.8392, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.88it/s, Train Loss=5.5813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8603, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.1098, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000:  33%|███▎      | 2/6 [00:01<00:01,  2.02it/s, Train Loss=5.9741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8227, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8255, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=5.8773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.6537, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5196, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.07it/s, Train Loss=5.6362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9731, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.9750, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.7840]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.0017, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.9714, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 16/1000:\n",
      "  Train Loss: 5.5746\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_16.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7318, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.5540, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.91it/s, Train Loss=5.9989]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.1977, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.9347, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.86it/s, Train Loss=6.6202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.6748, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.6010, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000:  50%|█████     | 3/6 [00:01<00:01,  1.93it/s, Train Loss=6.4700]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.9989, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.1244, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=6.4773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.3271, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3280, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.6581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.4535, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.8228, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 17/1000:\n",
      "  Train Loss: 5.9739\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_17.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.2815, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.6303, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.88it/s, Train Loss=6.1597]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.2201, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.5742, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.95it/s, Train Loss=6.3390]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.1479, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.5736, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=6.4445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6344, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.3175, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.05it/s, Train Loss=6.0433]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.1414, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.9014, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.0385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.3320, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.6013, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 18/1000:\n",
      "  Train Loss: 5.8922\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_18.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.0152, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.9163, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.99it/s, Train Loss=5.8549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5850, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.1760, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.13it/s, Train Loss=5.7667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.0193, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0309, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000:  50%|█████     | 3/6 [00:01<00:01,  2.01it/s, Train Loss=5.7336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1299, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.7709, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=6.1299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.4765, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.3251, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.2197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.8498, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.9727, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 19/1000:\n",
      "  Train Loss: 5.7680\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_19.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.6079, device='cuda:0', grad_fn=<SumBackward0>) tensor(24.3148, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.83it/s, Train Loss=6.7637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.9142, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.3143, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.83it/s, Train Loss=5.6627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.7891, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2400, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000:  50%|█████     | 3/6 [00:01<00:01,  2.10it/s, Train Loss=5.7795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5288, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.4596, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.10it/s, Train Loss=6.0223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9196, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.4577, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.1620]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.8992, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.6190, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 20/1000:\n",
      "  Train Loss: 5.6717\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_20.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.4107, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3382, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.23it/s, Train Loss=5.8753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.6886, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.7521, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.13it/s, Train Loss=6.4790]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.5899, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.5745, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000:  50%|█████     | 3/6 [00:01<00:01,  2.20it/s, Train Loss=5.8362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.4883, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.2560, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.2058]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.2551, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.2593, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=3.8709]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7432, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.9252, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 21/1000:\n",
      "  Train Loss: 5.6077\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_21.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3113, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.3086, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.07it/s, Train Loss=6.0724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.1935, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.7845, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.97it/s, Train Loss=6.1012]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.0599, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.0942, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000:  50%|█████     | 3/6 [00:01<00:01,  2.16it/s, Train Loss=5.5057]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.4980, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.7045, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.16it/s, Train Loss=5.7550]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5297, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.6407, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=3.7837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.0005, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.6647, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 22/1000:\n",
      "  Train Loss: 5.4918\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_22.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.0238, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.0028, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.01it/s, Train Loss=5.9672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.8616, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.1504, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.00it/s, Train Loss=6.3910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8495, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.6656, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000:  50%|█████     | 3/6 [00:01<00:01,  2.07it/s, Train Loss=5.9680]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8895, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2152, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.09it/s, Train Loss=5.7455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7029, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.4284, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.8997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.7888, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.0778, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 23/1000:\n",
      "  Train Loss: 5.8603\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_23.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.1213, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0918, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.24it/s, Train Loss=5.8441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.3823, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.0886, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.15it/s, Train Loss=5.4009]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5515, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.5713, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:  50%|█████     | 3/6 [00:01<00:01,  2.11it/s, Train Loss=5.4180]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6832, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.9493, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=5.8015]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.0369, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.2295, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.5341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.5564, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.9873, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 24/1000:\n",
      "  Train Loss: 5.4538\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_24.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5799, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0233, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.03it/s, Train Loss=5.7527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.4512, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.4825, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.07it/s, Train Loss=5.2092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.2724, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.6973, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000:  50%|█████     | 3/6 [00:01<00:01,  2.03it/s, Train Loss=6.2220]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.7091, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2644, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000:  67%|██████▋   | 4/6 [00:01<00:01,  1.98it/s, Train Loss=5.7434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6446, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.7861, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.4513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.7961, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.7945, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 25/1000:\n",
      "  Train Loss: 5.5987\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_25.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.1829, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.5765, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.23it/s, Train Loss=5.7141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.4279, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.1371, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:  33%|███▎      | 2/6 [00:00<00:02,  1.98it/s, Train Loss=5.7218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.6376, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.0386, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:  50%|█████     | 3/6 [00:01<00:01,  1.98it/s, Train Loss=6.4686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.5210, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.5634, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000:  67%|██████▋   | 4/6 [00:01<00:01,  1.99it/s, Train Loss=6.1633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.8503, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.4052, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.5834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.3791, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.8047, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 26/1000:\n",
      "  Train Loss: 5.8618\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_26.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.0989, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.7409, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.85it/s, Train Loss=5.2693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.1931, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.5218, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.04it/s, Train Loss=6.4213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.9847, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.3034, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=6.2844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9145, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2236, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.10it/s, Train Loss=5.7107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.5247, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2334, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.4216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3862, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.0138, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 27/1000:\n",
      "  Train Loss: 5.6519\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_27.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5527, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.5477, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.99it/s, Train Loss=5.8456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.8729, device='cuda:0', grad_fn=<SumBackward0>) tensor(25.3822, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.11it/s, Train Loss=6.8683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.4713, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.5250, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000:  50%|█████     | 3/6 [00:01<00:01,  2.07it/s, Train Loss=6.3885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.2026, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2447, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000:  67%|██████▋   | 4/6 [00:01<00:01,  1.97it/s, Train Loss=5.6858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.2283, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.2972, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.4479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.0717, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.3167, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 28/1000:\n",
      "  Train Loss: 5.9049\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_28.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.2776, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.1852, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.12it/s, Train Loss=5.3651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.7561, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.6715, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.11it/s, Train Loss=6.2343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.1298, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.7694, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000:  50%|█████     | 3/6 [00:01<00:01,  2.16it/s, Train Loss=6.3200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.2378, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.5958, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.25it/s, Train Loss=6.5584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.5247, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8355, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: 100%|██████████| 6/6 [00:02<00:00,  2.34it/s, Train Loss=4.4046]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.3728, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1190, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 29/1000:\n",
      "  Train Loss: 5.7508\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_29.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8920, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.7523, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.99it/s, Train Loss=6.1875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.2653, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.6771, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.25it/s, Train Loss=5.8676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.0097, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.2106, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000:  50%|█████     | 3/6 [00:01<00:01,  2.12it/s, Train Loss=5.5427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.0564, device='cuda:0', grad_fn=<SumBackward0>) tensor(27.1088, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.9649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.7972, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.9519, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: 100%|██████████| 6/6 [00:02<00:00,  2.28it/s, Train Loss=3.7665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4329, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.7812, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 30/1000:\n",
      "  Train Loss: 5.6396\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_30.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8244, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.5541, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.93it/s, Train Loss=5.3977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8662, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7658, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.06it/s, Train Loss=5.7926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.7261, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.6250, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000:  50%|█████     | 3/6 [00:01<00:01,  2.09it/s, Train Loss=5.8783]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7190, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.9969, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=6.1642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.0648, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.5904, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: 100%|██████████| 6/6 [00:02<00:00,  2.19it/s, Train Loss=4.2387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9096, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.3541, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 31/1000:\n",
      "  Train Loss: 5.6882\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_31.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6694, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.8203, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.17it/s, Train Loss=6.1824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3489, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.2560, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.17it/s, Train Loss=6.0364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.8871, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.1268, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000:  50%|█████     | 3/6 [00:01<00:01,  2.10it/s, Train Loss=5.8236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.8695, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.3215, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=6.0920]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3184, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.8262, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, Train Loss=5.2675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.8699, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.2198, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 32/1000:\n",
      "  Train Loss: 5.9011\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_32.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.3522, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.2052, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.92it/s, Train Loss=5.9983]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.8534, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.5774, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.02it/s, Train Loss=6.3001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1355, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.0778, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000:  50%|█████     | 3/6 [00:01<00:01,  2.02it/s, Train Loss=6.2666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.2803, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.0034, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000:  67%|██████▋   | 4/6 [00:02<00:01,  2.00it/s, Train Loss=6.4389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.6891, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0994, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.9209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.9709, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.8890, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 33/1000:\n",
      "  Train Loss: 5.7472\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_33.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1613, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.1570, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.97it/s, Train Loss=6.1552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.5707, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.0197, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.88it/s, Train Loss=6.3178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.5702, device='cuda:0', grad_fn=<SumBackward0>) tensor(23.1526, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000:  50%|█████     | 3/6 [00:01<00:01,  1.95it/s, Train Loss=6.6557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.1992, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.9760, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s, Train Loss=5.5270]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.6193, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.3280, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.5613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.6222, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.5453, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 34/1000:\n",
      "  Train Loss: 5.7350\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_34.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.0804, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8842, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.98it/s, Train Loss=5.7574]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.6537, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.4140, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.17it/s, Train Loss=5.7635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.1005, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.2978, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000:  50%|█████     | 3/6 [00:01<00:01,  2.12it/s, Train Loss=5.9234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.6037, device='cuda:0', grad_fn=<SumBackward0>) tensor(22.9401, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.02it/s, Train Loss=6.7330]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7715, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.9956, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, Train Loss=3.4923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.2424, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.2084, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 35/1000:\n",
      "  Train Loss: 5.6654\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_35.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.8353, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.8848, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.02it/s, Train Loss=6.1880]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.5590, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.9210, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:  33%|███▎      | 2/6 [00:01<00:02,  2.00it/s, Train Loss=6.3116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6738, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.7136, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:  50%|█████     | 3/6 [00:01<00:01,  1.96it/s, Train Loss=6.3609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.9411, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.4999, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.08it/s, Train Loss=5.8910]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5706, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3784, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000: 100%|██████████| 6/6 [00:02<00:00,  2.28it/s, Train Loss=4.5401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.0099, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.8020, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 36/1000:\n",
      "  Train Loss: 5.8460\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_36.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.5401, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.1657, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.15it/s, Train Loss=6.1333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5299, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.1451, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000:  33%|███▎      | 2/6 [00:00<00:02,  2.00it/s, Train Loss=5.4915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.2675, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.9995, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000:  50%|█████     | 3/6 [00:01<00:01,  2.02it/s, Train Loss=5.8972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.9006, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.8125, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s, Train Loss=6.4798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6559, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.0252, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s, Train Loss=3.7437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.5256, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.6662, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 37/1000:\n",
      "  Train Loss: 5.6777\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_37.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.9307, device='cuda:0', grad_fn=<SumBackward0>) tensor(22.5047, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.92it/s, Train Loss=6.6081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.6702, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.7737, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000:  33%|███▎      | 2/6 [00:01<00:01,  2.00it/s, Train Loss=6.3226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5842, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.4809, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000:  50%|█████     | 3/6 [00:01<00:01,  2.09it/s, Train Loss=5.1729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.5926, device='cuda:0', grad_fn=<SumBackward0>) tensor(23.6752, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.08it/s, Train Loss=6.8680]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.0189, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5476, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.8483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.7660, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.6748, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 38/1000:\n",
      "  Train Loss: 5.7258\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_38.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.3173, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.6900, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.09it/s, Train Loss=6.3792]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.8775, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.1681, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.96it/s, Train Loss=5.7761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.0014, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3069, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000:  50%|█████     | 3/6 [00:01<00:01,  1.90it/s, Train Loss=5.7928]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.3765, device='cuda:0', grad_fn=<SumBackward0>) tensor(22.6053, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.94it/s, Train Loss=6.6842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.1316, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.1455, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.8550]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2832, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.4000, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 39/1000:\n",
      "  Train Loss: 5.9665\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_39.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5355, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.6769, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.93it/s, Train Loss=6.6084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.4654, device='cuda:0', grad_fn=<SumBackward0>) tensor(22.1515, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.30it/s, Train Loss=6.6084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7654, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.9282, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000:  50%|█████     | 3/6 [00:01<00:01,  2.40it/s, Train Loss=6.0966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.6578, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7697, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.14it/s, Train Loss=5.8553]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.6496, device='cuda:0', grad_fn=<SumBackward0>) tensor(25.8217, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000: 100%|██████████| 6/6 [00:02<00:00,  2.37it/s, Train Loss=4.5218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.9236, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.6075, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 40/1000:\n",
      "  Train Loss: 5.9391\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_40.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.1710, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.9899, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.10it/s, Train Loss=5.2633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.7867, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.3271, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000:  33%|███▎      | 2/6 [00:00<00:02,  1.99it/s, Train Loss=5.7871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.5460, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.9986, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000:  50%|█████     | 3/6 [00:01<00:01,  1.94it/s, Train Loss=6.2240]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5845, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7209, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s, Train Loss=5.8142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5294, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.3667, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.0739]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.1572, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.8357, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 41/1000:\n",
      "  Train Loss: 5.4632\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_41.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.2280, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.6601, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.95it/s, Train Loss=5.6708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.9518, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.1422, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.95it/s, Train Loss=5.8623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.7315, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.3189, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=6.2496]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.4596, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.7267, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s, Train Loss=5.8193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8445, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.8101, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.2977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.7577, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.7634, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 42/1000:\n",
      "  Train Loss: 5.7014\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_42.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.5080, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.3240, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.05it/s, Train Loss=4.6930]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.0373, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.3358, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000:  33%|███▎      | 2/6 [00:00<00:02,  2.00it/s, Train Loss=6.0637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.6514, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.9545, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000:  50%|█████     | 3/6 [00:01<00:01,  1.93it/s, Train Loss=6.5158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.8719, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.6996, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=5.3360]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.6696, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.9153, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.0093]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.4238, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.3861, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 43/1000:\n",
      "  Train Loss: 5.6239\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_43.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5484, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.3036, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.04it/s, Train Loss=5.9765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.6784, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.0443, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.08it/s, Train Loss=6.5725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.3502, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.9285, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000:  50%|█████     | 3/6 [00:01<00:01,  2.08it/s, Train Loss=6.2447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.1597, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5985, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=5.6249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8282, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.6021, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.2284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9896, device='cuda:0', grad_fn=<SumBackward0>) tensor(2.5265, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 44/1000:\n",
      "  Train Loss: 5.6475\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_44.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.1777, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7848, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.86it/s, Train Loss=5.0138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2220, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.1916, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.84it/s, Train Loss=5.9545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5973, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.5906, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000:  50%|█████     | 3/6 [00:01<00:01,  2.00it/s, Train Loss=5.3446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6651, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.5759, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000:  67%|██████▋   | 4/6 [00:02<00:00,  2.04it/s, Train Loss=6.3528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2295, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.0178, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.0618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.5752, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.5245, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 45/1000:\n",
      "  Train Loss: 5.5982\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_45.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4990, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7675, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.85it/s, Train Loss=5.8152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.2871, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.3625, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.10it/s, Train Loss=5.1863]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.3569, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.6554, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=5.7647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.2465, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.7283, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s, Train Loss=5.5190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.4230, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.0211, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.1957]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.2886, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0815, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 46/1000:\n",
      "  Train Loss: 5.6063\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_46.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.1425, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.6914, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.21it/s, Train Loss=6.5471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6652, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.0215, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.07it/s, Train Loss=6.2291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4205, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.7557, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=5.8931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.2302, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.6865, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=4.9555]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.2957, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.7234, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=5.4732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.0481, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.8826, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 47/1000:\n",
      "  Train Loss: 5.8653\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_47.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.2921, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5836, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.98it/s, Train Loss=5.5898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.0128, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8298, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.00it/s, Train Loss=5.8481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4110, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.8527, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=5.8193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.1902, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2045, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.02it/s, Train Loss=5.7553]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.4827, device='cuda:0', grad_fn=<SumBackward0>) tensor(22.5571, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.3356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.4190, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.6912, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 48/1000:\n",
      "  Train Loss: 5.6722\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_48.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3610, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.4575, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.95it/s, Train Loss=5.9792]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2123, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.4939, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.03it/s, Train Loss=5.8164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.2882, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.5427, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:  50%|█████     | 3/6 [00:01<00:01,  2.11it/s, Train Loss=6.3948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.2831, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.4147, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.02it/s, Train Loss=6.3585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.4625, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.1752, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000: 100%|██████████| 6/6 [00:02<00:00,  2.19it/s, Train Loss=5.2975]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.1905, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.3427, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 49/1000:\n",
      "  Train Loss: 5.9622\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_49.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.4911, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.8772, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.08it/s, Train Loss=6.0477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.1844, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.3912, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.21it/s, Train Loss=5.5424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.3696, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.5085, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000:  50%|█████     | 3/6 [00:01<00:01,  2.14it/s, Train Loss=5.6616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2873, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.2275, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.05it/s, Train Loss=5.8808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.6885, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.1376, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.4257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3117, device='cuda:0', grad_fn=<SumBackward0>) tensor(2.9816, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 50/1000:\n",
      "  Train Loss: 5.4906\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_50.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.3999, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.8442, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.96it/s, Train Loss=6.6506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5543, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0441, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.05it/s, Train Loss=5.6811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.6079, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.4043, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000:  50%|█████     | 3/6 [00:01<00:01,  2.02it/s, Train Loss=6.3123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.2410, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.4563, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=5.5757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.7760, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.2428, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.1386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.9682, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.8359, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 51/1000:\n",
      "  Train Loss: 5.7169\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_51.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.4751, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.0826, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.91it/s, Train Loss=4.9178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.0094, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8542, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.03it/s, Train Loss=5.7171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.6460, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.8522, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000:  50%|█████     | 3/6 [00:01<00:01,  2.01it/s, Train Loss=6.6579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.7866, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.1310, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000:  67%|██████▋   | 4/6 [00:02<00:01,  2.00it/s, Train Loss=5.5259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.4530, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.7761, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.9909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.9607, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.1882, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 52/1000:\n",
      "  Train Loss: 5.6242\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_52.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.8272, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.9662, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.13it/s, Train Loss=6.4853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.6807, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.8601, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.04it/s, Train Loss=5.9446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.6276, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.8702, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000:  50%|█████     | 3/6 [00:01<00:01,  2.03it/s, Train Loss=6.3050]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9548, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.7921, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=5.3473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.1890, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.9041, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.0059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.7548, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.4760, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 53/1000:\n",
      "  Train Loss: 5.9360\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_53.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.7752, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.5470, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.88it/s, Train Loss=6.5781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.4749, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.6524, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000:  33%|███▎      | 2/6 [00:01<00:01,  2.02it/s, Train Loss=6.4587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.6858, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.9882, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=5.7299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37.9292, device='cuda:0', grad_fn=<SumBackward0>) tensor(24.1074, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.8182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.9701, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.5928, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000: 100%|██████████| 6/6 [00:02<00:00,  2.23it/s, Train Loss=3.4061]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8818, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.0508, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 54/1000:\n",
      "  Train Loss: 5.7765\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_54.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6300, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.9637, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.07it/s, Train Loss=6.1500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7256, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.9851, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.01it/s, Train Loss=6.3205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8926, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.3431, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000:  50%|█████     | 3/6 [00:01<00:01,  2.07it/s, Train Loss=6.2837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.4607, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.8952, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.01it/s, Train Loss=6.1825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.3815, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.0572, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.2135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.5318, device='cuda:0', grad_fn=<SumBackward0>) tensor(5.8617, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 55/1000:\n",
      "  Train Loss: 5.8442\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_55.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.7569, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.2264, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.38it/s, Train Loss=6.2726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.2916, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.9587, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.06it/s, Train Loss=5.6634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.3931, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.3932, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000:  50%|█████     | 3/6 [00:01<00:01,  2.07it/s, Train Loss=6.3027]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.2270, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.6396, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.08it/s, Train Loss=5.4606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.5175, device='cuda:0', grad_fn=<SumBackward0>) tensor(23.4359, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000: 100%|██████████| 6/6 [00:02<00:00,  2.30it/s, Train Loss=3.4828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7732, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.3306, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 56/1000:\n",
      "  Train Loss: 5.6646\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_56.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.4884, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.6996, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.84it/s, Train Loss=5.6469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.7013, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2733, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000:  33%|███▎      | 2/6 [00:01<00:01,  2.01it/s, Train Loss=5.7072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6527, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.6490, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000:  50%|█████     | 3/6 [00:01<00:01,  2.00it/s, Train Loss=6.0041]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.4809, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.8898, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s, Train Loss=6.2761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.5548, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.1117, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=3.4458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3995, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.0164, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 57/1000:\n",
      "  Train Loss: 5.5010\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_57.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.4767, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.4929, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.91it/s, Train Loss=6.2209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.8440, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.5922, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.92it/s, Train Loss=6.4445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.0425, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.7932, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000:  50%|█████     | 3/6 [00:01<00:01,  2.03it/s, Train Loss=6.1622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.4098, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.6388, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000:  67%|██████▋   | 4/6 [00:02<00:01,  2.00it/s, Train Loss=5.5996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.3897, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.0707, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.9148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.1309, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.4493, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 58/1000:\n",
      "  Train Loss: 5.9001\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_58.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.8675, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.0892, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.42it/s, Train Loss=6.3309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.7968, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.6220, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.20it/s, Train Loss=5.5523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.4457, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.2593, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000:  50%|█████     | 3/6 [00:01<00:01,  2.05it/s, Train Loss=6.6248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.8575, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.0039, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=6.1006]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.3119, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.1778, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=5.5307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.0753, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.5683, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 59/1000:\n",
      "  Train Loss: 5.9120\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_59.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.3754, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.2113, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.26it/s, Train Loss=5.7971]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3220, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.3874, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.36it/s, Train Loss=6.4179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3132, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.7721, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=5.9588]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.6969, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.9524, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.04it/s, Train Loss=6.5960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9620, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.4029, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.4744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.2304, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1657, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 60/1000:\n",
      "  Train Loss: 5.8288\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_60.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1737, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.2826, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.05it/s, Train Loss=6.1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6079, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.7680, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000:  33%|███▎      | 2/6 [00:00<00:02,  2.00it/s, Train Loss=6.2729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6674, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.8137, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000:  50%|█████     | 3/6 [00:01<00:01,  2.03it/s, Train Loss=5.7895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.0451, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.8014, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.01it/s, Train Loss=5.6395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.7950, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.2646, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.5419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.1103, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.2120, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 61/1000:\n",
      "  Train Loss: 5.7018\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_61.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.0854, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.1754, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.29it/s, Train Loss=6.2188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.0381, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.6783, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.10it/s, Train Loss=6.0510]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.5954, device='cuda:0', grad_fn=<SumBackward0>) tensor(8.1937, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000:  50%|█████     | 3/6 [00:01<00:01,  2.04it/s, Train Loss=5.3059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.8712, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.7957, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.00it/s, Train Loss=5.9120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.7128, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.7561, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=3.7995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.5697, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.2269, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 62/1000:\n",
      "  Train Loss: 5.5555\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_62.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.3450, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.9773, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.88it/s, Train Loss=5.7543]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22.9379, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.8895, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.04it/s, Train Loss=5.6084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.7427, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.4886, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000:  50%|█████     | 3/6 [00:01<00:01,  2.02it/s, Train Loss=6.3192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3385, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.9119, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.03it/s, Train Loss=5.8023]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.1147, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.4432, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.0505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.2950, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.3196, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 63/1000:\n",
      "  Train Loss: 5.6391\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_63.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.4735, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.1156, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.07it/s, Train Loss=6.1649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.3069, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.7648, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.16it/s, Train Loss=6.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.6124, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.6372, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000:  50%|█████     | 3/6 [00:01<00:01,  2.13it/s, Train Loss=5.7356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.8910, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.9179, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.06it/s, Train Loss=6.5805]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6765, device='cuda:0', grad_fn=<SumBackward0>) tensor(9.7868, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.4865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.4081, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1637, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 64/1000:\n",
      "  Train Loss: 5.7777\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_64.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6867, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.7338, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.91it/s, Train Loss=5.7834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2701, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.5729, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.88it/s, Train Loss=6.0557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5405, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.0343, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000:  50%|█████     | 3/6 [00:01<00:01,  1.91it/s, Train Loss=5.7167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.2748, device='cuda:0', grad_fn=<SumBackward0>) tensor(14.7652, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000:  67%|██████▋   | 4/6 [00:02<00:00,  2.00it/s, Train Loss=6.1971]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.2473, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.2033, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=4.4511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.6714, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7649, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 65/1000:\n",
      "  Train Loss: 5.7683\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_65.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.4047, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0386, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.88it/s, Train Loss=5.4594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.9356, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.3843, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.94it/s, Train Loss=6.2909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(35.4268, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.4389, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000:  50%|█████     | 3/6 [00:01<00:01,  1.90it/s, Train Loss=6.4262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.9361, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.5792, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s, Train Loss=5.8483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.7542, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.7035, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=3.8003]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2895, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.9607, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 66/1000:\n",
      "  Train Loss: 5.6393\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_66.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3764, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.0352, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.13it/s, Train Loss=6.4541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.1295, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.4763, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.00it/s, Train Loss=5.1528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.0606, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.5819, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000:  50%|█████     | 3/6 [00:01<00:01,  1.94it/s, Train Loss=5.9016]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.6041, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.3379, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s, Train Loss=6.1515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.5291, device='cuda:0', grad_fn=<SumBackward0>) tensor(20.2527, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=5.0629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.6760, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.0821, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 67/1000:\n",
      "  Train Loss: 5.8470\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_67.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7140, device='cuda:0', grad_fn=<SumBackward0>) tensor(19.9545, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.01it/s, Train Loss=6.4813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8116, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.8446, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000:  33%|███▎      | 2/6 [00:01<00:02,  1.93it/s, Train Loss=5.7227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.0713, device='cuda:0', grad_fn=<SumBackward0>) tensor(17.9077, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000:  50%|█████     | 3/6 [00:01<00:01,  2.01it/s, Train Loss=6.2550]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.1343, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.2051, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.08it/s, Train Loss=5.6237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.1262, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.2852, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=5.4255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.4756, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.9973, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 68/1000:\n",
      "  Train Loss: 5.9953\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_68.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.3898, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.8854, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000:  17%|█▋        | 1/6 [00:00<00:02,  1.98it/s, Train Loss=6.4162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8813, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.4715, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.11it/s, Train Loss=6.0307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1968, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.0129, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000:  50%|█████     | 3/6 [00:01<00:01,  2.04it/s, Train Loss=5.9400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.3464, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.8114, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000:  67%|██████▋   | 4/6 [00:01<00:00,  2.07it/s, Train Loss=6.2347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.2496, device='cuda:0', grad_fn=<SumBackward0>) tensor(13.3630, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000: 100%|██████████| 6/6 [00:02<00:00,  2.24it/s, Train Loss=4.1082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.5759, device='cuda:0', grad_fn=<SumBackward0>) tensor(4.8375, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 69/1000:\n",
      "  Train Loss: 5.7830\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_69.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1393, device='cuda:0', grad_fn=<SumBackward0>) tensor(15.0013, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000:  17%|█▋        | 1/6 [00:00<00:02,  2.24it/s, Train Loss=6.0802]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5638, device='cuda:0', grad_fn=<SumBackward0>) tensor(18.0486, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000:  33%|███▎      | 2/6 [00:00<00:01,  2.10it/s, Train Loss=6.2096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.7440, device='cuda:0', grad_fn=<SumBackward0>) tensor(10.7228, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000:  50%|█████     | 3/6 [00:01<00:01,  1.99it/s, Train Loss=5.6587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.6893, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.9580, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000:  67%|██████▋   | 4/6 [00:02<00:01,  1.93it/s, Train Loss=5.8024]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.0724, device='cuda:0', grad_fn=<SumBackward0>) tensor(11.1882, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000: 100%|██████████| 6/6 [00:02<00:00,  2.20it/s, Train Loss=5.2389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2271, device='cuda:0', grad_fn=<SumBackward0>) tensor(12.3769, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5421, device='cuda:0') tensor(0.7446, device='cuda:0')\n",
      "tensor(1.2795, device='cuda:0') tensor(0.0933, device='cuda:0')\n",
      "Epoch 70/1000:\n",
      "  Train Loss: 5.7711\n",
      "  Val Loss: -0.3540\n",
      "  Val MSE: 0.0455\n",
      "  Val R2: -2.4513\n",
      "  Model saved as model_epoch_70.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000:   0%|          | 0/6 [00:00<?, ?it/s]/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.7124, device='cuda:0', grad_fn=<SumBackward0>) tensor(16.6125, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted. Saving current model state...\n",
      "Model saved as interrupted_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize model with the given hyperparameters\n",
    "model = MetaLayerGNN(\n",
    "    in_channels=normalized_data_list[0].num_node_features,\n",
    "    hidden_channels=64,  # Given hidden_channels\n",
    "    out_channels=2,  # Predicting both mean and std\n",
    "    num_layers=3,  # Given num_layers\n",
    "    dropout_rate=0.4131546144018554,  # Given dropout_rate\n",
    "    activation='LeakyReLU',  # Given activation function\n",
    "    skip_connection=True,  # Given skip_connection\n",
    "    self_loops=False  # Given self_loops\n",
    ").to(device)\n",
    "\n",
    "# Train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=1000, load_best_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   5%|▍         | 1/22 [00:00<00:06,  3.25it/s, Train Loss=2.0137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6595, grad_fn=<SumBackward0>) tensor(4.5140, grad_fn=<SumBackward0>)\n",
      "tensor(15.0023, grad_fn=<SumBackward0>) tensor(6.8163, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   9%|▉         | 2/22 [00:00<00:07,  2.75it/s, Train Loss=4.6275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7748, grad_fn=<SumBackward0>) tensor(0.2002, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  14%|█▎        | 3/22 [00:01<00:08,  2.34it/s, Train Loss=-1.0346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2914, grad_fn=<SumBackward0>) tensor(0.0928, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  18%|█▊        | 4/22 [00:01<00:07,  2.45it/s, Train Loss=-2.1212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0357, grad_fn=<SumBackward0>) tensor(0.4721, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  23%|██▎       | 5/22 [00:02<00:07,  2.31it/s, Train Loss=-0.0397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2104, grad_fn=<SumBackward0>) tensor(0.0560, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  27%|██▋       | 6/22 [00:02<00:07,  2.24it/s, Train Loss=-2.6919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3061, grad_fn=<SumBackward0>) tensor(0.0921, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  32%|███▏      | 7/22 [00:02<00:06,  2.23it/s, Train Loss=-2.1179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9942, grad_fn=<SumBackward0>) tensor(0.1018, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  41%|████      | 9/22 [00:03<00:05,  2.50it/s, Train Loss=-3.0382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1765, grad_fn=<SumBackward0>) tensor(0.0407, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.81it/s, Train Loss=-3.1030]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8669, grad_fn=<SumBackward0>) tensor(0.0518, grad_fn=<SumBackward0>)\n",
      "tensor(0.8149, grad_fn=<SumBackward0>) tensor(0.0367, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  50%|█████     | 11/22 [00:04<00:04,  2.70it/s, Train Loss=-3.5088]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5610, grad_fn=<SumBackward0>) tensor(0.0266, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  55%|█████▍    | 12/22 [00:04<00:04,  2.35it/s, Train Loss=-4.2032]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5070, grad_fn=<SumBackward0>) tensor(0.0179, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.29it/s, Train Loss=-4.7024]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7710, grad_fn=<SumBackward0>) tensor(0.0225, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.44it/s, Train Loss=-4.8482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6660, grad_fn=<SumBackward0>) tensor(0.0118, grad_fn=<SumBackward0>)\n",
      "tensor(0.3641, grad_fn=<SumBackward0>) tensor(0.0056, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.40it/s, Train Loss=-6.1888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7184, grad_fn=<SumBackward0>) tensor(0.0474, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  77%|███████▋  | 17/22 [00:07<00:02,  2.40it/s, Train Loss=-3.3808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5031, grad_fn=<SumBackward0>) tensor(0.0149, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.40it/s, Train Loss=-4.8922]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9283, grad_fn=<SumBackward0>) tensor(0.0828, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  86%|████████▋ | 19/22 [00:08<00:01,  2.15it/s, Train Loss=-2.5657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8180, grad_fn=<SumBackward0>) tensor(0.0325, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.14it/s, Train Loss=-3.6264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6365, grad_fn=<SumBackward0>) tensor(0.0189, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 22/22 [00:09<00:00,  2.37it/s, Train Loss=-4.1440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6484, grad_fn=<SumBackward0>) tensor(0.0245, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4298) tensor(0.0046)\n",
      "tensor(0.4304) tensor(0.0060)\n",
      "tensor(0.4550) tensor(0.0067)\n",
      "tensor(0.2604) tensor(0.0027)\n",
      "tensor(0.1893) tensor(0.0023)\n",
      "Epoch 1/1000:\n",
      "  Train Loss: -2.7423\n",
      "  Val Loss: -6.6011\n",
      "  Val MSE: 0.0118\n",
      "  Val R2: 0.0459\n",
      "  Model saved as model_epoch_1.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8281, grad_fn=<SumBackward0>) tensor(0.0274, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:   5%|▍         | 1/22 [00:00<00:09,  2.15it/s, Train Loss=-3.7848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6088, grad_fn=<SumBackward0>) tensor(0.0249, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:   9%|▉         | 2/22 [00:00<00:08,  2.28it/s, Train Loss=-4.1874]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3461, grad_fn=<SumBackward0>) tensor(0.0088, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  14%|█▎        | 3/22 [00:01<00:07,  2.39it/s, Train Loss=-5.7928]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5836, grad_fn=<SumBackward0>) tensor(0.0141, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  18%|█▊        | 4/22 [00:01<00:07,  2.34it/s, Train Loss=-4.7996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4459, grad_fn=<SumBackward0>) tensor(0.0124, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.64it/s, Train Loss=-3.7743]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7335, grad_fn=<SumBackward0>) tensor(0.0313, grad_fn=<SumBackward0>)\n",
      "tensor(0.5374, grad_fn=<SumBackward0>) tensor(0.0103, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.65it/s, Train Loss=-5.0075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6056, grad_fn=<SumBackward0>) tensor(0.0110, grad_fn=<SumBackward0>)\n",
      "tensor(0.7276, grad_fn=<SumBackward0>) tensor(0.0322, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  41%|████      | 9/22 [00:03<00:05,  2.42it/s, Train Loss=-3.7535]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4735, grad_fn=<SumBackward0>) tensor(0.0166, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  50%|█████     | 11/22 [00:04<00:04,  2.71it/s, Train Loss=-4.9276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5850, grad_fn=<SumBackward0>) tensor(0.0124, grad_fn=<SumBackward0>)\n",
      "tensor(0.5335, grad_fn=<SumBackward0>) tensor(0.0160, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  55%|█████▍    | 12/22 [00:04<00:03,  2.73it/s, Train Loss=-4.7626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5370, grad_fn=<SumBackward0>) tensor(0.0130, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.66it/s, Train Loss=-4.9660]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4294, grad_fn=<SumBackward0>) tensor(0.0133, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  68%|██████▊   | 15/22 [00:05<00:02,  2.78it/s, Train Loss=-5.6507]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3879, grad_fn=<SumBackward0>) tensor(0.0091, grad_fn=<SumBackward0>)\n",
      "tensor(0.6225, grad_fn=<SumBackward0>) tensor(0.0143, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.61it/s, Train Loss=-4.7185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5147, grad_fn=<SumBackward0>) tensor(0.0108, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  77%|███████▋  | 17/22 [00:07<00:02,  2.46it/s, Train Loss=-5.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5521, grad_fn=<SumBackward0>) tensor(0.0112, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.56it/s, Train Loss=-5.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3573, grad_fn=<SumBackward0>) tensor(0.0097, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.36it/s, Train Loss=-5.6652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4464, grad_fn=<SumBackward0>) tensor(0.0122, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.31it/s, Train Loss=-5.2124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5069, grad_fn=<SumBackward0>) tensor(0.0097, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.07it/s, Train Loss=-5.3115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4275, grad_fn=<SumBackward0>) tensor(0.0069, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, Train Loss=-5.8194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4110) tensor(0.0041)\n",
      "tensor(0.4091) tensor(0.0040)\n",
      "tensor(0.3994) tensor(0.0050)\n",
      "tensor(0.3007) tensor(0.0026)\n",
      "tensor(0.1650) tensor(0.0019)\n",
      "Epoch 2/1000:\n",
      "  Train Loss: -4.9465\n",
      "  Val Loss: -6.8585\n",
      "  Val MSE: 0.0112\n",
      "  Val R2: 0.0890\n",
      "  Model saved as model_epoch_2.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5403, grad_fn=<SumBackward0>) tensor(0.0064, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:   5%|▍         | 1/22 [00:00<00:06,  3.14it/s, Train Loss=-5.6719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5841, grad_fn=<SumBackward0>) tensor(0.0138, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:   9%|▉         | 2/22 [00:00<00:08,  2.48it/s, Train Loss=-4.8217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4385, grad_fn=<SumBackward0>) tensor(0.0066, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  18%|█▊        | 4/22 [00:01<00:06,  2.59it/s, Train Loss=-4.0587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5927, grad_fn=<SumBackward0>) tensor(0.0291, grad_fn=<SumBackward0>)\n",
      "tensor(0.4962, grad_fn=<SumBackward0>) tensor(0.0059, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  27%|██▋       | 6/22 [00:02<00:05,  2.72it/s, Train Loss=-3.4258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8529, grad_fn=<SumBackward0>) tensor(0.0381, grad_fn=<SumBackward0>)\n",
      "tensor(0.5000, grad_fn=<SumBackward0>) tensor(0.0102, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  36%|███▋      | 8/22 [00:02<00:04,  2.80it/s, Train Loss=-5.5399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4085, grad_fn=<SumBackward0>) tensor(0.0096, grad_fn=<SumBackward0>)\n",
      "tensor(0.2950, grad_fn=<SumBackward0>) tensor(0.0039, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  41%|████      | 9/22 [00:03<00:04,  2.60it/s, Train Loss=-6.7707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5365, grad_fn=<SumBackward0>) tensor(0.0108, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  45%|████▌     | 10/22 [00:03<00:05,  2.38it/s, Train Loss=-5.1528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4688, grad_fn=<SumBackward0>) tensor(0.0085, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  50%|█████     | 11/22 [00:04<00:04,  2.36it/s, Train Loss=-5.5210]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2767, grad_fn=<SumBackward0>) tensor(0.0061, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  55%|█████▍    | 12/22 [00:04<00:04,  2.26it/s, Train Loss=-6.3780]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5004, grad_fn=<SumBackward0>) tensor(0.0160, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  59%|█████▉    | 13/22 [00:05<00:04,  2.24it/s, Train Loss=-4.8305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5081, grad_fn=<SumBackward0>) tensor(0.0074, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.25it/s, Train Loss=-5.5895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3865, grad_fn=<SumBackward0>) tensor(0.0087, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  68%|██████▊   | 15/22 [00:06<00:03,  2.16it/s, Train Loss=-5.6963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6530, grad_fn=<SumBackward0>) tensor(0.0185, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.20it/s, Train Loss=-4.4158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3860, grad_fn=<SumBackward0>) tensor(0.0124, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  77%|███████▋  | 17/22 [00:07<00:02,  2.26it/s, Train Loss=-5.3420]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3578, grad_fn=<SumBackward0>) tensor(0.0057, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.22it/s, Train Loss=-6.1930]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2719, grad_fn=<SumBackward0>) tensor(0.0073, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.24it/s, Train Loss=-6.2213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3769, grad_fn=<SumBackward0>) tensor(0.0084, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.35it/s, Train Loss=-5.7509]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4737, grad_fn=<SumBackward0>) tensor(0.0120, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 22/22 [00:09<00:00,  2.40it/s, Train Loss=-6.7076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3542, grad_fn=<SumBackward0>) tensor(0.0034, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3943) tensor(0.0045)\n",
      "tensor(0.3941) tensor(0.0044)\n",
      "tensor(0.3673) tensor(0.0052)\n",
      "tensor(0.3005) tensor(0.0021)\n",
      "tensor(0.1616) tensor(0.0014)\n",
      "Epoch 3/1000:\n",
      "  Train Loss: -5.4641\n",
      "  Val Loss: -6.9485\n",
      "  Val MSE: 0.0108\n",
      "  Val R2: 0.1254\n",
      "  Model saved as model_epoch_3.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3244, grad_fn=<SumBackward0>) tensor(0.0060, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:   5%|▍         | 1/22 [00:00<00:07,  2.64it/s, Train Loss=-6.2453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5483, grad_fn=<SumBackward0>) tensor(0.0098, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:   9%|▉         | 2/22 [00:00<00:08,  2.42it/s, Train Loss=-5.2304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4270, grad_fn=<SumBackward0>) tensor(0.0084, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  14%|█▎        | 3/22 [00:01<00:08,  2.35it/s, Train Loss=-5.6295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3310, grad_fn=<SumBackward0>) tensor(0.0038, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  23%|██▎       | 5/22 [00:01<00:06,  2.63it/s, Train Loss=-6.9113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2738, grad_fn=<SumBackward0>) tensor(0.0036, grad_fn=<SumBackward0>)\n",
      "tensor(0.2828, grad_fn=<SumBackward0>) tensor(0.0037, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.72it/s, Train Loss=-5.0674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4932, grad_fn=<SumBackward0>) tensor(0.0128, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.76it/s, Train Loss=-5.3386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4723, grad_fn=<SumBackward0>) tensor(0.0102, grad_fn=<SumBackward0>)\n",
      "tensor(0.5506, grad_fn=<SumBackward0>) tensor(0.0114, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  41%|████      | 9/22 [00:03<00:05,  2.55it/s, Train Loss=-6.2528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3469, grad_fn=<SumBackward0>) tensor(0.0055, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.62it/s, Train Loss=-6.2528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4071, grad_fn=<SumBackward0>) tensor(0.0069, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  50%|█████     | 11/22 [00:04<00:04,  2.65it/s, Train Loss=-5.8705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3021, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  55%|█████▍    | 12/22 [00:04<00:03,  2.53it/s, Train Loss=-6.6696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5830, grad_fn=<SumBackward0>) tensor(0.0222, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.49it/s, Train Loss=-4.3451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4630, grad_fn=<SumBackward0>) tensor(0.0095, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.54it/s, Train Loss=-5.4231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3104, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  68%|██████▊   | 15/22 [00:05<00:03,  2.32it/s, Train Loss=-6.6458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4040, grad_fn=<SumBackward0>) tensor(0.0089, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.18it/s, Train Loss=-5.6308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4314, grad_fn=<SumBackward0>) tensor(0.0127, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  77%|███████▋  | 17/22 [00:06<00:02,  2.29it/s, Train Loss=-5.2086]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9697, grad_fn=<SumBackward0>) tensor(0.0349, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.29it/s, Train Loss=-3.3855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3350, grad_fn=<SumBackward0>) tensor(0.0114, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.16it/s, Train Loss=-5.5638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3242, grad_fn=<SumBackward0>) tensor(0.0040, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.22it/s, Train Loss=-6.6370]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3826, grad_fn=<SumBackward0>) tensor(0.0077, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, Train Loss=-8.0090]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2081, grad_fn=<SumBackward0>) tensor(0.0016, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5987) tensor(0.0183)\n",
      "tensor(0.6171) tensor(0.0152)\n",
      "tensor(0.4883) tensor(0.0142)\n",
      "tensor(0.6005) tensor(0.0120)\n",
      "tensor(0.2834) tensor(0.0051)\n",
      "Epoch 4/1000:\n",
      "  Train Loss: -5.8417\n",
      "  Val Loss: -5.1262\n",
      "  Val MSE: 0.0173\n",
      "  Val R2: -0.3991\n",
      "  Model saved as model_epoch_4.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6148, grad_fn=<SumBackward0>) tensor(0.0227, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:   5%|▍         | 1/22 [00:00<00:07,  2.77it/s, Train Loss=-4.2727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6497, grad_fn=<SumBackward0>) tensor(0.0223, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:   9%|▉         | 2/22 [00:00<00:07,  2.69it/s, Train Loss=-4.2336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6701, grad_fn=<SumBackward0>) tensor(0.0222, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  14%|█▎        | 3/22 [00:01<00:08,  2.30it/s, Train Loss=-4.2059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6467, grad_fn=<SumBackward0>) tensor(0.0221, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  23%|██▎       | 5/22 [00:02<00:06,  2.51it/s, Train Loss=-5.7692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4010, grad_fn=<SumBackward0>) tensor(0.0078, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  27%|██▋       | 6/22 [00:02<00:05,  2.69it/s, Train Loss=-6.7169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2768, grad_fn=<SumBackward0>) tensor(0.0044, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.83it/s, Train Loss=-5.0650]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4476, grad_fn=<SumBackward0>) tensor(0.0141, grad_fn=<SumBackward0>)\n",
      "tensor(0.6179, grad_fn=<SumBackward0>) tensor(0.0195, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.68it/s, Train Loss=-4.4207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3238, grad_fn=<SumBackward0>) tensor(0.0058, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  41%|████      | 9/22 [00:03<00:05,  2.55it/s, Train Loss=-6.2819]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2831, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  45%|████▌     | 10/22 [00:03<00:05,  2.39it/s, Train Loss=-6.5462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3916, grad_fn=<SumBackward0>) tensor(0.0073, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  50%|█████     | 11/22 [00:04<00:04,  2.47it/s, Train Loss=-5.8599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2945, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  55%|█████▍    | 12/22 [00:04<00:03,  2.54it/s, Train Loss=-6.5042]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4081, grad_fn=<SumBackward0>) tensor(0.0060, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.34it/s, Train Loss=-6.0135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4564, grad_fn=<SumBackward0>) tensor(0.0076, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  68%|██████▊   | 15/22 [00:05<00:02,  2.69it/s, Train Loss=-6.8610]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2602, grad_fn=<SumBackward0>) tensor(0.0040, grad_fn=<SumBackward0>)\n",
      "tensor(0.3285, grad_fn=<SumBackward0>) tensor(0.0079, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.52it/s, Train Loss=-5.9511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2749, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  77%|███████▋  | 17/22 [00:06<00:02,  2.45it/s, Train Loss=-7.3427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3420, grad_fn=<SumBackward0>) tensor(0.0055, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.53it/s, Train Loss=-6.2730]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3431, grad_fn=<SumBackward0>) tensor(0.0043, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.48it/s, Train Loss=-6.5255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3050, grad_fn=<SumBackward0>) tensor(0.0057, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000:  91%|█████████ | 20/22 [00:07<00:00,  2.46it/s, Train Loss=-6.3499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3891, grad_fn=<SumBackward0>) tensor(0.0060, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, Train Loss=-7.7451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2078, grad_fn=<SumBackward0>) tensor(0.0021, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3386) tensor(0.0031)\n",
      "tensor(0.3666) tensor(0.0045)\n",
      "tensor(0.3742) tensor(0.0060)\n",
      "tensor(0.2261) tensor(0.0018)\n",
      "tensor(0.1530) tensor(0.0015)\n",
      "Epoch 5/1000:\n",
      "  Train Loss: -5.8595\n",
      "  Val Loss: -7.1190\n",
      "  Val MSE: 0.0097\n",
      "  Val R2: 0.2115\n",
      "  Model saved as model_epoch_5.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4569, grad_fn=<SumBackward0>) tensor(0.0090, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:   9%|▉         | 2/22 [00:00<00:07,  2.60it/s, Train Loss=-5.5720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4614, grad_fn=<SumBackward0>) tensor(0.0082, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  14%|█▎        | 3/22 [00:01<00:07,  2.70it/s, Train Loss=-5.8694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3814, grad_fn=<SumBackward0>) tensor(0.0074, grad_fn=<SumBackward0>)\n",
      "tensor(0.2793, grad_fn=<SumBackward0>) tensor(0.0053, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  23%|██▎       | 5/22 [00:01<00:06,  2.60it/s, Train Loss=-6.3877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3854, grad_fn=<SumBackward0>) tensor(0.0044, grad_fn=<SumBackward0>)\n",
      "tensor(0.2910, grad_fn=<SumBackward0>) tensor(0.0033, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.65it/s, Train Loss=-6.9459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3074, grad_fn=<SumBackward0>) tensor(0.0064, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.56it/s, Train Loss=-6.2263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2791, grad_fn=<SumBackward0>) tensor(0.0029, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.53it/s, Train Loss=-7.1326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2104, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.54it/s, Train Loss=-7.2489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1858, grad_fn=<SumBackward0>) tensor(0.0038, grad_fn=<SumBackward0>)\n",
      "tensor(0.2513, grad_fn=<SumBackward0>) tensor(0.0036, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  50%|█████     | 11/22 [00:04<00:04,  2.40it/s, Train Loss=-6.9955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5133, grad_fn=<SumBackward0>) tensor(0.0123, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.60it/s, Train Loss=-4.0532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6955, grad_fn=<SumBackward0>) tensor(0.0250, grad_fn=<SumBackward0>)\n",
      "tensor(0.2755, grad_fn=<SumBackward0>) tensor(0.0036, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.39it/s, Train Loss=-6.9110]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3159, grad_fn=<SumBackward0>) tensor(0.0063, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.43it/s, Train Loss=-6.2160]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2831, grad_fn=<SumBackward0>) tensor(0.0055, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.47it/s, Train Loss=-6.4724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3115, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  77%|███████▋  | 17/22 [00:06<00:01,  2.52it/s, Train Loss=-6.5038]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3161, grad_fn=<SumBackward0>) tensor(0.0050, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.53it/s, Train Loss=-6.4589]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4134, grad_fn=<SumBackward0>) tensor(0.0076, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.41it/s, Train Loss=-5.7603]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3877, grad_fn=<SumBackward0>) tensor(0.0080, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.62it/s, Train Loss=-6.9644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.0032, grad_fn=<SumBackward0>)\n",
      "tensor(0.6717, grad_fn=<SumBackward0>) tensor(0.0262, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, Train Loss=-4.0414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4541) tensor(0.0107)\n",
      "tensor(0.5370) tensor(0.0188)\n",
      "tensor(0.5945) tensor(0.0205)\n",
      "tensor(0.2844) tensor(0.0058)\n",
      "tensor(0.2740) tensor(0.0063)\n",
      "Epoch 6/1000:\n",
      "  Train Loss: -6.1917\n",
      "  Val Loss: -5.4201\n",
      "  Val MSE: 0.0143\n",
      "  Val R2: -0.1591\n",
      "  Model saved as model_epoch_6.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6314, grad_fn=<SumBackward0>) tensor(0.0225, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:   9%|▉         | 2/22 [00:00<00:07,  2.76it/s, Train Loss=-5.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4693, grad_fn=<SumBackward0>) tensor(0.0133, grad_fn=<SumBackward0>)\n",
      "tensor(0.3811, grad_fn=<SumBackward0>) tensor(0.0068, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  18%|█▊        | 4/22 [00:01<00:06,  2.85it/s, Train Loss=-6.7610]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3535, grad_fn=<SumBackward0>) tensor(0.0033, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  23%|██▎       | 5/22 [00:01<00:05,  2.86it/s, Train Loss=-6.4833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3091, grad_fn=<SumBackward0>) tensor(0.0049, grad_fn=<SumBackward0>)\n",
      "tensor(0.3068, grad_fn=<SumBackward0>) tensor(0.0037, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.59it/s, Train Loss=-6.7770]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3293, grad_fn=<SumBackward0>) tensor(0.0045, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  32%|███▏      | 7/22 [00:02<00:06,  2.36it/s, Train Loss=-6.5128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2861, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  41%|████      | 9/22 [00:03<00:04,  2.67it/s, Train Loss=-7.1899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2842, grad_fn=<SumBackward0>) tensor(0.0027, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.72it/s, Train Loss=-5.7801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3901, grad_fn=<SumBackward0>) tensor(0.0079, grad_fn=<SumBackward0>)\n",
      "tensor(0.4313, grad_fn=<SumBackward0>) tensor(0.0082, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  50%|█████     | 11/22 [00:04<00:04,  2.64it/s, Train Loss=-5.6473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3461, grad_fn=<SumBackward0>) tensor(0.0041, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  59%|█████▉    | 13/22 [00:04<00:03,  2.66it/s, Train Loss=-6.2636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3205, grad_fn=<SumBackward0>) tensor(0.0059, grad_fn=<SumBackward0>)\n",
      "tensor(0.2431, grad_fn=<SumBackward0>) tensor(0.0026, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.51it/s, Train Loss=-7.3592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3552, grad_fn=<SumBackward0>) tensor(0.0075, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  68%|██████▊   | 15/22 [00:05<00:02,  2.45it/s, Train Loss=-5.9226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4075, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  77%|███████▋  | 17/22 [00:06<00:01,  2.57it/s, Train Loss=-5.8951]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4005, grad_fn=<SumBackward0>) tensor(0.0069, grad_fn=<SumBackward0>)\n",
      "tensor(0.2299, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  82%|████████▏ | 18/22 [00:06<00:01,  2.60it/s, Train Loss=-7.5165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3036, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.44it/s, Train Loss=-6.4657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2712, grad_fn=<SumBackward0>) tensor(0.0041, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  91%|█████████ | 20/22 [00:07<00:00,  2.37it/s, Train Loss=-6.7927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4677, grad_fn=<SumBackward0>) tensor(0.0085, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.30it/s, Train Loss=-5.5276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3215, grad_fn=<SumBackward0>) tensor(0.0069, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, Train Loss=-6.1057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3677) tensor(0.0064)\n",
      "tensor(0.5640) tensor(0.0198)\n",
      "tensor(0.4957) tensor(0.0137)\n",
      "tensor(0.2255) tensor(0.0041)\n",
      "tensor(0.2176) tensor(0.0041)\n",
      "Epoch 7/1000:\n",
      "  Train Loss: -6.2612\n",
      "  Val Loss: -5.9100\n",
      "  Val MSE: 0.0125\n",
      "  Val R2: -0.0112\n",
      "  Model saved as model_epoch_7.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2072, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:   5%|▍         | 1/22 [00:00<00:07,  2.71it/s, Train Loss=-6.9037]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3991, grad_fn=<SumBackward0>) tensor(0.0078, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  14%|█▎        | 3/22 [00:01<00:06,  3.13it/s, Train Loss=-7.1374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3091, grad_fn=<SumBackward0>) tensor(0.0026, grad_fn=<SumBackward0>)\n",
      "tensor(0.3547, grad_fn=<SumBackward0>) tensor(0.0045, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  18%|█▊        | 4/22 [00:01<00:06,  2.83it/s, Train Loss=-6.4442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3364, grad_fn=<SumBackward0>) tensor(0.0041, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  23%|██▎       | 5/22 [00:01<00:06,  2.55it/s, Train Loss=-6.5914]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2313, grad_fn=<SumBackward0>) tensor(0.0023, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.40it/s, Train Loss=-7.5567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2700, grad_fn=<SumBackward0>) tensor(0.0017, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  32%|███▏      | 7/22 [00:02<00:06,  2.31it/s, Train Loss=-7.6866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2296, grad_fn=<SumBackward0>) tensor(0.0022, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.39it/s, Train Loss=-7.5971]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3108, grad_fn=<SumBackward0>) tensor(0.0106, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  41%|████      | 9/22 [00:03<00:05,  2.47it/s, Train Loss=-5.7197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5082, grad_fn=<SumBackward0>) tensor(0.0192, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  45%|████▌     | 10/22 [00:04<00:05,  2.33it/s, Train Loss=-4.6288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3070, grad_fn=<SumBackward0>) tensor(0.0074, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  55%|█████▍    | 12/22 [00:04<00:03,  2.56it/s, Train Loss=-7.2006]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2763, grad_fn=<SumBackward0>) tensor(0.0027, grad_fn=<SumBackward0>)\n",
      "tensor(0.5735, grad_fn=<SumBackward0>) tensor(0.0138, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.61it/s, Train Loss=-4.8414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4740, grad_fn=<SumBackward0>) tensor(0.0115, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.59it/s, Train Loss=-5.2118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5944, grad_fn=<SumBackward0>) tensor(0.0178, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.39it/s, Train Loss=-4.5511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4482, grad_fn=<SumBackward0>) tensor(0.0072, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.38it/s, Train Loss=-5.7337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3810, grad_fn=<SumBackward0>) tensor(0.0081, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  77%|███████▋  | 17/22 [00:06<00:02,  2.39it/s, Train Loss=-5.7788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4841, grad_fn=<SumBackward0>) tensor(0.0102, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.57it/s, Train Loss=-7.3023]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2028, grad_fn=<SumBackward0>) tensor(0.0033, grad_fn=<SumBackward0>)\n",
      "tensor(0.6649, grad_fn=<SumBackward0>) tensor(0.0262, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.53it/s, Train Loss=-5.6179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2936, grad_fn=<SumBackward0>) tensor(0.0124, grad_fn=<SumBackward0>)\n",
      "tensor(0.5789, grad_fn=<SumBackward0>) tensor(0.0186, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, Train Loss=-4.5319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3549) tensor(0.0057)\n",
      "tensor(0.3855) tensor(0.0086)\n",
      "tensor(0.4790) tensor(0.0128)\n",
      "tensor(0.2343) tensor(0.0041)\n",
      "tensor(0.1906) tensor(0.0036)\n",
      "Epoch 8/1000:\n",
      "  Train Loss: -6.0114\n",
      "  Val Loss: -6.2519\n",
      "  Val MSE: 0.0110\n",
      "  Val R2: 0.1110\n",
      "  Model saved as model_epoch_8.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3821, grad_fn=<SumBackward0>) tensor(0.0084, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:   5%|▍         | 1/22 [00:00<00:09,  2.23it/s, Train Loss=-5.7438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3349, grad_fn=<SumBackward0>) tensor(0.0068, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  14%|█▎        | 3/22 [00:01<00:07,  2.63it/s, Train Loss=-5.3082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4851, grad_fn=<SumBackward0>) tensor(0.0102, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  18%|█▊        | 4/22 [00:01<00:06,  2.77it/s, Train Loss=-6.0597]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3517, grad_fn=<SumBackward0>) tensor(0.0066, grad_fn=<SumBackward0>)\n",
      "tensor(0.4103, grad_fn=<SumBackward0>) tensor(0.0107, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  23%|██▎       | 5/22 [00:02<00:07,  2.20it/s, Train Loss=-5.4265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4597, grad_fn=<SumBackward0>) tensor(0.0126, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  27%|██▋       | 6/22 [00:02<00:07,  2.20it/s, Train Loss=-5.1518]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5777, grad_fn=<SumBackward0>) tensor(0.0143, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.52it/s, Train Loss=-5.9369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3901, grad_fn=<SumBackward0>) tensor(0.0068, grad_fn=<SumBackward0>)\n",
      "tensor(0.4044, grad_fn=<SumBackward0>) tensor(0.0052, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  41%|████      | 9/22 [00:03<00:05,  2.49it/s, Train Loss=-6.1731]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3911, grad_fn=<SumBackward0>) tensor(0.0056, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  45%|████▌     | 10/22 [00:04<00:04,  2.42it/s, Train Loss=-6.1176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3102, grad_fn=<SumBackward0>) tensor(0.0054, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  50%|█████     | 11/22 [00:04<00:04,  2.45it/s, Train Loss=-6.4008]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2912, grad_fn=<SumBackward0>) tensor(0.0041, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  55%|█████▍    | 12/22 [00:04<00:04,  2.39it/s, Train Loss=-6.7373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4067, grad_fn=<SumBackward0>) tensor(0.0105, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.55it/s, Train Loss=-6.6261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3158, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n",
      "tensor(0.3051, grad_fn=<SumBackward0>) tensor(0.0034, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.48it/s, Train Loss=-6.8788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3484, grad_fn=<SumBackward0>) tensor(0.0050, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.46it/s, Train Loss=-6.3587]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3971, grad_fn=<SumBackward0>) tensor(0.0068, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  77%|███████▋  | 17/22 [00:07<00:02,  2.36it/s, Train Loss=-5.9190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3659, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.50it/s, Train Loss=-7.4094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2640, grad_fn=<SumBackward0>) tensor(0.0023, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.66it/s, Train Loss=-6.1753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3622, grad_fn=<SumBackward0>) tensor(0.0057, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.72it/s, Train Loss=-5.8815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4374, grad_fn=<SumBackward0>) tensor(0.0064, grad_fn=<SumBackward0>)\n",
      "tensor(0.3108, grad_fn=<SumBackward0>) tensor(0.0053, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, Train Loss=-6.4117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3369) tensor(0.0042)\n",
      "tensor(0.4087) tensor(0.0093)\n",
      "tensor(0.4421) tensor(0.0093)\n",
      "tensor(0.1985) tensor(0.0040)\n",
      "tensor(0.1859) tensor(0.0030)\n",
      "Epoch 9/1000:\n",
      "  Train Loss: -6.0635\n",
      "  Val Loss: -6.4499\n",
      "  Val MSE: 0.0105\n",
      "  Val R2: 0.1501\n",
      "  Model saved as model_epoch_9.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3914, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   5%|▍         | 1/22 [00:00<00:08,  2.43it/s, Train Loss=-6.2767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3633, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:   9%|▉         | 2/22 [00:00<00:08,  2.26it/s, Train Loss=-6.2890]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3339, grad_fn=<SumBackward0>) tensor(0.0052, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  14%|█▎        | 3/22 [00:01<00:08,  2.33it/s, Train Loss=-6.3557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2202, grad_fn=<SumBackward0>) tensor(0.0022, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  23%|██▎       | 5/22 [00:02<00:06,  2.57it/s, Train Loss=-6.6021]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3711, grad_fn=<SumBackward0>) tensor(0.0037, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  27%|██▋       | 6/22 [00:02<00:05,  2.71it/s, Train Loss=-6.5224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2850, grad_fn=<SumBackward0>) tensor(0.0052, grad_fn=<SumBackward0>)\n",
      "tensor(0.4841, grad_fn=<SumBackward0>) tensor(0.0093, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.71it/s, Train Loss=-5.4013]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2571, grad_fn=<SumBackward0>) tensor(0.0037, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  36%|███▋      | 8/22 [00:03<00:06,  2.18it/s, Train Loss=-6.9626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2963, grad_fn=<SumBackward0>) tensor(0.0041, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  45%|████▌     | 10/22 [00:04<00:04,  2.48it/s, Train Loss=-6.1720]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3587, grad_fn=<SumBackward0>) tensor(0.0058, grad_fn=<SumBackward0>)\n",
      "tensor(0.3450, grad_fn=<SumBackward0>) tensor(0.0032, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  50%|█████     | 11/22 [00:04<00:04,  2.40it/s, Train Loss=-6.8237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3323, grad_fn=<SumBackward0>) tensor(0.0063, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.54it/s, Train Loss=-6.2908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3297, grad_fn=<SumBackward0>) tensor(0.0056, grad_fn=<SumBackward0>)\n",
      "tensor(0.2659, grad_fn=<SumBackward0>) tensor(0.0050, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  64%|██████▎   | 14/22 [00:06<00:03,  2.49it/s, Train Loss=-7.1932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2560, grad_fn=<SumBackward0>) tensor(0.0029, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.58it/s, Train Loss=-7.1932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.51it/s, Train Loss=-6.5615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.0032, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  77%|███████▋  | 17/22 [00:06<00:01,  2.55it/s, Train Loss=-6.9697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2855, grad_fn=<SumBackward0>) tensor(0.0037, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.48it/s, Train Loss=-6.8500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3329, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.35it/s, Train Loss=-6.3747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3012, grad_fn=<SumBackward0>) tensor(0.0027, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000:  91%|█████████ | 20/22 [00:08<00:00,  2.39it/s, Train Loss=-7.1153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4107, grad_fn=<SumBackward0>) tensor(0.0070, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 22/22 [00:08<00:00,  2.49it/s, Train Loss=-6.7754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2718, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2846) tensor(0.0038)\n",
      "tensor(0.3465) tensor(0.0093)\n",
      "tensor(0.4049) tensor(0.0096)\n",
      "tensor(0.1543) tensor(0.0017)\n",
      "tensor(0.1893) tensor(0.0031)\n",
      "Epoch 10/1000:\n",
      "  Train Loss: -6.5701\n",
      "  Val Loss: -6.7565\n",
      "  Val MSE: 0.0092\n",
      "  Val R2: 0.2542\n",
      "  Model saved as model_epoch_10.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2811, grad_fn=<SumBackward0>) tensor(0.0053, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:   9%|▉         | 2/22 [00:00<00:06,  3.10it/s, Train Loss=-7.5885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2029, grad_fn=<SumBackward0>) tensor(0.0025, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  14%|█▎        | 3/22 [00:01<00:06,  2.98it/s, Train Loss=-7.5596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2468, grad_fn=<SumBackward0>) tensor(0.0021, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  18%|█▊        | 4/22 [00:01<00:05,  3.12it/s, Train Loss=-8.2174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1833, grad_fn=<SumBackward0>) tensor(0.0015, grad_fn=<SumBackward0>)\n",
      "tensor(0.2768, grad_fn=<SumBackward0>) tensor(0.0044, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  23%|██▎       | 5/22 [00:01<00:06,  2.74it/s, Train Loss=-6.7136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2790, grad_fn=<SumBackward0>) tensor(0.0054, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.65it/s, Train Loss=-6.4955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2758, grad_fn=<SumBackward0>) tensor(0.0018, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.56it/s, Train Loss=-7.5862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2699, grad_fn=<SumBackward0>) tensor(0.0059, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.45it/s, Train Loss=-6.4444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2562, grad_fn=<SumBackward0>) tensor(0.0018, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.61it/s, Train Loss=-7.0317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2338, grad_fn=<SumBackward0>) tensor(0.0038, grad_fn=<SumBackward0>)\n",
      "tensor(0.2977, grad_fn=<SumBackward0>) tensor(0.0040, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  50%|█████     | 11/22 [00:04<00:04,  2.37it/s, Train Loss=-6.7331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2488, grad_fn=<SumBackward0>) tensor(0.0036, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  55%|█████▍    | 12/22 [00:04<00:04,  2.41it/s, Train Loss=-7.0238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3129, grad_fn=<SumBackward0>) tensor(0.0049, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.49it/s, Train Loss=-6.4815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2608, grad_fn=<SumBackward0>) tensor(0.0031, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.39it/s, Train Loss=-7.1179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2049, grad_fn=<SumBackward0>) tensor(0.0021, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  68%|██████▊   | 15/22 [00:05<00:02,  2.36it/s, Train Loss=-7.7344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3188, grad_fn=<SumBackward0>) tensor(0.0050, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.24it/s, Train Loss=-6.4484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2512, grad_fn=<SumBackward0>) tensor(0.0055, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  77%|███████▋  | 17/22 [00:06<00:02,  2.32it/s, Train Loss=-6.5929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1925, grad_fn=<SumBackward0>) tensor(0.0020, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.53it/s, Train Loss=-6.4415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3504, grad_fn=<SumBackward0>) tensor(0.0045, grad_fn=<SumBackward0>)\n",
      "tensor(0.2482, grad_fn=<SumBackward0>) tensor(0.0029, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  91%|█████████ | 20/22 [00:07<00:00,  2.47it/s, Train Loss=-7.2447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2006, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.56it/s, Train Loss=-7.6594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1932, grad_fn=<SumBackward0>) tensor(0.0042, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 22/22 [00:08<00:00,  2.52it/s, Train Loss=-7.1087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3789) tensor(0.0077)\n",
      "tensor(0.3564) tensor(0.0092)\n",
      "tensor(0.5446) tensor(0.0203)\n",
      "tensor(0.2895) tensor(0.0039)\n",
      "tensor(0.2899) tensor(0.0069)\n",
      "Epoch 11/1000:\n",
      "  Train Loss: -7.1017\n",
      "  Val Loss: -5.8131\n",
      "  Val MSE: 0.0124\n",
      "  Val R2: -0.0052\n",
      "  Model saved as model_epoch_11.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3310, grad_fn=<SumBackward0>) tensor(0.0066, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:   5%|▍         | 1/22 [00:00<00:09,  2.25it/s, Train Loss=-6.1242]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3050, grad_fn=<SumBackward0>) tensor(0.0077, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:   9%|▉         | 2/22 [00:00<00:09,  2.09it/s, Train Loss=-6.0552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3201, grad_fn=<SumBackward0>) tensor(0.0050, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  14%|█▎        | 3/22 [00:01<00:08,  2.26it/s, Train Loss=-6.4409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2159, grad_fn=<SumBackward0>) tensor(0.0029, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  18%|█▊        | 4/22 [00:01<00:07,  2.43it/s, Train Loss=-7.3635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3285, grad_fn=<SumBackward0>) tensor(0.0073, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  27%|██▋       | 6/22 [00:02<00:05,  2.68it/s, Train Loss=-5.6789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3647, grad_fn=<SumBackward0>) tensor(0.0094, grad_fn=<SumBackward0>)\n",
      "tensor(0.5702, grad_fn=<SumBackward0>) tensor(0.0154, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.65it/s, Train Loss=-4.7151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5140, grad_fn=<SumBackward0>) tensor(0.0174, grad_fn=<SumBackward0>)\n",
      "tensor(0.3543, grad_fn=<SumBackward0>) tensor(0.0055, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  41%|████      | 9/22 [00:03<00:05,  2.41it/s, Train Loss=-6.2407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3740, grad_fn=<SumBackward0>) tensor(0.0106, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  45%|████▌     | 10/22 [00:04<00:05,  2.31it/s, Train Loss=-5.5322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2607, grad_fn=<SumBackward0>) tensor(0.0027, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  50%|█████     | 11/22 [00:04<00:04,  2.38it/s, Train Loss=-7.2606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3829, grad_fn=<SumBackward0>) tensor(0.0084, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  55%|█████▍    | 12/22 [00:04<00:04,  2.41it/s, Train Loss=-5.7407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3389, grad_fn=<SumBackward0>) tensor(0.0066, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.38it/s, Train Loss=-6.1039]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.0051, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  68%|██████▊   | 15/22 [00:06<00:02,  2.71it/s, Train Loss=-5.2826]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4375, grad_fn=<SumBackward0>) tensor(0.0116, grad_fn=<SumBackward0>)\n",
      "tensor(0.4795, grad_fn=<SumBackward0>) tensor(0.0097, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  77%|███████▋  | 17/22 [00:06<00:01,  2.94it/s, Train Loss=-5.7309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4040, grad_fn=<SumBackward0>) tensor(0.0080, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  82%|████████▏ | 18/22 [00:06<00:01,  3.08it/s, Train Loss=-6.3402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3314, grad_fn=<SumBackward0>) tensor(0.0053, grad_fn=<SumBackward0>)\n",
      "tensor(0.3199, grad_fn=<SumBackward0>) tensor(0.0038, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  86%|████████▋ | 19/22 [00:07<00:01,  2.70it/s, Train Loss=-6.7134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3165, grad_fn=<SumBackward0>) tensor(0.0039, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  91%|█████████ | 20/22 [00:07<00:00,  2.62it/s, Train Loss=-6.6887]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4719, grad_fn=<SumBackward0>) tensor(0.0097, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.46it/s, Train Loss=-4.9575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4919, grad_fn=<SumBackward0>) tensor(0.0143, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 22/22 [00:08<00:00,  2.54it/s, Train Loss=-4.9575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3971) tensor(0.0087)\n",
      "tensor(0.3681) tensor(0.0048)\n",
      "tensor(0.3542) tensor(0.0063)\n",
      "tensor(0.3442) tensor(0.0043)\n",
      "tensor(0.1842) tensor(0.0024)\n",
      "Epoch 12/1000:\n",
      "  Train Loss: -5.9553\n",
      "  Val Loss: -6.4637\n",
      "  Val MSE: 0.0110\n",
      "  Val R2: 0.1091\n",
      "  Model saved as model_epoch_12.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4895, grad_fn=<SumBackward0>) tensor(0.0090, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:   5%|▍         | 1/22 [00:00<00:08,  2.49it/s, Train Loss=-5.4214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4242, grad_fn=<SumBackward0>) tensor(0.0072, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:   9%|▉         | 2/22 [00:00<00:08,  2.42it/s, Train Loss=-5.7919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4542, grad_fn=<SumBackward0>) tensor(0.0081, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  14%|█▎        | 3/22 [00:01<00:07,  2.55it/s, Train Loss=-5.6090]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2162, grad_fn=<SumBackward0>) tensor(0.0027, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  18%|█▊        | 4/22 [00:01<00:08,  2.15it/s, Train Loss=-7.4294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3087, grad_fn=<SumBackward0>) tensor(0.0048, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  27%|██▋       | 6/22 [00:02<00:06,  2.41it/s, Train Loss=-7.7880]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2146, grad_fn=<SumBackward0>) tensor(0.0019, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  32%|███▏      | 7/22 [00:02<00:05,  2.63it/s, Train Loss=-7.6601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2001, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n",
      "tensor(0.1647, grad_fn=<SumBackward0>) tensor(0.0031, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  36%|███▋      | 8/22 [00:03<00:05,  2.47it/s, Train Loss=-7.5648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2343, grad_fn=<SumBackward0>) tensor(0.0020, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  45%|████▌     | 10/22 [00:03<00:04,  2.71it/s, Train Loss=-7.3456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2544, grad_fn=<SumBackward0>) tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "tensor(0.3011, grad_fn=<SumBackward0>) tensor(0.0033, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  50%|█████     | 11/22 [00:04<00:04,  2.63it/s, Train Loss=-6.9163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3094, grad_fn=<SumBackward0>) tensor(0.0061, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  55%|█████▍    | 12/22 [00:04<00:03,  2.63it/s, Train Loss=-6.2660]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2721, grad_fn=<SumBackward0>) tensor(0.0056, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  59%|█████▉    | 13/22 [00:05<00:03,  2.50it/s, Train Loss=-6.4905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2593, grad_fn=<SumBackward0>) tensor(0.0024, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  64%|██████▎   | 14/22 [00:05<00:03,  2.42it/s, Train Loss=-7.4020]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2760, grad_fn=<SumBackward0>) tensor(0.0030, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  73%|███████▎  | 16/22 [00:06<00:02,  2.56it/s, Train Loss=-7.2765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2751, grad_fn=<SumBackward0>) tensor(0.0025, grad_fn=<SumBackward0>)\n",
      "tensor(0.1636, grad_fn=<SumBackward0>) tensor(0.0017, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  77%|███████▋  | 17/22 [00:06<00:01,  2.60it/s, Train Loss=-8.2042]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4814, grad_fn=<SumBackward0>) tensor(0.0183, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  82%|████████▏ | 18/22 [00:07<00:01,  2.55it/s, Train Loss=-4.7332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1393, grad_fn=<SumBackward0>) tensor(0.0021, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  91%|█████████ | 20/22 [00:07<00:00,  2.73it/s, Train Loss=-6.7462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2384, grad_fn=<SumBackward0>) tensor(0.0049, grad_fn=<SumBackward0>)\n",
      "tensor(0.2347, grad_fn=<SumBackward0>) tensor(0.0088, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000:  95%|█████████▌| 21/22 [00:08<00:00,  2.54it/s, Train Loss=-6.1810]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3209, grad_fn=<SumBackward0>) tensor(0.0071, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 22/22 [00:08<00:00,  2.49it/s, Train Loss=-6.0839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2083) tensor(0.0018)\n",
      "tensor(0.1971) tensor(0.0023)\n",
      "tensor(0.2739) tensor(0.0039)\n",
      "tensor(0.1659) tensor(0.0027)\n",
      "tensor(0.1454) tensor(0.0013)\n",
      "Epoch 13/1000:\n",
      "  Train Loss: -6.8341\n",
      "  Val Loss: -7.7452\n",
      "  Val MSE: 0.0066\n",
      "  Val R2: 0.4645\n",
      "  Model saved as model_epoch_13.pth\n",
      "  New best model saved as best_model.pth!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4332, grad_fn=<SumBackward0>) tensor(0.0087, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:   5%|▍         | 1/22 [00:00<00:08,  2.37it/s, Train Loss=-5.5756]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3457, grad_fn=<SumBackward0>) tensor(0.0044, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000:   9%|▉         | 2/22 [00:00<00:08,  2.33it/s, Train Loss=-6.4981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted. Saving current model state...\n",
      "Model saved as interrupted_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "in_channels_dict = {\n",
    "    'A': normalized_data_list[0]['A'].num_features,\n",
    "    'B': normalized_data_list[0]['B'].num_features,\n",
    "    'C': normalized_data_list[0]['C'].num_features\n",
    "}\n",
    "hidden_channels = 64\n",
    "model = HeteroMetaLayerGNN(in_channels_dict, hidden_channels, out_channels=2, num_layers=4, \n",
    "                           dropout_rate=0.1, activation='ReLU', \n",
    "                           skip_connection=True, self_loops=True)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0011485487514082274,  # Given learning rate\n",
    "    weight_decay=0.00039972867471824516  # Given weight_decay\n",
    ")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1375872494500921,  # Given scheduler_factor\n",
    "    patience=11  # Given scheduler_patience\n",
    ")\n",
    "\n",
    "\n",
    "# Assuming you have your data split into train, val, and test sets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device('cpu')\n",
    "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomCosmoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCosmoLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Split the predictions into mean and standard deviation\n",
    "        mean_pred = pred[:, 0]\n",
    "        std_pred = pred[:, 1]\n",
    "\n",
    "        # Ensure the standard deviation is positive\n",
    "        std_pred = F.softplus(std_pred)\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse_loss = torch.sum((mean_pred - target) ** 2)\n",
    "\n",
    "        # Likelihood-free inference (LFI) loss\n",
    "        lfi_loss = torch.sum(((mean_pred - target) ** 2 - std_pred ** 2) ** 2)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = torch.log(mse_loss) + torch.log(lfi_loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def augment_data(data, jitter_strength, dropout_prob):\n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.x.shape) > dropout_prob\n",
    "        data.x = data.x * mask.float()\n",
    "    \n",
    "    data.x += torch.randn_like(data.x) * jitter_strength\n",
    "    \n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.edge_index.shape[1]) > dropout_prob\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, num_features, device, epochs, train_dataset, val_dataset):\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.best_trial_number = None\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        save_dir = '/scratch/gpfs/hk4638/astrid_optimization'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        fout = os.path.join(save_dir, f'loss_{trial.number}.txt')\n",
    "        fmodel = os.path.join(save_dir, f'model_{trial.number}.pth')\n",
    "        fhyper = os.path.join(save_dir, f'hyperparameters_{trial.number}.json')\n",
    "\n",
    "        # Suggest hyperparameters for MetaLayerGNN\n",
    "        hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128])\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "        activation = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'ELU'])\n",
    "        skip_connection = trial.suggest_categorical('skip_connection', [True, False])\n",
    "        grad_clip = trial.suggest_float('grad_clip', 0.5, 5.0)\n",
    "        self_loops = trial.suggest_categorical('self_loops', [True, False])\n",
    "\n",
    "        # Augmentation parameters\n",
    "        use_augmentation = trial.suggest_categorical('use_augmentation', [True, False])\n",
    "        if use_augmentation:\n",
    "            jitter_strength = trial.suggest_float('jitter_strength', 0.0, 0.1)\n",
    "            dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.2)\n",
    "        else:\n",
    "            jitter_strength = 0.0\n",
    "            dropout_prob = 0.0\n",
    "\n",
    "        hyperparameters = {\n",
    "            'hidden_channels': hidden_channels,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': 32,  # Fixed batch size\n",
    "            'optimizer': optimizer_name,\n",
    "            'activation': activation,\n",
    "            'skip_connection': skip_connection,\n",
    "            'grad_clip': grad_clip,\n",
    "            'self_loops': self_loops,\n",
    "            'use_augmentation': use_augmentation,\n",
    "            'jitter_strength': jitter_strength if use_augmentation else None,\n",
    "            'dropout_prob': dropout_prob if use_augmentation else None,\n",
    "        }\n",
    "        with open(fhyper, 'w') as f:\n",
    "            json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "        # Generate the model architecture with out_channels=2\n",
    "        model = MetaLayerGNN(in_channels=self.num_features, \n",
    "                             hidden_channels=hidden_channels,\n",
    "                             out_channels=2,  # output mean and std\n",
    "                             num_layers=num_layers, \n",
    "                             dropout_rate=dropout_rate,\n",
    "                             activation=activation,\n",
    "                             skip_connection=skip_connection,\n",
    "                             self_loops=self_loops).to(self.device)\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use CyclicLR scheduler\n",
    "        base_lr = lr / 10  # Setting base learning rate as one-tenth of the suggested lr\n",
    "        max_lr = lr  # Setting max learning rate as the suggested lr\n",
    "        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=self.epochs//4, mode='triangular', cycle_momentum = False)\n",
    "\n",
    "        # Custom criterion\n",
    "        criterion = CustomCosmoLoss()\n",
    "\n",
    "        def augment_batch(batch):\n",
    "            return augment_data(batch, jitter_strength, dropout_prob) if use_augmentation else batch\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=32, shuffle=True, collate_fn=augment_batch)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=32)\n",
    "\n",
    "        def train():\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            train_bar = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            for data in train_bar:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Model output (mean and std)\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            return total_loss / len(train_loader)\n",
    "\n",
    "        def validate(loader):\n",
    "            model.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in loader:\n",
    "                    data = data.to(self.device)\n",
    "\n",
    "                    # Model output (mean and std)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            return total_loss / len(loader)\n",
    "\n",
    "        trial_best_val_loss = float('inf')\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_loss = train()\n",
    "            val_loss = validate(val_loader)\n",
    "            \n",
    "            if val_loss < trial_best_val_loss:\n",
    "                trial_best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), fmodel)\n",
    "            \n",
    "            with open(fout, 'a') as f:\n",
    "                f.write(f'{epoch} {train_loss:.5e} {val_loss:.5e} {trial_best_val_loss:.5e}\\n')\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "            \n",
    "            # Only consider pruning after half the epochs are completed\n",
    "            if epoch >= self.epochs // 2 and trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if trial_best_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = trial_best_val_loss\n",
    "            self.best_trial_number = trial.number\n",
    "\n",
    "            with open(os.path.join(save_dir, 'best_trial.json'), 'w') as f:\n",
    "                json.dump({\n",
    "                    'best_trial_number': self.best_trial_number,\n",
    "                    'best_val_loss': self.best_val_loss\n",
    "                }, f, indent=4)\n",
    "\n",
    "        print(f\"\\n--- Trial {trial.number} Results ---\")\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\nCurrent Model Architecture:\\n{model}\")\n",
    "        print(f\"\\nBest Validation Loss for this trial: {trial_best_val_loss:.5e}\")\n",
    "        print(f\"Best Overall Validation Loss: {self.best_val_loss:.5e} (Trial {self.best_trial_number})\")\n",
    "\n",
    "        del model, optimizer, scheduler, criterion\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return trial_best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_data(data, jitter_strength, dropout_prob):\n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.x.shape) > dropout_prob\n",
    "        data.x = data.x * mask.float()\n",
    "    \n",
    "    data.x += torch.randn_like(data.x) * jitter_strength\n",
    "    \n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.edge_index.shape[1]) > dropout_prob\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, num_features, device, epochs, train_dataset, val_dataset):\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.best_trial_number = None\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        save_dir = '/scratch/gpfs/hk4638/astrid_optimization'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        fout = os.path.join(save_dir, f'loss_{trial.number}.txt')\n",
    "        fmodel = os.path.join(save_dir, f'model_{trial.number}.pth')\n",
    "        fhyper = os.path.join(save_dir, f'hyperparameters_{trial.number}.json')\n",
    "\n",
    "        # Suggest hyperparameters for EnhancedMetaLayerGNN\n",
    "        hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128])\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-8, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32])\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "        scheduler_factor = trial.suggest_float('scheduler_factor', 0.1, 0.5)\n",
    "        scheduler_patience = trial.suggest_int('scheduler_patience', 5, 15)\n",
    "        activation = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'ELU', 'GELU'])\n",
    "        skip_connection = trial.suggest_categorical('skip_connection', [True, False])\n",
    "        grad_clip = trial.suggest_float('grad_clip', 0.5, 5.0)\n",
    "        self_loops = trial.suggest_categorical('self_loops', [True, False])\n",
    "        num_heads = trial.suggest_int('num_heads', 1, 4)\n",
    "\n",
    "        # Augmentation parameters\n",
    "        use_augmentation = trial.suggest_categorical('use_augmentation', [True, False])\n",
    "        if use_augmentation:\n",
    "            jitter_strength = trial.suggest_float('jitter_strength', 0.0, 0.1)\n",
    "            dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.2)\n",
    "        else:\n",
    "            jitter_strength = 0.0\n",
    "            dropout_prob = 0.0\n",
    "\n",
    "        hyperparameters = {\n",
    "            'hidden_channels': hidden_channels,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': batch_size,\n",
    "            'optimizer': optimizer_name,\n",
    "            'scheduler_factor': scheduler_factor,\n",
    "            'scheduler_patience': scheduler_patience,\n",
    "            'activation': activation,\n",
    "            'skip_connection': skip_connection,\n",
    "            'grad_clip': grad_clip,\n",
    "            'self_loops': self_loops,\n",
    "            'num_heads': num_heads,\n",
    "            'use_augmentation': use_augmentation,\n",
    "            'jitter_strength': jitter_strength if use_augmentation else None,\n",
    "            'dropout_prob': dropout_prob if use_augmentation else None,\n",
    "        }\n",
    "        with open(fhyper, 'w') as f:\n",
    "            json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "        # Generate the model architecture\n",
    "        model = EnhancedMetaLayerGNN(\n",
    "            in_channels=self.num_features, \n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=1,\n",
    "            num_layers=num_layers, \n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=activation,\n",
    "            skip_connection=skip_connection,\n",
    "            self_loops=self_loops,\n",
    "            num_heads=num_heads\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use ReduceLROnPlateau scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience\n",
    "        )\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Update train and validation loaders with new batch size and augmentation\n",
    "        def augment_batch(batch):\n",
    "            return augment_data(batch, jitter_strength, dropout_prob) if use_augmentation else batch\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, collate_fn=augment_batch)\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=batch_size)\n",
    "\n",
    "        def train():\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            train_bar = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            for data in train_bar:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out.squeeze(), data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            return total_loss / len(train_loader)\n",
    "\n",
    "        def validate(loader):\n",
    "            model.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in loader:\n",
    "                    data = data.to(self.device)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out.squeeze(), data.y)\n",
    "                    total_loss += loss.item()\n",
    "            return total_loss / len(loader)\n",
    "\n",
    "        trial_best_val_loss = float('inf')\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_loss = train()\n",
    "            val_loss = validate(val_loader)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < trial_best_val_loss:\n",
    "                trial_best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), fmodel)\n",
    "            \n",
    "            with open(fout, 'a') as f:\n",
    "                f.write(f'{epoch} {train_loss:.5e} {val_loss:.5e} {trial_best_val_loss:.5e}\\n')\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "            \n",
    "            # Only consider pruning after half the epochs are completed\n",
    "            if epoch >= self.epochs // 2 and trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if trial_best_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = trial_best_val_loss\n",
    "            self.best_trial_number = trial.number\n",
    "\n",
    "            with open(os.path.join(save_dir, 'best_trial.json'), 'w') as f:\n",
    "                json.dump({\n",
    "                    'best_trial_number': self.best_trial_number,\n",
    "                    'best_val_loss': self.best_val_loss\n",
    "                }, f, indent=4)\n",
    "\n",
    "        print(f\"\\n--- Trial {trial.number} Results ---\")\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\nCurrent Model Architecture:\\n{model}\")\n",
    "        print(f\"\\nBest Validation Loss for this trial: {trial_best_val_loss:.5e}\")\n",
    "        print(f\"Best Overall Validation Loss: {self.best_val_loss:.5e} (Trial {self.best_trial_number})\")\n",
    "\n",
    "        del model, optimizer, scheduler, criterion\n",
    "        del train_loader, val_loader\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return trial_best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "class CustomCosmoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCosmoLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Split the predictions into mean and standard deviation\n",
    "        mean_pred = pred[:, 0]\n",
    "        std_pred = pred[:, 1]\n",
    "\n",
    "        # Ensure the standard deviation is positive\n",
    "        std_pred = F.softplus(std_pred)\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse_loss = torch.sum((mean_pred - target) ** 2)\n",
    "\n",
    "        # Likelihood-free inference (LFI) loss\n",
    "        lfi_loss = torch.sum(((mean_pred - target) ** 2 - std_pred ** 2) ** 2)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = torch.log(mse_loss) + torch.log(lfi_loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def augment_data(data, jitter_strength, dropout_prob):\n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.x.shape) > dropout_prob\n",
    "        data.x = data.x * mask.float()\n",
    "    \n",
    "    data.x += torch.randn_like(data.x) * jitter_strength\n",
    "    \n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.edge_index.shape[1]) > dropout_prob\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, num_features, device, epochs, directory):\n",
    "        self.num_features = num_features\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.directory = directory\n",
    "        self.best_trial_number = None\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        save_dir = '/scratch/gpfs/hk4638/astrid_optimization'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        fout = os.path.join(save_dir, f'loss_{trial.number}.txt')\n",
    "        fmodel = os.path.join(save_dir, f'model_{trial.number}.pth')\n",
    "        fhyper = os.path.join(save_dir, f'hyperparameters_{trial.number}.json')\n",
    "\n",
    "        # Suggest hyperparameters\n",
    "        hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128])\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "        activation = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'ELU'])\n",
    "        skip_connection = trial.suggest_categorical('skip_connection', [True, False])\n",
    "        grad_clip = trial.suggest_float('grad_clip', 0.5, 5.0)\n",
    "        self_loops = trial.suggest_categorical('self_loops', [True, False])\n",
    "\n",
    "        # New hyperparameter for k value\n",
    "        k_val = trial.suggest_int('k_val', 6, 16)\n",
    "\n",
    "        # Augmentation parameters\n",
    "        use_augmentation = trial.suggest_categorical('use_augmentation', [True, False])\n",
    "        if use_augmentation:\n",
    "            jitter_strength = trial.suggest_float('jitter_strength', 0.0, 0.1)\n",
    "            dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.2)\n",
    "        else:\n",
    "            jitter_strength = 0.0\n",
    "            dropout_prob = 0.0\n",
    "\n",
    "        hyperparameters = {\n",
    "            'hidden_channels': hidden_channels,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': 32,  # Fixed batch size\n",
    "            'optimizer': optimizer_name,\n",
    "            'activation': activation,\n",
    "            'skip_connection': skip_connection,\n",
    "            'grad_clip': grad_clip,\n",
    "            'self_loops': self_loops,\n",
    "            'k_val': k_val,\n",
    "            'use_augmentation': use_augmentation,\n",
    "            'jitter_strength': jitter_strength if use_augmentation else None,\n",
    "            'dropout_prob': dropout_prob if use_augmentation else None,\n",
    "        }\n",
    "        with open(fhyper, 'w') as f:\n",
    "            json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "        # Load and process data\n",
    "        data_list = load_all_graphs(self.directory, k_val=k_val)\n",
    "        x_params, edge_attr_params = calculate_normalization_params(data_list)\n",
    "        normalized_data_list = normalize_dataset(data_list, x_params, edge_attr_params)\n",
    "        normalized_data_list = [ensure_torch_float32(data) for data in normalized_data_list]\n",
    "\n",
    "        # Split data\n",
    "        total_len = len(normalized_data_list)\n",
    "        train_len = int(0.7 * total_len)\n",
    "        val_len = int(0.15 * total_len)\n",
    "        test_len = total_len - train_len - val_len\n",
    "        train_data, val_data, test_data = random_split(normalized_data_list, [train_len, val_len, test_len])\n",
    "\n",
    "        def augment_batch(batch):\n",
    "            return augment_data(batch, jitter_strength, dropout_prob) if use_augmentation else batch\n",
    "\n",
    "        # Create DataLoaders\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=augment_batch)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Generate the model architecture\n",
    "        model = MetaLayerGNN(in_channels=self.num_features, \n",
    "                             hidden_channels=hidden_channels,\n",
    "                             out_channels=2,  # output mean and std\n",
    "                             num_layers=num_layers, \n",
    "                             dropout_rate=dropout_rate,\n",
    "                             activation=activation,\n",
    "                             skip_connection=skip_connection,\n",
    "                             self_loops=self_loops).to(self.device)\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use CyclicLR scheduler\n",
    "        base_lr = lr / 10\n",
    "        max_lr = lr\n",
    "        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=self.epochs//4, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "        # Custom criterion\n",
    "        criterion = CustomCosmoLoss()\n",
    "\n",
    "\n",
    "\n",
    "        # Training and validation functions remain the same\n",
    "        def train():\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            train_bar = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            for data in train_bar:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "            return total_loss / len(train_loader)\n",
    "\n",
    "        def validate(loader):\n",
    "            model.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in loader:\n",
    "                    data = data.to(self.device)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y)\n",
    "                    total_loss += loss.item()\n",
    "            return total_loss / len(loader)\n",
    "\n",
    "        trial_best_val_loss = float('inf')\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_loss = train()\n",
    "            val_loss = validate(val_loader)\n",
    "            \n",
    "            if val_loss < trial_best_val_loss:\n",
    "                trial_best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), fmodel)\n",
    "            \n",
    "            with open(fout, 'a') as f:\n",
    "                f.write(f'{epoch} {train_loss:.5e} {val_loss:.5e} {trial_best_val_loss:.5e}\\n')\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "            \n",
    "            if epoch >= self.epochs // 2 and trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if trial_best_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = trial_best_val_loss\n",
    "            self.best_trial_number = trial.number\n",
    "\n",
    "            with open(os.path.join(save_dir, 'best_trial.json'), 'w') as f:\n",
    "                json.dump({\n",
    "                    'best_trial_number': self.best_trial_number,\n",
    "                    'best_val_loss': self.best_val_loss\n",
    "                }, f, indent=4)\n",
    "\n",
    "        print(f\"\\n--- Trial {trial.number} Results ---\")\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\nCurrent Model Architecture:\\n{model}\")\n",
    "        print(f\"\\nBest Validation Loss for this trial: {trial_best_val_loss:.5e}\")\n",
    "        print(f\"Best Overall Validation Loss: {self.best_val_loss:.5e} (Trial {self.best_trial_number})\")\n",
    "\n",
    "        del model, optimizer, scheduler, criterion\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return trial_best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hk4638/.conda/envs/com/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "class CustomCosmoLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCosmoLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Split the predictions into mean and standard deviation\n",
    "        mean_pred = pred[:, 0]\n",
    "        std_pred = pred[:, 1]\n",
    "\n",
    "        # Ensure the standard deviation is positive\n",
    "        std_pred = F.softplus(std_pred)\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse_loss = torch.sum((mean_pred - target) ** 2)\n",
    "\n",
    "        # Likelihood-free inference (LFI) loss\n",
    "        lfi_loss = torch.sum(((mean_pred - target) ** 2 - std_pred ** 2) ** 2)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = torch.log(mse_loss) + torch.log(lfi_loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "def augment_data(data, jitter_strength, dropout_prob):\n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.x.shape) > dropout_prob\n",
    "        data.x = data.x * mask.float()\n",
    "    \n",
    "    data.x += torch.randn_like(data.x) * jitter_strength\n",
    "    \n",
    "    if torch.rand(1) < dropout_prob:\n",
    "        mask = torch.rand(data.edge_index.shape[1]) > dropout_prob\n",
    "        data.edge_index = data.edge_index[:, mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[mask]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, in_channels_dict, device, epochs, directory):\n",
    "        self.in_channels_dict = in_channels_dict\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.directory = directory\n",
    "        self.best_trial_number = None\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        save_dir = '/scratch/gpfs/hk4638/astrid_optimization'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        fout = os.path.join(save_dir, f'loss_{trial.number}.txt')\n",
    "        fmodel = os.path.join(save_dir, f'model_{trial.number}.pth')\n",
    "        fhyper = os.path.join(save_dir, f'hyperparameters_{trial.number}.json')\n",
    "\n",
    "        # Suggest hyperparameters\n",
    "        hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128])\n",
    "        num_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "        activation = trial.suggest_categorical('activation', ['ReLU', 'LeakyReLU', 'ELU'])\n",
    "        skip_connection = trial.suggest_categorical('skip_connection', [True, False])\n",
    "        grad_clip = trial.suggest_float('grad_clip', 0.5, 5.0)\n",
    "        self_loops = trial.suggest_categorical('self_loops', [True, False])\n",
    "\n",
    "        # New hyperparameter for k value\n",
    "        k_val = trial.suggest_int('k_val', 6, 16)\n",
    "\n",
    "        # Augmentation parameters\n",
    "        use_augmentation = trial.suggest_categorical('use_augmentation', [True, False])\n",
    "        if use_augmentation:\n",
    "            jitter_strength = trial.suggest_float('jitter_strength', 0.0, 0.1)\n",
    "            dropout_prob = trial.suggest_float('dropout_prob', 0.0, 0.2)\n",
    "        else:\n",
    "            jitter_strength = 0.0\n",
    "            dropout_prob = 0.0\n",
    "\n",
    "        hyperparameters = {\n",
    "            'hidden_channels': hidden_channels,\n",
    "            'num_layers': num_layers,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': 32,  # Fixed batch size\n",
    "            'optimizer': optimizer_name,\n",
    "            'activation': activation,\n",
    "            'skip_connection': skip_connection,\n",
    "            'grad_clip': grad_clip,\n",
    "            'self_loops': self_loops,\n",
    "            'k_val': k_val,\n",
    "            'use_augmentation': use_augmentation,\n",
    "            'jitter_strength': jitter_strength if use_augmentation else None,\n",
    "            'dropout_prob': dropout_prob if use_augmentation else None,\n",
    "        }\n",
    "        with open(fhyper, 'w') as f:\n",
    "            json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "        # Load and process data\n",
    "        split_ratios = [0.3, 0.3, 0.4]  # Adjust split ratios if needed\n",
    "        data_list = load_all_heterogeneous_graphs(self.directory, split_ratios, k_val=k_val)\n",
    "        normalized_data_list, norm_params = normalize_dataset(data_list)\n",
    "        normalized_data_list = [ensure_torch_float32(data) for data in normalized_data_list]\n",
    "\n",
    "        # Split data\n",
    "        total_len = len(normalized_data_list)\n",
    "        train_len = int(0.7 * total_len)\n",
    "        val_len = int(0.15 * total_len)\n",
    "        test_len = total_len - train_len - val_len\n",
    "        train_data, val_data, test_data = random_split(normalized_data_list, [train_len, val_len, test_len])\n",
    "\n",
    "        def augment_batch(batch):\n",
    "            return augment_data(batch, jitter_strength, dropout_prob) if use_augmentation else batch\n",
    "\n",
    "        # Create DataLoaders\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=augment_batch)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Generate the model architecture\n",
    "        model = HeteroMetaLayerGNN(\n",
    "            in_channels_dict=self.in_channels_dict, \n",
    "            hidden_channels=hidden_channels,\n",
    "            out_channels=2,  # output mean and std\n",
    "            num_layers=num_layers, \n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=activation,\n",
    "            skip_connection=skip_connection,\n",
    "            self_loops=self_loops\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Choose optimizer\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Use CyclicLR scheduler\n",
    "        base_lr = lr / 10\n",
    "        max_lr = lr\n",
    "        scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size_up=self.epochs//4, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "        # Custom criterion\n",
    "        criterion = CustomCosmoLoss()\n",
    "\n",
    "        # Training and validation functions remain the same\n",
    "        def train():\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            train_bar = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            for data in train_bar:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = criterion(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "            return total_loss / len(train_loader)\n",
    "\n",
    "        def validate(loader):\n",
    "            model.eval()\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in loader:\n",
    "                    data = data.to(self.device)\n",
    "                    out = model(data)\n",
    "                    loss = criterion(out, data.y)\n",
    "                    total_loss += loss.item()\n",
    "            return total_loss / len(loader)\n",
    "\n",
    "        trial_best_val_loss = float('inf')\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            train_loss = train()\n",
    "            val_loss = validate(val_loader)\n",
    "            \n",
    "            if val_loss < trial_best_val_loss:\n",
    "                trial_best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), fmodel)\n",
    "            \n",
    "            with open(fout, 'a') as f:\n",
    "                f.write(f'{epoch} {train_loss:.5e} {val_loss:.5e} {trial_best_val_loss:.5e}\\n')\n",
    "\n",
    "            trial.report(val_loss, epoch)\n",
    "            \n",
    "            if epoch >= self.epochs // 2 and trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if trial_best_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = trial_best_val_loss\n",
    "            self.best_trial_number = trial.number\n",
    "\n",
    "            with open(os.path.join(save_dir, 'best_trial.json'), 'w') as f:\n",
    "                json.dump({\n",
    "                    'best_trial_number': self.best_trial_number,\n",
    "                    'best_val_loss': self.best_val_loss\n",
    "                }, f, indent=4)\n",
    "\n",
    "        print(f\"\\n--- Trial {trial.number} Results ---\")\n",
    "        print(\"Hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"\\nCurrent Model Architecture:\\n{model}\")\n",
    "        print(f\"\\nBest Validation Loss for this trial: {trial_best_val_loss:.5e}\")\n",
    "        print(f\"Best Overall Validation Loss: {self.best_val_loss:.5e} (Trial {self.best_trial_number})\")\n",
    "\n",
    "        del model, optimizer, scheduler, criterion\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return trial_best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-21 20:34:30,963] A new study created in RDB with name: heterotrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new study 'heterotrain'\n",
      "Running 100 additional trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading graphs: 100%|██████████| 1000/1000 [03:06<00:00,  5.37it/s]\n",
      "/tmp/ipykernel_3473201/3858287339.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[node_type].x = torch.tensor(data[node_type].x, dtype=torch.float32)\n",
      "/tmp/ipykernel_3473201/3858287339.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[edge_type].edge_attr = torch.tensor(data[edge_type].edge_attr, dtype=torch.float32)\n",
      "/tmp/ipykernel_3473201/3858287339.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data.y = torch.tensor(data.y, dtype=torch.float32)\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\n",
      "Epoch 1/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1/300:   5%|▍         | 1/22 [00:04<01:39,  4.76s/it]\u001b[A\n",
      "Epoch 1/300:  14%|█▎        | 3/22 [00:04<00:24,  1.28s/it]\u001b[A\n",
      "Epoch 1/300:  27%|██▋       | 6/22 [00:05<00:08,  1.92it/s]\u001b[A\n",
      "Epoch 1/300:  41%|████      | 9/22 [00:05<00:03,  3.39it/s]\u001b[A\n",
      "Epoch 1/300:  55%|█████▍    | 12/22 [00:05<00:01,  5.20it/s]\u001b[A\n",
      "Epoch 1/300:  68%|██████▊   | 15/22 [00:05<00:01,  6.26it/s]\u001b[A\n",
      "Epoch 1/300:  82%|████████▏ | 18/22 [00:05<00:00,  8.50it/s]\u001b[A\n",
      "Epoch 1/300:  95%|█████████▌| 21/22 [00:05<00:00, 10.96it/s]\u001b[A\n",
      "  0%|          | 1/300 [00:06<29:59,  6.02s/it]             \u001b[A\n",
      "Epoch 2/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2/300:  14%|█▎        | 3/22 [00:00<00:00, 28.79it/s]\u001b[A\n",
      "Epoch 2/300:  27%|██▋       | 6/22 [00:00<00:00, 27.51it/s]\u001b[A\n",
      "Epoch 2/300:  41%|████      | 9/22 [00:00<00:00, 28.05it/s]\u001b[A\n",
      "Epoch 2/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.88it/s]\u001b[A\n",
      "Epoch 2/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.48it/s]\u001b[A\n",
      "Epoch 2/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.22it/s]\u001b[A\n",
      "Epoch 2/300:  95%|█████████▌| 21/22 [00:00<00:00, 27.11it/s]\u001b[A\n",
      "  1%|          | 2/300 [00:06<15:02,  3.03s/it]             \u001b[A\n",
      "Epoch 3/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3/300:  14%|█▎        | 3/22 [00:00<00:00, 28.60it/s]\u001b[A\n",
      "Epoch 3/300:  27%|██▋       | 6/22 [00:00<00:00, 28.06it/s]\u001b[A\n",
      "Epoch 3/300:  41%|████      | 9/22 [00:00<00:00, 27.89it/s]\u001b[A\n",
      "Epoch 3/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.52it/s]\u001b[A\n",
      "Epoch 3/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.06it/s]\u001b[A\n",
      "Epoch 3/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.72it/s]\u001b[A\n",
      "Epoch 3/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.67it/s]\u001b[A\n",
      "  1%|          | 3/300 [00:07<10:19,  2.09s/it]             \u001b[A\n",
      "Epoch 4/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4/300:  14%|█▎        | 3/22 [00:00<00:00, 27.61it/s]\u001b[A\n",
      "Epoch 4/300:  27%|██▋       | 6/22 [00:00<00:00, 27.92it/s]\u001b[A\n",
      "Epoch 4/300:  41%|████      | 9/22 [00:00<00:00, 27.37it/s]\u001b[A\n",
      "Epoch 4/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.45it/s]\u001b[A\n",
      "Epoch 4/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.33it/s]\u001b[A\n",
      "Epoch 4/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.20it/s]\u001b[A\n",
      "Epoch 4/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.45it/s]\u001b[A\n",
      "  1%|▏         | 4/300 [00:08<08:04,  1.64s/it]             \u001b[A\n",
      "Epoch 5/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5/300:  14%|█▎        | 3/22 [00:00<00:00, 27.89it/s]\u001b[A\n",
      "Epoch 5/300:  27%|██▋       | 6/22 [00:00<00:00, 26.88it/s]\u001b[A\n",
      "Epoch 5/300:  41%|████      | 9/22 [00:00<00:00, 26.90it/s]\u001b[A\n",
      "Epoch 5/300:  55%|█████▍    | 12/22 [00:00<00:00, 26.93it/s]\u001b[A\n",
      "Epoch 5/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.65it/s]\u001b[A\n",
      "Epoch 5/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.37it/s]\u001b[A\n",
      "Epoch 5/300:  95%|█████████▌| 21/22 [00:00<00:00, 27.31it/s]\u001b[A\n",
      "  2%|▏         | 5/300 [00:09<06:48,  1.39s/it]             \u001b[A\n",
      "Epoch 6/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6/300:  14%|█▎        | 3/22 [00:00<00:00, 27.73it/s]\u001b[A\n",
      "Epoch 6/300:  27%|██▋       | 6/22 [00:00<00:00, 27.44it/s]\u001b[A\n",
      "Epoch 6/300:  41%|████      | 9/22 [00:00<00:00, 26.95it/s]\u001b[A\n",
      "Epoch 6/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.39it/s]\u001b[A\n",
      "Epoch 6/300:  68%|██████▊   | 15/22 [00:00<00:00, 26.92it/s]\u001b[A\n",
      "Epoch 6/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.13it/s]\u001b[A\n",
      "Epoch 6/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.64it/s]\u001b[A\n",
      "  2%|▏         | 6/300 [00:10<06:04,  1.24s/it]             \u001b[A\n",
      "Epoch 7/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7/300:  14%|█▎        | 3/22 [00:00<00:00, 26.78it/s]\u001b[A\n",
      "Epoch 7/300:  27%|██▋       | 6/22 [00:00<00:00, 28.10it/s]\u001b[A\n",
      "Epoch 7/300:  41%|████      | 9/22 [00:00<00:00, 27.45it/s]\u001b[A\n",
      "Epoch 7/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.31it/s]\u001b[A\n",
      "Epoch 7/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.71it/s]\u001b[A\n",
      "Epoch 7/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.99it/s]\u001b[A\n",
      "Epoch 7/300:  95%|█████████▌| 21/22 [00:00<00:00, 27.61it/s]\u001b[A\n",
      "  2%|▏         | 7/300 [00:11<05:33,  1.14s/it]             \u001b[A\n",
      "Epoch 8/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8/300:  14%|█▎        | 3/22 [00:00<00:00, 27.92it/s]\u001b[A\n",
      "Epoch 8/300:  27%|██▋       | 6/22 [00:00<00:00, 26.50it/s]\u001b[A\n",
      "Epoch 8/300:  41%|████      | 9/22 [00:00<00:00, 26.91it/s]\u001b[A\n",
      "Epoch 8/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.23it/s]\u001b[A\n",
      "Epoch 8/300:  68%|██████▊   | 15/22 [00:00<00:00, 26.95it/s]\u001b[A\n",
      "Epoch 8/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.54it/s]\u001b[A\n",
      "Epoch 8/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.81it/s]\u001b[A\n",
      "  3%|▎         | 8/300 [00:12<05:15,  1.08s/it]             \u001b[A\n",
      "Epoch 9/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9/300:  14%|█▎        | 3/22 [00:00<00:00, 29.15it/s]\u001b[A\n",
      "Epoch 9/300:  27%|██▋       | 6/22 [00:00<00:00, 27.77it/s]\u001b[A\n",
      "Epoch 9/300:  41%|████      | 9/22 [00:00<00:00, 27.76it/s]\u001b[A\n",
      "Epoch 9/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.02it/s]\u001b[A\n",
      "Epoch 9/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.12it/s]\u001b[A\n",
      "Epoch 9/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.69it/s]\u001b[A\n",
      "Epoch 9/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.69it/s]\u001b[A\n",
      "  3%|▎         | 9/300 [00:13<05:02,  1.04s/it]             \u001b[A\n",
      "Epoch 10/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10/300:  14%|█▎        | 3/22 [00:00<00:00, 27.37it/s]\u001b[A\n",
      "Epoch 10/300:  27%|██▋       | 6/22 [00:00<00:00, 27.68it/s]\u001b[A\n",
      "Epoch 10/300:  41%|████      | 9/22 [00:00<00:00, 27.51it/s]\u001b[A\n",
      "Epoch 10/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.29it/s]\u001b[A\n",
      "Epoch 10/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.28it/s]\u001b[A\n",
      "Epoch 10/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.85it/s]\u001b[A\n",
      "Epoch 10/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.98it/s]\u001b[A\n",
      "  3%|▎         | 10/300 [00:14<04:53,  1.01s/it]             \u001b[A\n",
      "Epoch 11/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11/300:  14%|█▎        | 3/22 [00:00<00:00, 27.85it/s]\u001b[A\n",
      "Epoch 11/300:  27%|██▋       | 6/22 [00:00<00:00, 26.97it/s]\u001b[A\n",
      "Epoch 11/300:  41%|████      | 9/22 [00:00<00:00, 26.88it/s]\u001b[A\n",
      "Epoch 11/300:  55%|█████▍    | 12/22 [00:00<00:00, 26.60it/s]\u001b[A\n",
      "Epoch 11/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.17it/s]\u001b[A\n",
      "Epoch 11/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.88it/s]\u001b[A\n",
      "Epoch 11/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.67it/s]\u001b[A\n",
      "  4%|▎         | 11/300 [00:15<04:47,  1.00it/s]             \u001b[A\n",
      "Epoch 12/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12/300:  14%|█▎        | 3/22 [00:00<00:00, 28.87it/s]\u001b[A\n",
      "Epoch 12/300:  27%|██▋       | 6/22 [00:00<00:00, 26.40it/s]\u001b[A\n",
      "Epoch 12/300:  41%|████      | 9/22 [00:00<00:00, 27.33it/s]\u001b[A\n",
      "Epoch 12/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.19it/s]\u001b[A\n",
      "Epoch 12/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.17it/s]\u001b[A\n",
      "Epoch 12/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.78it/s]\u001b[A\n",
      "Epoch 12/300:  95%|█████████▌| 21/22 [00:00<00:00, 27.37it/s]\u001b[A\n",
      "  4%|▍         | 12/300 [00:16<04:41,  1.02it/s]             \u001b[A\n",
      "Epoch 13/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13/300:  14%|█▎        | 3/22 [00:00<00:00, 26.46it/s]\u001b[A\n",
      "Epoch 13/300:  27%|██▋       | 6/22 [00:00<00:00, 26.60it/s]\u001b[A\n",
      "Epoch 13/300:  41%|████      | 9/22 [00:00<00:00, 26.93it/s]\u001b[A\n",
      "Epoch 13/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.32it/s]\u001b[A\n",
      "Epoch 13/300:  68%|██████▊   | 15/22 [00:00<00:00, 26.92it/s]\u001b[A\n",
      "Epoch 13/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.95it/s]\u001b[A\n",
      "Epoch 13/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.95it/s]\u001b[A\n",
      "  4%|▍         | 13/300 [00:17<04:38,  1.03it/s]             \u001b[A\n",
      "Epoch 14/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14/300:  14%|█▎        | 3/22 [00:00<00:00, 28.73it/s]\u001b[A\n",
      "Epoch 14/300:  27%|██▋       | 6/22 [00:00<00:00, 26.73it/s]\u001b[A\n",
      "Epoch 14/300:  41%|████      | 9/22 [00:00<00:00, 27.07it/s]\u001b[A\n",
      "Epoch 14/300:  55%|█████▍    | 12/22 [00:00<00:00, 27.02it/s]\u001b[A\n",
      "Epoch 14/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.04it/s]\u001b[A\n",
      "Epoch 14/300:  82%|████████▏ | 18/22 [00:00<00:00, 26.71it/s]\u001b[A\n",
      "Epoch 14/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.81it/s]\u001b[A\n",
      "  5%|▍         | 14/300 [00:18<04:35,  1.04it/s]             \u001b[A\n",
      "Epoch 15/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15/300:  14%|█▎        | 3/22 [00:00<00:00, 27.45it/s]\u001b[A\n",
      "Epoch 15/300:  27%|██▋       | 6/22 [00:00<00:00, 27.09it/s]\u001b[A\n",
      "Epoch 15/300:  41%|████      | 9/22 [00:00<00:00, 27.00it/s]\u001b[A\n",
      "Epoch 15/300:  55%|█████▍    | 12/22 [00:00<00:00, 26.93it/s]\u001b[A\n",
      "Epoch 15/300:  68%|██████▊   | 15/22 [00:00<00:00, 27.01it/s]\u001b[A\n",
      "Epoch 15/300:  82%|████████▏ | 18/22 [00:00<00:00, 27.53it/s]\u001b[A\n",
      "Epoch 15/300:  95%|█████████▌| 21/22 [00:00<00:00, 26.99it/s]\u001b[A\n",
      "  5%|▌         | 15/300 [00:19<04:33,  1.04it/s]             \u001b[A\n",
      "Epoch 16/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16/300:  14%|█▎        | 3/22 [00:00<00:00, 24.28it/s]\u001b[A\n",
      "Epoch 16/300:  27%|██▋       | 6/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 16/300:  41%|████      | 9/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 16/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 16/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 16/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "Epoch 16/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.44it/s]\u001b[A\n",
      "  5%|▌         | 16/300 [00:20<04:37,  1.02it/s]             \u001b[A\n",
      "Epoch 17/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17/300:  14%|█▎        | 3/22 [00:00<00:00, 25.65it/s]\u001b[A\n",
      "Epoch 17/300:  27%|██▋       | 6/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 17/300:  41%|████      | 9/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 17/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.47it/s]\u001b[A\n",
      "Epoch 17/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.44it/s]\u001b[A\n",
      "Epoch 17/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 17/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "  6%|▌         | 17/300 [00:21<04:38,  1.01it/s]             \u001b[A\n",
      "Epoch 18/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18/300:  14%|█▎        | 3/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 18/300:  27%|██▋       | 6/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 18/300:  41%|████      | 9/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 18/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 18/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 18/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 18/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "  6%|▌         | 18/300 [00:22<04:40,  1.00it/s]             \u001b[A\n",
      "Epoch 19/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19/300:  14%|█▎        | 3/22 [00:00<00:00, 25.64it/s]\u001b[A\n",
      "Epoch 19/300:  27%|██▋       | 6/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      "Epoch 19/300:  41%|████      | 9/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 19/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 19/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 19/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 19/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "  6%|▋         | 19/300 [00:23<04:41,  1.00s/it]             \u001b[A\n",
      "Epoch 20/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20/300:  14%|█▎        | 3/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 20/300:  27%|██▋       | 6/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 20/300:  41%|████      | 9/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 20/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 20/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 20/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 20/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      "  7%|▋         | 20/300 [00:24<04:41,  1.01s/it]             \u001b[A\n",
      "Epoch 21/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21/300:  14%|█▎        | 3/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 21/300:  27%|██▋       | 6/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 21/300:  41%|████      | 9/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 21/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 21/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 21/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 21/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "  7%|▋         | 21/300 [00:25<04:41,  1.01s/it]             \u001b[A\n",
      "Epoch 22/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22/300:  14%|█▎        | 3/22 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "Epoch 22/300:  27%|██▋       | 6/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 22/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 22/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 22/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 22/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 22/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "  7%|▋         | 22/300 [00:26<04:41,  1.01s/it]             \u001b[A\n",
      "Epoch 23/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23/300:  14%|█▎        | 3/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 23/300:  27%|██▋       | 6/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 23/300:  41%|████      | 9/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 23/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 23/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 23/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 23/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "  8%|▊         | 23/300 [00:27<04:41,  1.02s/it]             \u001b[A\n",
      "Epoch 24/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24/300:  14%|█▎        | 3/22 [00:00<00:00, 26.00it/s]\u001b[A\n",
      "Epoch 24/300:  27%|██▋       | 6/22 [00:00<00:00, 25.89it/s]\u001b[A\n",
      "Epoch 24/300:  41%|████      | 9/22 [00:00<00:00, 25.60it/s]\u001b[A\n",
      "Epoch 24/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 24/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.70it/s]\u001b[A\n",
      "Epoch 24/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.53it/s]\u001b[A\n",
      "Epoch 24/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "  8%|▊         | 24/300 [00:28<04:39,  1.01s/it]             \u001b[A\n",
      "Epoch 25/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25/300:  14%|█▎        | 3/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 25/300:  27%|██▋       | 6/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 25/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 25/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 25/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 25/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 25/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "  8%|▊         | 25/300 [00:29<04:38,  1.01s/it]             \u001b[A\n",
      "Epoch 26/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26/300:  14%|█▎        | 3/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 26/300:  27%|██▋       | 6/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 26/300:  41%|████      | 9/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 26/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 26/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "Epoch 26/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 26/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "  9%|▊         | 26/300 [00:30<04:36,  1.01s/it]             \u001b[A\n",
      "Epoch 27/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27/300:  14%|█▎        | 3/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 27/300:  27%|██▋       | 6/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 27/300:  41%|████      | 9/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 27/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 27/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 27/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 27/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "  9%|▉         | 27/300 [00:31<04:37,  1.02s/it]             \u001b[A\n",
      "Epoch 28/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28/300:  14%|█▎        | 3/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 28/300:  27%|██▋       | 6/22 [00:00<00:00, 25.80it/s]\u001b[A\n",
      "Epoch 28/300:  41%|████      | 9/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 28/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 28/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 28/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 28/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "  9%|▉         | 28/300 [00:32<04:35,  1.01s/it]             \u001b[A\n",
      "Epoch 29/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29/300:  14%|█▎        | 3/22 [00:00<00:00, 26.26it/s]\u001b[A\n",
      "Epoch 29/300:  27%|██▋       | 6/22 [00:00<00:00, 25.92it/s]\u001b[A\n",
      "Epoch 29/300:  41%|████      | 9/22 [00:00<00:00, 25.69it/s]\u001b[A\n",
      "Epoch 29/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 29/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 29/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 29/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      " 10%|▉         | 29/300 [00:33<04:34,  1.01s/it]             \u001b[A\n",
      "Epoch 30/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30/300:  14%|█▎        | 3/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 30/300:  27%|██▋       | 6/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 30/300:  41%|████      | 9/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 30/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 30/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 30/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 30/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      " 10%|█         | 30/300 [00:34<04:33,  1.01s/it]             \u001b[A\n",
      "Epoch 31/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31/300:  14%|█▎        | 3/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 31/300:  27%|██▋       | 6/22 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Epoch 31/300:  41%|████      | 9/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "Epoch 31/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 31/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 31/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 31/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      " 10%|█         | 31/300 [00:35<04:32,  1.01s/it]             \u001b[A\n",
      "Epoch 32/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32/300:  14%|█▎        | 3/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 32/300:  27%|██▋       | 6/22 [00:00<00:00, 25.79it/s]\u001b[A\n",
      "Epoch 32/300:  41%|████      | 9/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 32/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 32/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 32/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 32/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.43it/s]\u001b[A\n",
      " 11%|█         | 32/300 [00:36<04:30,  1.01s/it]             \u001b[A\n",
      "Epoch 33/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33/300:  14%|█▎        | 3/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 33/300:  27%|██▋       | 6/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 33/300:  41%|████      | 9/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 33/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 33/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 33/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 33/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      " 11%|█         | 33/300 [00:37<04:30,  1.01s/it]             \u001b[A\n",
      "Epoch 34/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34/300:  14%|█▎        | 3/22 [00:00<00:00, 26.54it/s]\u001b[A\n",
      "Epoch 34/300:  27%|██▋       | 6/22 [00:00<00:00, 26.13it/s]\u001b[A\n",
      "Epoch 34/300:  41%|████      | 9/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 34/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.57it/s]\u001b[A\n",
      "Epoch 34/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 34/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 34/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 11%|█▏        | 34/300 [00:38<04:28,  1.01s/it]             \u001b[A\n",
      "Epoch 35/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35/300:  14%|█▎        | 3/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 35/300:  27%|██▋       | 6/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 35/300:  41%|████      | 9/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 35/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 35/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 35/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 35/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      " 12%|█▏        | 35/300 [00:39<04:27,  1.01s/it]             \u001b[A\n",
      "Epoch 36/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36/300:  14%|█▎        | 3/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 36/300:  27%|██▋       | 6/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 36/300:  41%|████      | 9/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 36/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 36/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 36/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 36/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      " 12%|█▏        | 36/300 [00:40<04:26,  1.01s/it]             \u001b[A\n",
      "Epoch 37/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37/300:  14%|█▎        | 3/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 37/300:  27%|██▋       | 6/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 37/300:  41%|████      | 9/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 37/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 37/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 37/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 37/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      " 12%|█▏        | 37/300 [00:41<04:26,  1.01s/it]             \u001b[A\n",
      "Epoch 38/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38/300:  14%|█▎        | 3/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 38/300:  27%|██▋       | 6/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 38/300:  41%|████      | 9/22 [00:00<00:00, 25.64it/s]\u001b[A\n",
      "Epoch 38/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 38/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 38/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 38/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      " 13%|█▎        | 38/300 [00:42<04:25,  1.01s/it]             \u001b[A\n",
      "Epoch 39/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39/300:  14%|█▎        | 3/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 39/300:  27%|██▋       | 6/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 39/300:  41%|████      | 9/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 39/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 39/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 39/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 39/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      " 13%|█▎        | 39/300 [00:43<04:24,  1.01s/it]             \u001b[A\n",
      "Epoch 40/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40/300:  14%|█▎        | 3/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 40/300:  27%|██▋       | 6/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 40/300:  41%|████      | 9/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 40/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 40/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 40/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 40/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      " 13%|█▎        | 40/300 [00:44<04:23,  1.01s/it]             \u001b[A\n",
      "Epoch 41/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41/300:  14%|█▎        | 3/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 41/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 41/300:  41%|████      | 9/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 41/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 41/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 41/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 41/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      " 14%|█▎        | 41/300 [00:45<04:22,  1.01s/it]             \u001b[A\n",
      "Epoch 42/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42/300:  14%|█▎        | 3/22 [00:00<00:00, 26.10it/s]\u001b[A\n",
      "Epoch 42/300:  27%|██▋       | 6/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      "Epoch 42/300:  41%|████      | 9/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 42/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 42/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 42/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 42/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      " 14%|█▍        | 42/300 [00:46<04:21,  1.01s/it]             \u001b[A\n",
      "Epoch 43/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43/300:  14%|█▎        | 3/22 [00:00<00:00, 25.66it/s]\u001b[A\n",
      "Epoch 43/300:  27%|██▋       | 6/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 43/300:  41%|████      | 9/22 [00:00<00:00, 25.61it/s]\u001b[A\n",
      "Epoch 43/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 43/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 43/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 43/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 14%|█▍        | 43/300 [00:47<04:20,  1.01s/it]             \u001b[A\n",
      "Epoch 44/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44/300:  14%|█▎        | 3/22 [00:00<00:00, 25.55it/s]\u001b[A\n",
      "Epoch 44/300:  27%|██▋       | 6/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 44/300:  41%|████      | 9/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 44/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 44/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 44/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 44/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      " 15%|█▍        | 44/300 [00:48<04:19,  1.01s/it]             \u001b[A\n",
      "Epoch 45/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45/300:  14%|█▎        | 3/22 [00:00<00:00, 27.56it/s]\u001b[A\n",
      "Epoch 45/300:  27%|██▋       | 6/22 [00:00<00:00, 26.27it/s]\u001b[A\n",
      "Epoch 45/300:  41%|████      | 9/22 [00:00<00:00, 25.66it/s]\u001b[A\n",
      "Epoch 45/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 45/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 45/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 45/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      " 15%|█▌        | 45/300 [00:49<04:18,  1.01s/it]             \u001b[A\n",
      "Epoch 46/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46/300:  14%|█▎        | 3/22 [00:00<00:00, 24.11it/s]\u001b[A\n",
      "Epoch 46/300:  27%|██▋       | 6/22 [00:00<00:00, 24.15it/s]\u001b[A\n",
      "Epoch 46/300:  41%|████      | 9/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 46/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 46/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 46/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 46/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      " 15%|█▌        | 46/300 [00:50<04:17,  1.01s/it]             \u001b[A\n",
      "Epoch 47/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47/300:  14%|█▎        | 3/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 47/300:  27%|██▋       | 6/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 47/300:  41%|████      | 9/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 47/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 47/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 47/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 47/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      " 16%|█▌        | 47/300 [00:51<04:17,  1.02s/it]             \u001b[A\n",
      "Epoch 48/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48/300:  14%|█▎        | 3/22 [00:00<00:00, 25.78it/s]\u001b[A\n",
      "Epoch 48/300:  27%|██▋       | 6/22 [00:00<00:00, 25.58it/s]\u001b[A\n",
      "Epoch 48/300:  41%|████      | 9/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 48/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 48/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 48/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.32it/s]\u001b[A\n",
      "Epoch 48/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      " 16%|█▌        | 48/300 [00:52<04:17,  1.02s/it]             \u001b[A\n",
      "Epoch 49/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49/300:  14%|█▎        | 3/22 [00:00<00:00, 26.13it/s]\u001b[A\n",
      "Epoch 49/300:  27%|██▋       | 6/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 49/300:  41%|████      | 9/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 49/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 49/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 49/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 49/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      " 16%|█▋        | 49/300 [00:53<04:14,  1.02s/it]             \u001b[A\n",
      "Epoch 50/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50/300:  14%|█▎        | 3/22 [00:00<00:00, 26.28it/s]\u001b[A\n",
      "Epoch 50/300:  27%|██▋       | 6/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 50/300:  41%|████      | 9/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 50/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 50/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 50/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 50/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      " 17%|█▋        | 50/300 [00:54<04:13,  1.01s/it]             \u001b[A\n",
      "Epoch 51/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51/300:  14%|█▎        | 3/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 51/300:  27%|██▋       | 6/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 51/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 51/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 51/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 51/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 51/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      " 17%|█▋        | 51/300 [00:55<04:12,  1.01s/it]             \u001b[A\n",
      "Epoch 52/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52/300:  14%|█▎        | 3/22 [00:00<00:00, 25.83it/s]\u001b[A\n",
      "Epoch 52/300:  27%|██▋       | 6/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 52/300:  41%|████      | 9/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 52/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 52/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 52/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 52/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      " 17%|█▋        | 52/300 [00:56<04:11,  1.01s/it]             \u001b[A\n",
      "Epoch 53/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53/300:  14%|█▎        | 3/22 [00:00<00:00, 26.50it/s]\u001b[A\n",
      "Epoch 53/300:  27%|██▋       | 6/22 [00:00<00:00, 25.47it/s]\u001b[A\n",
      "Epoch 53/300:  41%|████      | 9/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 53/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 53/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 53/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 53/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      " 18%|█▊        | 53/300 [00:57<04:10,  1.01s/it]             \u001b[A\n",
      "Epoch 54/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54/300:  14%|█▎        | 3/22 [00:00<00:00, 25.60it/s]\u001b[A\n",
      "Epoch 54/300:  27%|██▋       | 6/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 54/300:  41%|████      | 9/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 54/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "Epoch 54/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.16it/s]\u001b[A\n",
      "Epoch 54/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.12it/s]\u001b[A\n",
      "Epoch 54/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.27it/s]\u001b[A\n",
      " 18%|█▊        | 54/300 [00:58<04:10,  1.02s/it]             \u001b[A\n",
      "Epoch 55/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55/300:  14%|█▎        | 3/22 [00:00<00:00, 25.79it/s]\u001b[A\n",
      "Epoch 55/300:  27%|██▋       | 6/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 55/300:  41%|████      | 9/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 55/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 55/300:  68%|██████▊   | 15/22 [00:00<00:00, 26.03it/s]\u001b[A\n",
      "Epoch 55/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 55/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      " 18%|█▊        | 55/300 [00:59<04:07,  1.01s/it]             \u001b[A\n",
      "Epoch 56/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56/300:  14%|█▎        | 3/22 [00:00<00:00, 26.11it/s]\u001b[A\n",
      "Epoch 56/300:  27%|██▋       | 6/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 56/300:  41%|████      | 9/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 56/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 56/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 56/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 56/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      " 19%|█▊        | 56/300 [01:00<04:06,  1.01s/it]             \u001b[A\n",
      "Epoch 57/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57/300:  14%|█▎        | 3/22 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "Epoch 57/300:  27%|██▋       | 6/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 57/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 57/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 57/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 57/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 57/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      " 19%|█▉        | 57/300 [01:01<04:05,  1.01s/it]             \u001b[A\n",
      "Epoch 58/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58/300:  14%|█▎        | 3/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 58/300:  27%|██▋       | 6/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 58/300:  41%|████      | 9/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 58/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 58/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 58/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 58/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      " 19%|█▉        | 58/300 [01:02<04:04,  1.01s/it]             \u001b[A\n",
      "Epoch 59/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59/300:  14%|█▎        | 3/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 59/300:  27%|██▋       | 6/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 59/300:  41%|████      | 9/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 59/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 59/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 59/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 59/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      " 20%|█▉        | 59/300 [01:03<04:04,  1.01s/it]             \u001b[A\n",
      "Epoch 60/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60/300:  14%|█▎        | 3/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 60/300:  27%|██▋       | 6/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 60/300:  41%|████      | 9/22 [00:00<00:00, 25.65it/s]\u001b[A\n",
      "Epoch 60/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 60/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 60/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 60/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 20%|██        | 60/300 [01:04<04:02,  1.01s/it]             \u001b[A\n",
      "Epoch 61/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61/300:  14%|█▎        | 3/22 [00:00<00:00, 26.87it/s]\u001b[A\n",
      "Epoch 61/300:  27%|██▋       | 6/22 [00:00<00:00, 25.92it/s]\u001b[A\n",
      "Epoch 61/300:  41%|████      | 9/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 61/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 61/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 61/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 61/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 20%|██        | 61/300 [01:05<04:00,  1.01s/it]             \u001b[A\n",
      "Epoch 62/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62/300:  14%|█▎        | 3/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 62/300:  27%|██▋       | 6/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 62/300:  41%|████      | 9/22 [00:00<00:00, 25.75it/s]\u001b[A\n",
      "Epoch 62/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 62/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 62/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 62/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      " 21%|██        | 62/300 [01:06<03:59,  1.01s/it]             \u001b[A\n",
      "Epoch 63/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63/300:  14%|█▎        | 3/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 63/300:  27%|██▋       | 6/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 63/300:  41%|████      | 9/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 63/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 63/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 63/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 63/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      " 21%|██        | 63/300 [01:07<03:59,  1.01s/it]             \u001b[A\n",
      "Epoch 64/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64/300:  14%|█▎        | 3/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 64/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 64/300:  41%|████      | 9/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 64/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 64/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.88it/s]\u001b[A\n",
      "Epoch 64/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.60it/s]\u001b[A\n",
      "Epoch 64/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      " 21%|██▏       | 64/300 [01:08<03:58,  1.01s/it]             \u001b[A\n",
      "Epoch 65/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65/300:  14%|█▎        | 3/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 65/300:  27%|██▋       | 6/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 65/300:  41%|████      | 9/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 65/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.05it/s]\u001b[A\n",
      "Epoch 65/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.19it/s]\u001b[A\n",
      "Epoch 65/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 65/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 22%|██▏       | 65/300 [01:09<03:58,  1.01s/it]             \u001b[A\n",
      "Epoch 66/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66/300:  14%|█▎        | 3/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 66/300:  27%|██▋       | 6/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 66/300:  41%|████      | 9/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 66/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 66/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 66/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 66/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      " 22%|██▏       | 66/300 [01:10<03:58,  1.02s/it]             \u001b[A\n",
      "Epoch 67/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67/300:  14%|█▎        | 3/22 [00:00<00:00, 26.33it/s]\u001b[A\n",
      "Epoch 67/300:  27%|██▋       | 6/22 [00:00<00:00, 25.71it/s]\u001b[A\n",
      "Epoch 67/300:  41%|████      | 9/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 67/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 67/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 67/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 67/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 22%|██▏       | 67/300 [01:12<03:57,  1.02s/it]             \u001b[A\n",
      "Epoch 68/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68/300:  14%|█▎        | 3/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 68/300:  27%|██▋       | 6/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 68/300:  41%|████      | 9/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 68/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 68/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 68/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 68/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      " 23%|██▎       | 68/300 [01:13<03:56,  1.02s/it]             \u001b[A\n",
      "Epoch 69/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69/300:  14%|█▎        | 3/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 69/300:  27%|██▋       | 6/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 69/300:  41%|████      | 9/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 69/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 69/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 69/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 69/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 23%|██▎       | 69/300 [01:14<03:55,  1.02s/it]             \u001b[A\n",
      "Epoch 70/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70/300:  14%|█▎        | 3/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 70/300:  27%|██▋       | 6/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 70/300:  41%|████      | 9/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 70/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 70/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 70/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 70/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 23%|██▎       | 70/300 [01:15<03:53,  1.02s/it]             \u001b[A\n",
      "Epoch 71/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71/300:  14%|█▎        | 3/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 71/300:  27%|██▋       | 6/22 [00:00<00:00, 24.08it/s]\u001b[A\n",
      "Epoch 71/300:  41%|████      | 9/22 [00:00<00:00, 23.79it/s]\u001b[A\n",
      "Epoch 71/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.27it/s]\u001b[A\n",
      "Epoch 71/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 71/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 71/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      " 24%|██▎       | 71/300 [01:16<03:52,  1.01s/it]             \u001b[A\n",
      "Epoch 72/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72/300:  14%|█▎        | 3/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 72/300:  27%|██▋       | 6/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 72/300:  41%|████      | 9/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 72/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 72/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 72/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 72/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      " 24%|██▍       | 72/300 [01:17<03:51,  1.02s/it]             \u001b[A\n",
      "Epoch 73/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73/300:  14%|█▎        | 3/22 [00:00<00:00, 26.94it/s]\u001b[A\n",
      "Epoch 73/300:  27%|██▋       | 6/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 73/300:  41%|████      | 9/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      "Epoch 73/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.96it/s]\u001b[A\n",
      "Epoch 73/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 73/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 73/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      " 24%|██▍       | 73/300 [01:18<03:50,  1.01s/it]             \u001b[A\n",
      "Epoch 74/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74/300:  14%|█▎        | 3/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 74/300:  27%|██▋       | 6/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 74/300:  41%|████      | 9/22 [00:00<00:00, 25.70it/s]\u001b[A\n",
      "Epoch 74/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 74/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 74/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 74/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      " 25%|██▍       | 74/300 [01:19<03:48,  1.01s/it]             \u001b[A\n",
      "Epoch 75/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75/300:  14%|█▎        | 3/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 75/300:  27%|██▋       | 6/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 75/300:  41%|████      | 9/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 75/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 75/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 75/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      "Epoch 75/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      " 25%|██▌       | 75/300 [01:20<03:47,  1.01s/it]             \u001b[A\n",
      "Epoch 76/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76/300:  14%|█▎        | 3/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 76/300:  27%|██▋       | 6/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 76/300:  41%|████      | 9/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 76/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 76/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 76/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 76/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      " 25%|██▌       | 76/300 [01:21<03:46,  1.01s/it]             \u001b[A\n",
      "Epoch 77/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77/300:  14%|█▎        | 3/22 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "Epoch 77/300:  27%|██▋       | 6/22 [00:00<00:00, 25.57it/s]\u001b[A\n",
      "Epoch 77/300:  41%|████      | 9/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 77/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 77/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 77/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 77/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      " 26%|██▌       | 77/300 [01:22<03:46,  1.01s/it]             \u001b[A\n",
      "Epoch 78/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78/300:  14%|█▎        | 3/22 [00:00<00:00, 24.20it/s]\u001b[A\n",
      "Epoch 78/300:  27%|██▋       | 6/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 78/300:  41%|████      | 9/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 78/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 78/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 78/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 78/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 26%|██▌       | 78/300 [01:23<03:46,  1.02s/it]             \u001b[A\n",
      "Epoch 79/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79/300:  14%|█▎        | 3/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 79/300:  27%|██▋       | 6/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 79/300:  41%|████      | 9/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 79/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 79/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 79/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 79/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      " 26%|██▋       | 79/300 [01:24<03:44,  1.02s/it]             \u001b[A\n",
      "Epoch 80/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80/300:  14%|█▎        | 3/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 80/300:  27%|██▋       | 6/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 80/300:  41%|████      | 9/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 80/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 80/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 80/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 80/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.91it/s]\u001b[A\n",
      " 27%|██▋       | 80/300 [01:25<03:42,  1.01s/it]             \u001b[A\n",
      "Epoch 81/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81/300:  14%|█▎        | 3/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 81/300:  27%|██▋       | 6/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 81/300:  41%|████      | 9/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 81/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 81/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 81/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 81/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 27%|██▋       | 81/300 [01:26<03:41,  1.01s/it]             \u001b[A\n",
      "Epoch 82/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82/300:  14%|█▎        | 3/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 82/300:  27%|██▋       | 6/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 82/300:  41%|████      | 9/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 82/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 82/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 82/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 82/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      " 27%|██▋       | 82/300 [01:27<03:40,  1.01s/it]             \u001b[A\n",
      "Epoch 83/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83/300:  14%|█▎        | 3/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 83/300:  27%|██▋       | 6/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 83/300:  41%|████      | 9/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 83/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 83/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 83/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 83/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      " 28%|██▊       | 83/300 [01:28<03:39,  1.01s/it]             \u001b[A\n",
      "Epoch 84/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84/300:  14%|█▎        | 3/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 84/300:  27%|██▋       | 6/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 84/300:  41%|████      | 9/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 84/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 84/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 84/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 84/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 28%|██▊       | 84/300 [01:29<03:39,  1.01s/it]             \u001b[A\n",
      "Epoch 85/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85/300:  14%|█▎        | 3/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "Epoch 85/300:  27%|██▋       | 6/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 85/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 85/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 85/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 85/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.58it/s]\u001b[A\n",
      "Epoch 85/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      " 28%|██▊       | 85/300 [01:30<03:37,  1.01s/it]             \u001b[A\n",
      "Epoch 86/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 86/300:  14%|█▎        | 3/22 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "Epoch 86/300:  27%|██▋       | 6/22 [00:00<00:00, 24.12it/s]\u001b[A\n",
      "Epoch 86/300:  41%|████      | 9/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 86/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 86/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 86/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 86/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      " 29%|██▊       | 86/300 [01:31<03:36,  1.01s/it]             \u001b[A\n",
      "Epoch 87/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87/300:  14%|█▎        | 3/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 87/300:  27%|██▋       | 6/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 87/300:  41%|████      | 9/22 [00:00<00:00, 24.16it/s]\u001b[A\n",
      "Epoch 87/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 87/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 87/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 87/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.48it/s]\u001b[A\n",
      " 29%|██▉       | 87/300 [01:32<03:35,  1.01s/it]             \u001b[A\n",
      "Epoch 88/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88/300:  14%|█▎        | 3/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 88/300:  27%|██▋       | 6/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 88/300:  41%|████      | 9/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 88/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 88/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 88/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 88/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 29%|██▉       | 88/300 [01:33<03:34,  1.01s/it]             \u001b[A\n",
      "Epoch 89/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89/300:  14%|█▎        | 3/22 [00:00<00:00, 25.70it/s]\u001b[A\n",
      "Epoch 89/300:  27%|██▋       | 6/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 89/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 89/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 89/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 89/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 89/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      " 30%|██▉       | 89/300 [01:34<03:33,  1.01s/it]             \u001b[A\n",
      "Epoch 90/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90/300:  14%|█▎        | 3/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 90/300:  27%|██▋       | 6/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 90/300:  41%|████      | 9/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 90/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 90/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 90/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 90/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 30%|███       | 90/300 [01:35<03:32,  1.01s/it]             \u001b[A\n",
      "Epoch 91/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91/300:  14%|█▎        | 3/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 91/300:  27%|██▋       | 6/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 91/300:  41%|████      | 9/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 91/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 91/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 91/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 91/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      " 30%|███       | 91/300 [01:36<03:31,  1.01s/it]             \u001b[A\n",
      "Epoch 92/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92/300:  14%|█▎        | 3/22 [00:00<00:00, 26.99it/s]\u001b[A\n",
      "Epoch 92/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 92/300:  41%|████      | 9/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 92/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 92/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 92/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 92/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      " 31%|███       | 92/300 [01:37<03:30,  1.01s/it]             \u001b[A\n",
      "Epoch 93/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93/300:  14%|█▎        | 3/22 [00:00<00:00, 25.65it/s]\u001b[A\n",
      "Epoch 93/300:  27%|██▋       | 6/22 [00:00<00:00, 25.98it/s]\u001b[A\n",
      "Epoch 93/300:  41%|████      | 9/22 [00:00<00:00, 25.65it/s]\u001b[A\n",
      "Epoch 93/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 93/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 93/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 93/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      " 31%|███       | 93/300 [01:38<03:29,  1.01s/it]             \u001b[A\n",
      "Epoch 94/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94/300:  14%|█▎        | 3/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 94/300:  27%|██▋       | 6/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 94/300:  41%|████      | 9/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 94/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 94/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 94/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 94/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 31%|███▏      | 94/300 [01:39<03:28,  1.01s/it]             \u001b[A\n",
      "Epoch 95/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95/300:  14%|█▎        | 3/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 95/300:  27%|██▋       | 6/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 95/300:  41%|████      | 9/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 95/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 95/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 95/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 95/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      " 32%|███▏      | 95/300 [01:40<03:27,  1.01s/it]             \u001b[A\n",
      "Epoch 96/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96/300:  14%|█▎        | 3/22 [00:00<00:00, 25.69it/s]\u001b[A\n",
      "Epoch 96/300:  27%|██▋       | 6/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 96/300:  41%|████      | 9/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 96/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 96/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 96/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 96/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      " 32%|███▏      | 96/300 [01:41<03:26,  1.01s/it]             \u001b[A\n",
      "Epoch 97/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97/300:  14%|█▎        | 3/22 [00:00<00:00, 25.74it/s]\u001b[A\n",
      "Epoch 97/300:  27%|██▋       | 6/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 97/300:  41%|████      | 9/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 97/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 97/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 97/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 97/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      " 32%|███▏      | 97/300 [01:42<03:25,  1.01s/it]             \u001b[A\n",
      "Epoch 98/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98/300:  14%|█▎        | 3/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 98/300:  27%|██▋       | 6/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 98/300:  41%|████      | 9/22 [00:00<00:00, 24.22it/s]\u001b[A\n",
      "Epoch 98/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 98/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 98/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.09it/s]\u001b[A\n",
      "Epoch 98/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.13it/s]\u001b[A\n",
      " 33%|███▎      | 98/300 [01:43<03:26,  1.02s/it]             \u001b[A\n",
      "Epoch 99/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99/300:  14%|█▎        | 3/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 99/300:  27%|██▋       | 6/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 99/300:  41%|████      | 9/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 99/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 99/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 99/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 99/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      " 33%|███▎      | 99/300 [01:44<03:24,  1.02s/it]             \u001b[A\n",
      "Epoch 100/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100/300:  14%|█▎        | 3/22 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 100/300:  27%|██▋       | 6/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "Epoch 100/300:  41%|████      | 9/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 100/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 100/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 100/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 100/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      " 33%|███▎      | 100/300 [01:45<03:22,  1.01s/it]             \u001b[A\n",
      "Epoch 101/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101/300:  14%|█▎        | 3/22 [00:00<00:00, 23.51it/s]\u001b[A\n",
      "Epoch 101/300:  27%|██▋       | 6/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 101/300:  41%|████      | 9/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 101/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 101/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 101/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 101/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 34%|███▎      | 101/300 [01:46<03:22,  1.02s/it]             \u001b[A\n",
      "Epoch 102/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102/300:  14%|█▎        | 3/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 102/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 102/300:  41%|████      | 9/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 102/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 102/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 102/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 102/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 34%|███▍      | 102/300 [01:47<03:21,  1.02s/it]             \u001b[A\n",
      "Epoch 103/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103/300:  14%|█▎        | 3/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 103/300:  27%|██▋       | 6/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 103/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 103/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 103/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 103/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 103/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      " 34%|███▍      | 103/300 [01:48<03:19,  1.01s/it]             \u001b[A\n",
      "Epoch 104/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 104/300:  14%|█▎        | 3/22 [00:00<00:00, 27.30it/s]\u001b[A\n",
      "Epoch 104/300:  27%|██▋       | 6/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 104/300:  41%|████      | 9/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 104/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 104/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 104/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 104/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      " 35%|███▍      | 104/300 [01:49<03:18,  1.01s/it]             \u001b[A\n",
      "Epoch 105/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 105/300:  14%|█▎        | 3/22 [00:00<00:00, 24.16it/s]\u001b[A\n",
      "Epoch 105/300:  27%|██▋       | 6/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 105/300:  41%|████      | 9/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 105/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "Epoch 105/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 105/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 105/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      " 35%|███▌      | 105/300 [01:50<03:17,  1.01s/it]             \u001b[A\n",
      "Epoch 106/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 106/300:  14%|█▎        | 3/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 106/300:  27%|██▋       | 6/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 106/300:  41%|████      | 9/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 106/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 106/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 106/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 106/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      " 35%|███▌      | 106/300 [01:51<03:16,  1.01s/it]             \u001b[A\n",
      "Epoch 107/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 107/300:  14%|█▎        | 3/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 107/300:  27%|██▋       | 6/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 107/300:  41%|████      | 9/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 107/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 107/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 107/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 107/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 36%|███▌      | 107/300 [01:52<03:16,  1.02s/it]             \u001b[A\n",
      "Epoch 108/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 108/300:  14%|█▎        | 3/22 [00:00<00:00, 26.11it/s]\u001b[A\n",
      "Epoch 108/300:  27%|██▋       | 6/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 108/300:  41%|████      | 9/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 108/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 108/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 108/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 108/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      " 36%|███▌      | 108/300 [01:53<03:15,  1.02s/it]             \u001b[A\n",
      "Epoch 109/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109/300:  14%|█▎        | 3/22 [00:00<00:00, 25.72it/s]\u001b[A\n",
      "Epoch 109/300:  27%|██▋       | 6/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 109/300:  41%|████      | 9/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 109/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 109/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 109/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 109/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      " 36%|███▋      | 109/300 [01:54<03:14,  1.02s/it]             \u001b[A\n",
      "Epoch 110/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 110/300:  14%|█▎        | 3/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 110/300:  27%|██▋       | 6/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 110/300:  41%|████      | 9/22 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "Epoch 110/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 110/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 110/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 110/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      " 37%|███▋      | 110/300 [01:55<03:13,  1.02s/it]             \u001b[A\n",
      "Epoch 111/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 111/300:  14%|█▎        | 3/22 [00:00<00:00, 25.56it/s]\u001b[A\n",
      "Epoch 111/300:  27%|██▋       | 6/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 111/300:  41%|████      | 9/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 111/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 111/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 111/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 111/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      " 37%|███▋      | 111/300 [01:56<03:12,  1.02s/it]             \u001b[A\n",
      "Epoch 112/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 112/300:  14%|█▎        | 3/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 112/300:  27%|██▋       | 6/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 112/300:  41%|████      | 9/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 112/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 112/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 112/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 112/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      " 37%|███▋      | 112/300 [01:57<03:11,  1.02s/it]             \u001b[A\n",
      "Epoch 113/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 113/300:  14%|█▎        | 3/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 113/300:  27%|██▋       | 6/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 113/300:  41%|████      | 9/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 113/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 113/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 113/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 113/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      " 38%|███▊      | 113/300 [01:58<03:10,  1.02s/it]             \u001b[A\n",
      "Epoch 114/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 114/300:  14%|█▎        | 3/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 114/300:  27%|██▋       | 6/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 114/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 114/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 114/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 114/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 114/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      " 38%|███▊      | 114/300 [01:59<03:09,  1.02s/it]             \u001b[A\n",
      "Epoch 115/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 115/300:  14%|█▎        | 3/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 115/300:  27%|██▋       | 6/22 [00:00<00:00, 25.48it/s]\u001b[A\n",
      "Epoch 115/300:  41%|████      | 9/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 115/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 115/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 115/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 115/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      " 38%|███▊      | 115/300 [02:00<03:08,  1.02s/it]             \u001b[A\n",
      "Epoch 116/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 116/300:  14%|█▎        | 3/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 116/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 116/300:  41%|████      | 9/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 116/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.16it/s]\u001b[A\n",
      "Epoch 116/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 116/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "Epoch 116/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 39%|███▊      | 116/300 [02:01<03:06,  1.02s/it]             \u001b[A\n",
      "Epoch 117/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 117/300:  14%|█▎        | 3/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 117/300:  27%|██▋       | 6/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 117/300:  41%|████      | 9/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 117/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 117/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 117/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 117/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      " 39%|███▉      | 117/300 [02:02<03:05,  1.01s/it]             \u001b[A\n",
      "Epoch 118/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 118/300:  14%|█▎        | 3/22 [00:00<00:00, 26.48it/s]\u001b[A\n",
      "Epoch 118/300:  27%|██▋       | 6/22 [00:00<00:00, 25.47it/s]\u001b[A\n",
      "Epoch 118/300:  41%|████      | 9/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 118/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 118/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.32it/s]\u001b[A\n",
      "Epoch 118/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 118/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      " 39%|███▉      | 118/300 [02:03<03:04,  1.01s/it]             \u001b[A\n",
      "Epoch 119/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 119/300:  14%|█▎        | 3/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 119/300:  27%|██▋       | 6/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 119/300:  41%|████      | 9/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 119/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 119/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 119/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 119/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 40%|███▉      | 119/300 [02:04<03:03,  1.02s/it]             \u001b[A\n",
      "Epoch 120/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 120/300:  14%|█▎        | 3/22 [00:00<00:00, 23.55it/s]\u001b[A\n",
      "Epoch 120/300:  27%|██▋       | 6/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 120/300:  41%|████      | 9/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 120/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 120/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.55it/s]\u001b[A\n",
      "Epoch 120/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.43it/s]\u001b[A\n",
      "Epoch 120/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      " 40%|████      | 120/300 [02:05<03:02,  1.01s/it]             \u001b[A\n",
      "Epoch 121/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 121/300:  14%|█▎        | 3/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 121/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 121/300:  41%|████      | 9/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 121/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 121/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 121/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 121/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      " 40%|████      | 121/300 [02:06<03:01,  1.01s/it]             \u001b[A\n",
      "Epoch 122/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 122/300:  14%|█▎        | 3/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 122/300:  27%|██▋       | 6/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 122/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 122/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 122/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 122/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 122/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 41%|████      | 122/300 [02:07<03:00,  1.02s/it]             \u001b[A\n",
      "Epoch 123/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 123/300:  14%|█▎        | 3/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 123/300:  27%|██▋       | 6/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 123/300:  41%|████      | 9/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 123/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.28it/s]\u001b[A\n",
      "Epoch 123/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 123/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 123/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      " 41%|████      | 123/300 [02:08<02:59,  1.02s/it]             \u001b[A\n",
      "Epoch 124/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 124/300:  14%|█▎        | 3/22 [00:00<00:00, 23.81it/s]\u001b[A\n",
      "Epoch 124/300:  27%|██▋       | 6/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 124/300:  41%|████      | 9/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 124/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 124/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 124/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 124/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      " 41%|████▏     | 124/300 [02:09<02:58,  1.02s/it]             \u001b[A\n",
      "Epoch 125/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 125/300:  14%|█▎        | 3/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 125/300:  27%|██▋       | 6/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 125/300:  41%|████      | 9/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 125/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 125/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 125/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 125/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 42%|████▏     | 125/300 [02:10<02:57,  1.01s/it]             \u001b[A\n",
      "Epoch 126/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 126/300:  14%|█▎        | 3/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 126/300:  27%|██▋       | 6/22 [00:00<00:00, 25.91it/s]\u001b[A\n",
      "Epoch 126/300:  41%|████      | 9/22 [00:00<00:00, 25.67it/s]\u001b[A\n",
      "Epoch 126/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 126/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 126/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 126/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      " 42%|████▏     | 126/300 [02:11<02:56,  1.01s/it]             \u001b[A\n",
      "Epoch 127/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 127/300:  14%|█▎        | 3/22 [00:00<00:00, 25.89it/s]\u001b[A\n",
      "Epoch 127/300:  27%|██▋       | 6/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 127/300:  41%|████      | 9/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 127/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 127/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 127/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 127/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      " 42%|████▏     | 127/300 [02:12<02:55,  1.01s/it]             \u001b[A\n",
      "Epoch 128/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 128/300:  14%|█▎        | 3/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 128/300:  27%|██▋       | 6/22 [00:00<00:00, 23.88it/s]\u001b[A\n",
      "Epoch 128/300:  41%|████      | 9/22 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "Epoch 128/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 128/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 128/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 128/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 43%|████▎     | 128/300 [02:13<02:54,  1.01s/it]             \u001b[A\n",
      "Epoch 129/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 129/300:  14%|█▎        | 3/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 129/300:  27%|██▋       | 6/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 129/300:  41%|████      | 9/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 129/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 129/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 129/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 129/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 43%|████▎     | 129/300 [02:14<02:53,  1.01s/it]             \u001b[A\n",
      "Epoch 130/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 130/300:  14%|█▎        | 3/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 130/300:  27%|██▋       | 6/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 130/300:  41%|████      | 9/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 130/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 130/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 130/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 130/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.47it/s]\u001b[A\n",
      " 43%|████▎     | 130/300 [02:15<02:52,  1.02s/it]             \u001b[A\n",
      "Epoch 131/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 131/300:  14%|█▎        | 3/22 [00:00<00:00, 26.73it/s]\u001b[A\n",
      "Epoch 131/300:  27%|██▋       | 6/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 131/300:  41%|████      | 9/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 131/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 131/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 131/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 131/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      " 44%|████▎     | 131/300 [02:16<02:51,  1.01s/it]             \u001b[A\n",
      "Epoch 132/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 132/300:  14%|█▎        | 3/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 132/300:  27%|██▋       | 6/22 [00:00<00:00, 24.23it/s]\u001b[A\n",
      "Epoch 132/300:  41%|████      | 9/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 132/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 132/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 132/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 132/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      " 44%|████▍     | 132/300 [02:17<02:50,  1.02s/it]             \u001b[A\n",
      "Epoch 133/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 133/300:  14%|█▎        | 3/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 133/300:  27%|██▋       | 6/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 133/300:  41%|████      | 9/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 133/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 133/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 133/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 133/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      " 44%|████▍     | 133/300 [02:18<02:49,  1.02s/it]             \u001b[A\n",
      "Epoch 134/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 134/300:  14%|█▎        | 3/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 134/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 134/300:  41%|████      | 9/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 134/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 134/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.23it/s]\u001b[A\n",
      "Epoch 134/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 134/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      " 45%|████▍     | 134/300 [02:19<02:49,  1.02s/it]             \u001b[A\n",
      "Epoch 135/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 135/300:  14%|█▎        | 3/22 [00:00<00:00, 26.38it/s]\u001b[A\n",
      "Epoch 135/300:  27%|██▋       | 6/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 135/300:  41%|████      | 9/22 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 135/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 135/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 135/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 135/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      " 45%|████▌     | 135/300 [02:21<02:48,  1.02s/it]             \u001b[A\n",
      "Epoch 136/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 136/300:  14%|█▎        | 3/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 136/300:  27%|██▋       | 6/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 136/300:  41%|████      | 9/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 136/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 136/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 136/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 136/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      " 45%|████▌     | 136/300 [02:22<02:46,  1.02s/it]             \u001b[A\n",
      "Epoch 137/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 137/300:  14%|█▎        | 3/22 [00:00<00:00, 25.56it/s]\u001b[A\n",
      "Epoch 137/300:  27%|██▋       | 6/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 137/300:  41%|████      | 9/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 137/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 137/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 137/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 137/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      " 46%|████▌     | 137/300 [02:23<02:45,  1.01s/it]             \u001b[A\n",
      "Epoch 138/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 138/300:  14%|█▎        | 3/22 [00:00<00:00, 24.18it/s]\u001b[A\n",
      "Epoch 138/300:  27%|██▋       | 6/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 138/300:  41%|████      | 9/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 138/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 138/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 138/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 138/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      " 46%|████▌     | 138/300 [02:24<02:44,  1.01s/it]             \u001b[A\n",
      "Epoch 139/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 139/300:  14%|█▎        | 3/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 139/300:  27%|██▋       | 6/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 139/300:  41%|████      | 9/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 139/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 139/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 139/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 139/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      " 46%|████▋     | 139/300 [02:25<02:43,  1.01s/it]             \u001b[A\n",
      "Epoch 140/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 140/300:  14%|█▎        | 3/22 [00:00<00:00, 24.17it/s]\u001b[A\n",
      "Epoch 140/300:  27%|██▋       | 6/22 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 140/300:  41%|████      | 9/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 140/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 140/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "Epoch 140/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 140/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      " 47%|████▋     | 140/300 [02:26<02:41,  1.01s/it]             \u001b[A\n",
      "Epoch 141/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 141/300:  14%|█▎        | 3/22 [00:00<00:00, 25.68it/s]\u001b[A\n",
      "Epoch 141/300:  27%|██▋       | 6/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 141/300:  41%|████      | 9/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 141/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 141/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 141/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 141/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      " 47%|████▋     | 141/300 [02:27<02:41,  1.01s/it]             \u001b[A\n",
      "Epoch 142/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 142/300:  14%|█▎        | 3/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 142/300:  27%|██▋       | 6/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 142/300:  41%|████      | 9/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 142/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 142/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 142/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 142/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      " 47%|████▋     | 142/300 [02:28<02:39,  1.01s/it]             \u001b[A\n",
      "Epoch 143/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 143/300:  14%|█▎        | 3/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 143/300:  27%|██▋       | 6/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 143/300:  41%|████      | 9/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 143/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 143/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 143/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 143/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 48%|████▊     | 143/300 [02:29<02:39,  1.01s/it]             \u001b[A\n",
      "Epoch 144/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 144/300:  14%|█▎        | 3/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 144/300:  27%|██▋       | 6/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 144/300:  41%|████      | 9/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 144/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 144/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 144/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 144/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      " 48%|████▊     | 144/300 [02:30<02:37,  1.01s/it]             \u001b[A\n",
      "Epoch 145/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 145/300:  14%|█▎        | 3/22 [00:00<00:00, 24.47it/s]\u001b[A\n",
      "Epoch 145/300:  27%|██▋       | 6/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 145/300:  41%|████      | 9/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 145/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 145/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 145/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 145/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      " 48%|████▊     | 145/300 [02:31<02:36,  1.01s/it]             \u001b[A\n",
      "Epoch 146/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 146/300:  14%|█▎        | 3/22 [00:00<00:00, 24.22it/s]\u001b[A\n",
      "Epoch 146/300:  27%|██▋       | 6/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 146/300:  41%|████      | 9/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 146/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.28it/s]\u001b[A\n",
      "Epoch 146/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 146/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 146/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      " 49%|████▊     | 146/300 [02:32<02:36,  1.02s/it]             \u001b[A\n",
      "Epoch 147/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 147/300:  14%|█▎        | 3/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 147/300:  27%|██▋       | 6/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 147/300:  41%|████      | 9/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 147/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 147/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 147/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 147/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.64it/s]\u001b[A\n",
      " 49%|████▉     | 147/300 [02:33<02:34,  1.01s/it]             \u001b[A\n",
      "Epoch 148/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 148/300:  14%|█▎        | 3/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 148/300:  27%|██▋       | 6/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 148/300:  41%|████      | 9/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 148/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 148/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 148/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 148/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      " 49%|████▉     | 148/300 [02:34<02:34,  1.02s/it]             \u001b[A\n",
      "Epoch 149/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149/300:  14%|█▎        | 3/22 [00:00<00:00, 24.22it/s]\u001b[A\n",
      "Epoch 149/300:  27%|██▋       | 6/22 [00:00<00:00, 24.23it/s]\u001b[A\n",
      "Epoch 149/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 149/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 149/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 149/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 149/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      " 50%|████▉     | 149/300 [02:35<02:33,  1.02s/it]             \u001b[A\n",
      "Epoch 150/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 150/300:  14%|█▎        | 3/22 [00:00<00:00, 23.63it/s]\u001b[A\n",
      "Epoch 150/300:  27%|██▋       | 6/22 [00:00<00:00, 23.98it/s]\u001b[A\n",
      "Epoch 150/300:  41%|████      | 9/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 150/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 150/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 150/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.31it/s]\u001b[A\n",
      "Epoch 150/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      " 50%|█████     | 150/300 [02:36<02:33,  1.02s/it]             \u001b[A\n",
      "Epoch 151/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 151/300:  14%|█▎        | 3/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 151/300:  27%|██▋       | 6/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 151/300:  41%|████      | 9/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 151/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 151/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 151/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 151/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      " 50%|█████     | 151/300 [02:37<02:33,  1.03s/it]             \u001b[A\n",
      "Epoch 152/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 152/300:  14%|█▎        | 3/22 [00:00<00:00, 24.20it/s]\u001b[A\n",
      "Epoch 152/300:  27%|██▋       | 6/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 152/300:  41%|████      | 9/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 152/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      "Epoch 152/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 152/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 152/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 51%|█████     | 152/300 [02:38<02:31,  1.02s/it]             \u001b[A\n",
      "Epoch 153/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 153/300:  14%|█▎        | 3/22 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "Epoch 153/300:  27%|██▋       | 6/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 153/300:  41%|████      | 9/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 153/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 153/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.17it/s]\u001b[A\n",
      "Epoch 153/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 153/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      " 51%|█████     | 153/300 [02:39<02:30,  1.03s/it]             \u001b[A\n",
      "Epoch 154/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 154/300:  14%|█▎        | 3/22 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "Epoch 154/300:  27%|██▋       | 6/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 154/300:  41%|████      | 9/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 154/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 154/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 154/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 154/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      " 51%|█████▏    | 154/300 [02:40<02:29,  1.03s/it]             \u001b[A\n",
      "Epoch 155/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 155/300:  14%|█▎        | 3/22 [00:00<00:00, 23.59it/s]\u001b[A\n",
      "Epoch 155/300:  27%|██▋       | 6/22 [00:00<00:00, 23.56it/s]\u001b[A\n",
      "Epoch 155/300:  41%|████      | 9/22 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "Epoch 155/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 155/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 155/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 155/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 52%|█████▏    | 155/300 [02:41<02:28,  1.03s/it]             \u001b[A\n",
      "Epoch 156/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 156/300:  14%|█▎        | 3/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 156/300:  27%|██▋       | 6/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 156/300:  41%|████      | 9/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 156/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 156/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 156/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 156/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      " 52%|█████▏    | 156/300 [02:42<02:27,  1.02s/it]             \u001b[A\n",
      "Epoch 157/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 157/300:  14%|█▎        | 3/22 [00:00<00:00, 26.29it/s]\u001b[A\n",
      "Epoch 157/300:  27%|██▋       | 6/22 [00:00<00:00, 25.43it/s]\u001b[A\n",
      "Epoch 157/300:  41%|████      | 9/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 157/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 157/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 157/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 157/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 52%|█████▏    | 157/300 [02:43<02:26,  1.02s/it]             \u001b[A\n",
      "Epoch 158/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 158/300:  14%|█▎        | 3/22 [00:00<00:00, 25.53it/s]\u001b[A\n",
      "Epoch 158/300:  27%|██▋       | 6/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 158/300:  41%|████      | 9/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 158/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 158/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 158/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 158/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      " 53%|█████▎    | 158/300 [02:44<02:24,  1.02s/it]             \u001b[A\n",
      "Epoch 159/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 159/300:  14%|█▎        | 3/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 159/300:  27%|██▋       | 6/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 159/300:  41%|████      | 9/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 159/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 159/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 159/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 159/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      " 53%|█████▎    | 159/300 [02:45<02:24,  1.02s/it]             \u001b[A\n",
      "Epoch 160/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 160/300:  14%|█▎        | 3/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 160/300:  27%|██▋       | 6/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 160/300:  41%|████      | 9/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 160/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 160/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 160/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 160/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      " 53%|█████▎    | 160/300 [02:46<02:22,  1.02s/it]             \u001b[A\n",
      "Epoch 161/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 161/300:  14%|█▎        | 3/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 161/300:  27%|██▋       | 6/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 161/300:  41%|████      | 9/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 161/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 161/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 161/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 161/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 54%|█████▎    | 161/300 [02:47<02:21,  1.02s/it]             \u001b[A\n",
      "Epoch 162/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 162/300:  14%|█▎        | 3/22 [00:00<00:00, 23.63it/s]\u001b[A\n",
      "Epoch 162/300:  27%|██▋       | 6/22 [00:00<00:00, 23.92it/s]\u001b[A\n",
      "Epoch 162/300:  41%|████      | 9/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 162/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 162/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 162/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 162/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      " 54%|█████▍    | 162/300 [02:48<02:21,  1.02s/it]             \u001b[A\n",
      "Epoch 163/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 163/300:  14%|█▎        | 3/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 163/300:  27%|██▋       | 6/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 163/300:  41%|████      | 9/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 163/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 163/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 163/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 163/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      " 54%|█████▍    | 163/300 [02:49<02:20,  1.03s/it]             \u001b[A\n",
      "Epoch 164/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 164/300:  14%|█▎        | 3/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 164/300:  27%|██▋       | 6/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 164/300:  41%|████      | 9/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 164/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 164/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 164/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 164/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 55%|█████▍    | 164/300 [02:50<02:19,  1.03s/it]             \u001b[A\n",
      "Epoch 165/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 165/300:  14%|█▎        | 3/22 [00:00<00:00, 26.54it/s]\u001b[A\n",
      "Epoch 165/300:  27%|██▋       | 6/22 [00:00<00:00, 25.45it/s]\u001b[A\n",
      "Epoch 165/300:  41%|████      | 9/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 165/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 165/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 165/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 165/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      " 55%|█████▌    | 165/300 [02:51<02:17,  1.02s/it]             \u001b[A\n",
      "Epoch 166/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 166/300:  14%|█▎        | 3/22 [00:00<00:00, 25.90it/s]\u001b[A\n",
      "Epoch 166/300:  27%|██▋       | 6/22 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "Epoch 166/300:  41%|████      | 9/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 166/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 166/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 166/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 166/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      " 55%|█████▌    | 166/300 [02:52<02:16,  1.02s/it]             \u001b[A\n",
      "Epoch 167/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 167/300:  14%|█▎        | 3/22 [00:00<00:00, 24.06it/s]\u001b[A\n",
      "Epoch 167/300:  27%|██▋       | 6/22 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "Epoch 167/300:  41%|████      | 9/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 167/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 167/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 167/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.78it/s]\u001b[A\n",
      "Epoch 167/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.23it/s]\u001b[A\n",
      " 56%|█████▌    | 167/300 [02:53<02:15,  1.02s/it]             \u001b[A\n",
      "Epoch 168/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 168/300:  14%|█▎        | 3/22 [00:00<00:00, 26.14it/s]\u001b[A\n",
      "Epoch 168/300:  27%|██▋       | 6/22 [00:00<00:00, 25.67it/s]\u001b[A\n",
      "Epoch 168/300:  41%|████      | 9/22 [00:00<00:00, 25.77it/s]\u001b[A\n",
      "Epoch 168/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 168/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 168/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 168/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      " 56%|█████▌    | 168/300 [02:54<02:14,  1.02s/it]             \u001b[A\n",
      "Epoch 169/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 169/300:  14%|█▎        | 3/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 169/300:  27%|██▋       | 6/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 169/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 169/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 169/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 169/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 169/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      " 56%|█████▋    | 169/300 [02:55<02:13,  1.02s/it]             \u001b[A\n",
      "Epoch 170/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 170/300:  14%|█▎        | 3/22 [00:00<00:00, 23.98it/s]\u001b[A\n",
      "Epoch 170/300:  27%|██▋       | 6/22 [00:00<00:00, 23.86it/s]\u001b[A\n",
      "Epoch 170/300:  41%|████      | 9/22 [00:00<00:00, 24.07it/s]\u001b[A\n",
      "Epoch 170/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 170/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 170/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 170/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      " 57%|█████▋    | 170/300 [02:56<02:12,  1.02s/it]             \u001b[A\n",
      "Epoch 171/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 171/300:  14%|█▎        | 3/22 [00:00<00:00, 23.69it/s]\u001b[A\n",
      "Epoch 171/300:  27%|██▋       | 6/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 171/300:  41%|████      | 9/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 171/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 171/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 171/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 171/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      " 57%|█████▋    | 171/300 [02:57<02:11,  1.02s/it]             \u001b[A\n",
      "Epoch 172/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 172/300:  14%|█▎        | 3/22 [00:00<00:00, 26.29it/s]\u001b[A\n",
      "Epoch 172/300:  27%|██▋       | 6/22 [00:00<00:00, 25.91it/s]\u001b[A\n",
      "Epoch 172/300:  41%|████      | 9/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 172/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 172/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 172/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 172/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      " 57%|█████▋    | 172/300 [02:58<02:10,  1.02s/it]             \u001b[A\n",
      "Epoch 173/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 173/300:  14%|█▎        | 3/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 173/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 173/300:  41%|████      | 9/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 173/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 173/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 173/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 173/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      " 58%|█████▊    | 173/300 [02:59<02:09,  1.02s/it]             \u001b[A\n",
      "Epoch 174/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 174/300:  14%|█▎        | 3/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 174/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 174/300:  41%|████      | 9/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 174/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 174/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 174/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 174/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      " 58%|█████▊    | 174/300 [03:00<02:08,  1.02s/it]             \u001b[A\n",
      "Epoch 175/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 175/300:  14%|█▎        | 3/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 175/300:  27%|██▋       | 6/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 175/300:  41%|████      | 9/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 175/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 175/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.06it/s]\u001b[A\n",
      "Epoch 175/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 175/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      " 58%|█████▊    | 175/300 [03:01<02:08,  1.02s/it]             \u001b[A\n",
      "Epoch 176/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 176/300:  14%|█▎        | 3/22 [00:00<00:00, 26.07it/s]\u001b[A\n",
      "Epoch 176/300:  27%|██▋       | 6/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 176/300:  41%|████      | 9/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 176/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 176/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 176/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 176/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      " 59%|█████▊    | 176/300 [03:02<02:06,  1.02s/it]             \u001b[A\n",
      "Epoch 177/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 177/300:  14%|█▎        | 3/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 177/300:  27%|██▋       | 6/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 177/300:  41%|████      | 9/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 177/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 177/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 177/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 177/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 59%|█████▉    | 177/300 [03:03<02:05,  1.02s/it]             \u001b[A\n",
      "Epoch 178/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 178/300:  14%|█▎        | 3/22 [00:00<00:00, 26.06it/s]\u001b[A\n",
      "Epoch 178/300:  27%|██▋       | 6/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "Epoch 178/300:  41%|████      | 9/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 178/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 178/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 178/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 178/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      " 59%|█████▉    | 178/300 [03:04<02:04,  1.02s/it]             \u001b[A\n",
      "Epoch 179/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 179/300:  14%|█▎        | 3/22 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "Epoch 179/300:  27%|██▋       | 6/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 179/300:  41%|████      | 9/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 179/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 179/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 179/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 179/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      " 60%|█████▉    | 179/300 [03:05<02:03,  1.02s/it]             \u001b[A\n",
      "Epoch 180/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 180/300:  14%|█▎        | 3/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 180/300:  27%|██▋       | 6/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 180/300:  41%|████      | 9/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 180/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 180/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 180/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 180/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      " 60%|██████    | 180/300 [03:06<02:02,  1.02s/it]             \u001b[A\n",
      "Epoch 181/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 181/300:  14%|█▎        | 3/22 [00:00<00:00, 26.12it/s]\u001b[A\n",
      "Epoch 181/300:  27%|██▋       | 6/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 181/300:  41%|████      | 9/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 181/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 181/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 181/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 181/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      " 60%|██████    | 181/300 [03:07<02:01,  1.02s/it]             \u001b[A\n",
      "Epoch 182/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 182/300:  14%|█▎        | 3/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 182/300:  27%|██▋       | 6/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 182/300:  41%|████      | 9/22 [00:00<00:00, 24.08it/s]\u001b[A\n",
      "Epoch 182/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 182/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "Epoch 182/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 182/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      " 61%|██████    | 182/300 [03:08<02:01,  1.03s/it]             \u001b[A\n",
      "Epoch 183/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 183/300:  14%|█▎        | 3/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 183/300:  27%|██▋       | 6/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 183/300:  41%|████      | 9/22 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "Epoch 183/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 183/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 183/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 183/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.83it/s]\u001b[A\n",
      " 61%|██████    | 183/300 [03:09<01:59,  1.02s/it]             \u001b[A\n",
      "Epoch 184/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 184/300:  14%|█▎        | 3/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 184/300:  27%|██▋       | 6/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 184/300:  41%|████      | 9/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 184/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 184/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 184/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 184/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 61%|██████▏   | 184/300 [03:10<01:58,  1.02s/it]             \u001b[A\n",
      "Epoch 185/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 185/300:  14%|█▎        | 3/22 [00:00<00:00, 25.74it/s]\u001b[A\n",
      "Epoch 185/300:  27%|██▋       | 6/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 185/300:  41%|████      | 9/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 185/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 185/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 185/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 185/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      " 62%|██████▏   | 185/300 [03:11<01:56,  1.02s/it]             \u001b[A\n",
      "Epoch 186/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 186/300:  14%|█▎        | 3/22 [00:00<00:00, 23.66it/s]\u001b[A\n",
      "Epoch 186/300:  27%|██▋       | 6/22 [00:00<00:00, 24.20it/s]\u001b[A\n",
      "Epoch 186/300:  41%|████      | 9/22 [00:00<00:00, 23.30it/s]\u001b[A\n",
      "Epoch 186/300:  55%|█████▍    | 12/22 [00:00<00:00, 23.87it/s]\u001b[A\n",
      "Epoch 186/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 186/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 186/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 62%|██████▏   | 186/300 [03:13<01:56,  1.03s/it]             \u001b[A\n",
      "Epoch 187/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 187/300:  14%|█▎        | 3/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 187/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 187/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 187/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 187/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 187/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 187/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      " 62%|██████▏   | 187/300 [03:14<01:56,  1.03s/it]             \u001b[A\n",
      "Epoch 188/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 188/300:  14%|█▎        | 3/22 [00:00<00:00, 25.96it/s]\u001b[A\n",
      "Epoch 188/300:  27%|██▋       | 6/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 188/300:  41%|████      | 9/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 188/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 188/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 188/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 188/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 63%|██████▎   | 188/300 [03:15<01:54,  1.02s/it]             \u001b[A\n",
      "Epoch 189/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 189/300:  14%|█▎        | 3/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 189/300:  27%|██▋       | 6/22 [00:00<00:00, 25.94it/s]\u001b[A\n",
      "Epoch 189/300:  41%|████      | 9/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 189/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 189/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 189/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 189/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      " 63%|██████▎   | 189/300 [03:16<01:53,  1.02s/it]             \u001b[A\n",
      "Epoch 190/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 190/300:  14%|█▎        | 3/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 190/300:  27%|██▋       | 6/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 190/300:  41%|████      | 9/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 190/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 190/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 190/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 190/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      " 63%|██████▎   | 190/300 [03:17<01:52,  1.02s/it]             \u001b[A\n",
      "Epoch 191/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 191/300:  14%|█▎        | 3/22 [00:00<00:00, 25.76it/s]\u001b[A\n",
      "Epoch 191/300:  27%|██▋       | 6/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 191/300:  41%|████      | 9/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 191/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 191/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 191/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 191/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      " 64%|██████▎   | 191/300 [03:18<01:51,  1.03s/it]             \u001b[A\n",
      "Epoch 192/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 192/300:  14%|█▎        | 3/22 [00:00<00:00, 26.50it/s]\u001b[A\n",
      "Epoch 192/300:  27%|██▋       | 6/22 [00:00<00:00, 25.84it/s]\u001b[A\n",
      "Epoch 192/300:  41%|████      | 9/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 192/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 192/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 192/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 192/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 64%|██████▍   | 192/300 [03:19<01:50,  1.02s/it]             \u001b[A\n",
      "Epoch 193/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 193/300:  14%|█▎        | 3/22 [00:00<00:00, 24.05it/s]\u001b[A\n",
      "Epoch 193/300:  27%|██▋       | 6/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 193/300:  41%|████      | 9/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 193/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 193/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 193/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 193/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      " 64%|██████▍   | 193/300 [03:20<01:49,  1.02s/it]             \u001b[A\n",
      "Epoch 194/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 194/300:  14%|█▎        | 3/22 [00:00<00:00, 25.69it/s]\u001b[A\n",
      "Epoch 194/300:  27%|██▋       | 6/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 194/300:  41%|████      | 9/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 194/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.47it/s]\u001b[A\n",
      "Epoch 194/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 194/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 194/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 65%|██████▍   | 194/300 [03:21<01:53,  1.07s/it]             \u001b[A\n",
      "Epoch 195/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 195/300:  14%|█▎        | 3/22 [00:00<00:00, 27.36it/s]\u001b[A\n",
      "Epoch 195/300:  27%|██▋       | 6/22 [00:00<00:00, 25.97it/s]\u001b[A\n",
      "Epoch 195/300:  41%|████      | 9/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 195/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 195/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 195/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 195/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      " 65%|██████▌   | 195/300 [03:22<01:50,  1.05s/it]             \u001b[A\n",
      "Epoch 196/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 196/300:  14%|█▎        | 3/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 196/300:  27%|██▋       | 6/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 196/300:  41%|████      | 9/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 196/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 196/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 196/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 196/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      " 65%|██████▌   | 196/300 [03:23<01:48,  1.04s/it]             \u001b[A\n",
      "Epoch 197/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 197/300:  14%|█▎        | 3/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 197/300:  27%|██▋       | 6/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 197/300:  41%|████      | 9/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 197/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 197/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 197/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 197/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.01it/s]\u001b[A\n",
      " 66%|██████▌   | 197/300 [03:24<01:46,  1.04s/it]             \u001b[A\n",
      "Epoch 198/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 198/300:  14%|█▎        | 3/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 198/300:  27%|██▋       | 6/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 198/300:  41%|████      | 9/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 198/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 198/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 198/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 198/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      " 66%|██████▌   | 198/300 [03:25<01:45,  1.03s/it]             \u001b[A\n",
      "Epoch 199/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199/300:  14%|█▎        | 3/22 [00:00<00:00, 26.20it/s]\u001b[A\n",
      "Epoch 199/300:  27%|██▋       | 6/22 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Epoch 199/300:  41%|████      | 9/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 199/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 199/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 199/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 199/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      " 66%|██████▋   | 199/300 [03:26<01:44,  1.03s/it]             \u001b[A\n",
      "Epoch 200/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 200/300:  14%|█▎        | 3/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 200/300:  27%|██▋       | 6/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 200/300:  41%|████      | 9/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 200/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 200/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.32it/s]\u001b[A\n",
      "Epoch 200/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 200/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 67%|██████▋   | 200/300 [03:27<01:42,  1.03s/it]             \u001b[A\n",
      "Epoch 201/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 201/300:  14%|█▎        | 3/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 201/300:  27%|██▋       | 6/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 201/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 201/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 201/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 201/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 201/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 67%|██████▋   | 201/300 [03:28<01:41,  1.03s/it]             \u001b[A\n",
      "Epoch 202/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 202/300:  14%|█▎        | 3/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 202/300:  27%|██▋       | 6/22 [00:00<00:00, 24.47it/s]\u001b[A\n",
      "Epoch 202/300:  41%|████      | 9/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 202/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 202/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.32it/s]\u001b[A\n",
      "Epoch 202/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.28it/s]\u001b[A\n",
      "Epoch 202/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      " 67%|██████▋   | 202/300 [03:29<01:40,  1.03s/it]             \u001b[A\n",
      "Epoch 203/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 203/300:  14%|█▎        | 3/22 [00:00<00:00, 25.83it/s]\u001b[A\n",
      "Epoch 203/300:  27%|██▋       | 6/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 203/300:  41%|████      | 9/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 203/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 203/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 203/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 203/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      " 68%|██████▊   | 203/300 [03:30<01:39,  1.03s/it]             \u001b[A\n",
      "Epoch 204/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 204/300:  14%|█▎        | 3/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 204/300:  27%|██▋       | 6/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 204/300:  41%|████      | 9/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 204/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 204/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 204/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 204/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      " 68%|██████▊   | 204/300 [03:31<01:38,  1.03s/it]             \u001b[A\n",
      "Epoch 205/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 205/300:  14%|█▎        | 3/22 [00:00<00:00, 24.17it/s]\u001b[A\n",
      "Epoch 205/300:  27%|██▋       | 6/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 205/300:  41%|████      | 9/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 205/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 205/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 205/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 205/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      " 68%|██████▊   | 205/300 [03:32<01:37,  1.03s/it]             \u001b[A\n",
      "Epoch 206/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 206/300:  14%|█▎        | 3/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 206/300:  27%|██▋       | 6/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 206/300:  41%|████      | 9/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 206/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      "Epoch 206/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Epoch 206/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.43it/s]\u001b[A\n",
      "Epoch 206/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      " 69%|██████▊   | 206/300 [03:33<01:35,  1.02s/it]             \u001b[A\n",
      "Epoch 207/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 207/300:  14%|█▎        | 3/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 207/300:  27%|██▋       | 6/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 207/300:  41%|████      | 9/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 207/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 207/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 207/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.10it/s]\u001b[A\n",
      "Epoch 207/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      " 69%|██████▉   | 207/300 [03:34<01:35,  1.02s/it]             \u001b[A\n",
      "Epoch 208/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 208/300:  14%|█▎        | 3/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 208/300:  27%|██▋       | 6/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 208/300:  41%|████      | 9/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 208/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 208/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 208/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 208/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 69%|██████▉   | 208/300 [03:35<01:34,  1.03s/it]             \u001b[A\n",
      "Epoch 209/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 209/300:  14%|█▎        | 3/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 209/300:  27%|██▋       | 6/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 209/300:  41%|████      | 9/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 209/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 209/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 209/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 209/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      " 70%|██████▉   | 209/300 [03:36<01:33,  1.03s/it]             \u001b[A\n",
      "Epoch 210/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 210/300:  14%|█▎        | 3/22 [00:00<00:00, 24.10it/s]\u001b[A\n",
      "Epoch 210/300:  27%|██▋       | 6/22 [00:00<00:00, 23.93it/s]\u001b[A\n",
      "Epoch 210/300:  41%|████      | 9/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 210/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 210/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.31it/s]\u001b[A\n",
      "Epoch 210/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 210/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      " 70%|███████   | 210/300 [03:37<01:32,  1.03s/it]             \u001b[A\n",
      "Epoch 211/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 211/300:  14%|█▎        | 3/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 211/300:  27%|██▋       | 6/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 211/300:  41%|████      | 9/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 211/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 211/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 211/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.48it/s]\u001b[A\n",
      "Epoch 211/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      " 70%|███████   | 211/300 [03:38<01:31,  1.02s/it]             \u001b[A\n",
      "Epoch 212/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 212/300:  14%|█▎        | 3/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 212/300:  27%|██▋       | 6/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 212/300:  41%|████      | 9/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      "Epoch 212/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 212/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.58it/s]\u001b[A\n",
      "Epoch 212/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 212/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 71%|███████   | 212/300 [03:39<01:29,  1.02s/it]             \u001b[A\n",
      "Epoch 213/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 213/300:  14%|█▎        | 3/22 [00:00<00:00, 24.45it/s]\u001b[A\n",
      "Epoch 213/300:  27%|██▋       | 6/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 213/300:  41%|████      | 9/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 213/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 213/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 213/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 213/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      " 71%|███████   | 213/300 [03:40<01:29,  1.02s/it]             \u001b[A\n",
      "Epoch 214/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 214/300:  14%|█▎        | 3/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 214/300:  27%|██▋       | 6/22 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "Epoch 214/300:  41%|████      | 9/22 [00:00<00:00, 24.16it/s]\u001b[A\n",
      "Epoch 214/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 214/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 214/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 214/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      " 71%|███████▏  | 214/300 [03:41<01:27,  1.02s/it]             \u001b[A\n",
      "Epoch 215/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 215/300:  14%|█▎        | 3/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 215/300:  27%|██▋       | 6/22 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "Epoch 215/300:  41%|████      | 9/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 215/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 215/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 215/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 215/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      " 72%|███████▏  | 215/300 [03:42<01:26,  1.02s/it]             \u001b[A\n",
      "Epoch 216/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 216/300:  14%|█▎        | 3/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 216/300:  27%|██▋       | 6/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 216/300:  41%|████      | 9/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 216/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.65it/s]\u001b[A\n",
      "Epoch 216/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 216/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 216/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.14it/s]\u001b[A\n",
      " 72%|███████▏  | 216/300 [03:43<01:26,  1.03s/it]             \u001b[A\n",
      "Epoch 217/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 217/300:  14%|█▎        | 3/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 217/300:  27%|██▋       | 6/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 217/300:  41%|████      | 9/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 217/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 217/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 217/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 217/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      " 72%|███████▏  | 217/300 [03:44<01:24,  1.02s/it]             \u001b[A\n",
      "Epoch 218/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 218/300:  14%|█▎        | 3/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 218/300:  27%|██▋       | 6/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 218/300:  41%|████      | 9/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 218/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 218/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 218/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.69it/s]\u001b[A\n",
      "Epoch 218/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      " 73%|███████▎  | 218/300 [03:45<01:23,  1.02s/it]             \u001b[A\n",
      "Epoch 219/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 219/300:  14%|█▎        | 3/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 219/300:  27%|██▋       | 6/22 [00:00<00:00, 24.15it/s]\u001b[A\n",
      "Epoch 219/300:  41%|████      | 9/22 [00:00<00:00, 24.19it/s]\u001b[A\n",
      "Epoch 219/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 219/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.53it/s]\u001b[A\n",
      "Epoch 219/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 219/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      " 73%|███████▎  | 219/300 [03:46<01:22,  1.02s/it]             \u001b[A\n",
      "Epoch 220/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 220/300:  14%|█▎        | 3/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 220/300:  27%|██▋       | 6/22 [00:00<00:00, 25.25it/s]\u001b[A\n",
      "Epoch 220/300:  41%|████      | 9/22 [00:00<00:00, 25.55it/s]\u001b[A\n",
      "Epoch 220/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 220/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 220/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 220/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      " 73%|███████▎  | 220/300 [03:47<01:21,  1.02s/it]             \u001b[A\n",
      "Epoch 221/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 221/300:  14%|█▎        | 3/22 [00:00<00:00, 27.40it/s]\u001b[A\n",
      "Epoch 221/300:  27%|██▋       | 6/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 221/300:  41%|████      | 9/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 221/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 221/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "Epoch 221/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 221/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 74%|███████▎  | 221/300 [03:48<01:20,  1.02s/it]             \u001b[A\n",
      "Epoch 222/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 222/300:  14%|█▎        | 3/22 [00:00<00:00, 24.19it/s]\u001b[A\n",
      "Epoch 222/300:  27%|██▋       | 6/22 [00:00<00:00, 24.00it/s]\u001b[A\n",
      "Epoch 222/300:  41%|████      | 9/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 222/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 222/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 222/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 222/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      " 74%|███████▍  | 222/300 [03:49<01:19,  1.02s/it]             \u001b[A\n",
      "Epoch 223/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 223/300:  14%|█▎        | 3/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 223/300:  27%|██▋       | 6/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 223/300:  41%|████      | 9/22 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "Epoch 223/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 223/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 223/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 223/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      " 74%|███████▍  | 223/300 [03:51<01:18,  1.02s/it]             \u001b[A\n",
      "Epoch 224/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 224/300:  14%|█▎        | 3/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 224/300:  27%|██▋       | 6/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 224/300:  41%|████      | 9/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 224/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 224/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 224/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 224/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.15it/s]\u001b[A\n",
      " 75%|███████▍  | 224/300 [03:52<01:17,  1.02s/it]             \u001b[A\n",
      "Epoch 225/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 225/300:  14%|█▎        | 3/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 225/300:  27%|██▋       | 6/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 225/300:  41%|████      | 9/22 [00:00<00:00, 25.36it/s]\u001b[A\n",
      "Epoch 225/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 225/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 225/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 225/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 75%|███████▌  | 225/300 [03:53<01:16,  1.02s/it]             \u001b[A\n",
      "Epoch 226/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 226/300:  14%|█▎        | 3/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 226/300:  27%|██▋       | 6/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 226/300:  41%|████      | 9/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 226/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 226/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 226/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 226/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      " 75%|███████▌  | 226/300 [03:54<01:15,  1.02s/it]             \u001b[A\n",
      "Epoch 227/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 227/300:  14%|█▎        | 3/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 227/300:  27%|██▋       | 6/22 [00:00<00:00, 25.44it/s]\u001b[A\n",
      "Epoch 227/300:  41%|████      | 9/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 227/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 227/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 227/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 227/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 76%|███████▌  | 227/300 [03:55<01:14,  1.02s/it]             \u001b[A\n",
      "Epoch 228/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 228/300:  14%|█▎        | 3/22 [00:00<00:00, 26.35it/s]\u001b[A\n",
      "Epoch 228/300:  27%|██▋       | 6/22 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "Epoch 228/300:  41%|████      | 9/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 228/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.51it/s]\u001b[A\n",
      "Epoch 228/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 228/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 228/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      " 76%|███████▌  | 228/300 [03:56<01:13,  1.02s/it]             \u001b[A\n",
      "Epoch 229/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 229/300:  14%|█▎        | 3/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 229/300:  27%|██▋       | 6/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 229/300:  41%|████      | 9/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 229/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 229/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 229/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 229/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 76%|███████▋  | 229/300 [03:57<01:12,  1.02s/it]             \u001b[A\n",
      "Epoch 230/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 230/300:  14%|█▎        | 3/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 230/300:  27%|██▋       | 6/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 230/300:  41%|████      | 9/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 230/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 230/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 230/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 230/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      " 77%|███████▋  | 230/300 [03:58<01:11,  1.02s/it]             \u001b[A\n",
      "Epoch 231/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 231/300:  14%|█▎        | 3/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 231/300:  27%|██▋       | 6/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 231/300:  41%|████      | 9/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 231/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 231/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      "Epoch 231/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 231/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      " 77%|███████▋  | 231/300 [03:59<01:10,  1.02s/it]             \u001b[A\n",
      "Epoch 232/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 232/300:  14%|█▎        | 3/22 [00:00<00:00, 25.51it/s]\u001b[A\n",
      "Epoch 232/300:  27%|██▋       | 6/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 232/300:  41%|████      | 9/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 232/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 232/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 232/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 232/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      " 77%|███████▋  | 232/300 [04:00<01:09,  1.02s/it]             \u001b[A\n",
      "Epoch 233/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 233/300:  14%|█▎        | 3/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 233/300:  27%|██▋       | 6/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 233/300:  41%|████      | 9/22 [00:00<00:00, 25.63it/s]\u001b[A\n",
      "Epoch 233/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 233/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 233/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 233/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      " 78%|███████▊  | 233/300 [04:01<01:08,  1.02s/it]             \u001b[A\n",
      "Epoch 234/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 234/300:  14%|█▎        | 3/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 234/300:  27%|██▋       | 6/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 234/300:  41%|████      | 9/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 234/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 234/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.06it/s]\u001b[A\n",
      "Epoch 234/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "Epoch 234/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      " 78%|███████▊  | 234/300 [04:02<01:07,  1.02s/it]             \u001b[A\n",
      "Epoch 235/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 235/300:  14%|█▎        | 3/22 [00:00<00:00, 26.13it/s]\u001b[A\n",
      "Epoch 235/300:  27%|██▋       | 6/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      "Epoch 235/300:  41%|████      | 9/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 235/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 235/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 235/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 235/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      " 78%|███████▊  | 235/300 [04:03<01:06,  1.02s/it]             \u001b[A\n",
      "Epoch 236/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 236/300:  14%|█▎        | 3/22 [00:00<00:00, 23.83it/s]\u001b[A\n",
      "Epoch 236/300:  27%|██▋       | 6/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 236/300:  41%|████      | 9/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "Epoch 236/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 236/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 236/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 236/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 79%|███████▊  | 236/300 [04:04<01:05,  1.02s/it]             \u001b[A\n",
      "Epoch 237/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 237/300:  14%|█▎        | 3/22 [00:00<00:00, 23.33it/s]\u001b[A\n",
      "Epoch 237/300:  27%|██▋       | 6/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 237/300:  41%|████      | 9/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 237/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 237/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 237/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 237/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      " 79%|███████▉  | 237/300 [04:05<01:04,  1.02s/it]             \u001b[A\n",
      "Epoch 238/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 238/300:  14%|█▎        | 3/22 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Epoch 238/300:  27%|██▋       | 6/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 238/300:  41%|████      | 9/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 238/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 238/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 238/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 238/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      " 79%|███████▉  | 238/300 [04:06<01:03,  1.02s/it]             \u001b[A\n",
      "Epoch 239/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 239/300:  14%|█▎        | 3/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 239/300:  27%|██▋       | 6/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 239/300:  41%|████      | 9/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 239/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 239/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 239/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 239/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      " 80%|███████▉  | 239/300 [04:07<01:02,  1.02s/it]             \u001b[A\n",
      "Epoch 240/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 240/300:  14%|█▎        | 3/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 240/300:  27%|██▋       | 6/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 240/300:  41%|████      | 9/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 240/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 240/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 240/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 240/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      " 80%|████████  | 240/300 [04:08<01:01,  1.02s/it]             \u001b[A\n",
      "Epoch 241/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 241/300:  14%|█▎        | 3/22 [00:00<00:00, 23.99it/s]\u001b[A\n",
      "Epoch 241/300:  27%|██▋       | 6/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 241/300:  41%|████      | 9/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 241/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "Epoch 241/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 241/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 241/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      " 80%|████████  | 241/300 [04:09<01:00,  1.02s/it]             \u001b[A\n",
      "Epoch 242/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 242/300:  14%|█▎        | 3/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 242/300:  27%|██▋       | 6/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 242/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 242/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 242/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 242/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 242/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      " 81%|████████  | 242/300 [04:10<00:59,  1.02s/it]             \u001b[A\n",
      "Epoch 243/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 243/300:  14%|█▎        | 3/22 [00:00<00:00, 23.43it/s]\u001b[A\n",
      "Epoch 243/300:  27%|██▋       | 6/22 [00:00<00:00, 23.74it/s]\u001b[A\n",
      "Epoch 243/300:  41%|████      | 9/22 [00:00<00:00, 24.56it/s]\u001b[A\n",
      "Epoch 243/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.31it/s]\u001b[A\n",
      "Epoch 243/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.06it/s]\u001b[A\n",
      "Epoch 243/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.39it/s]\u001b[A\n",
      "Epoch 243/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      " 81%|████████  | 243/300 [04:11<00:58,  1.03s/it]             \u001b[A\n",
      "Epoch 244/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 244/300:  14%|█▎        | 3/22 [00:00<00:00, 27.20it/s]\u001b[A\n",
      "Epoch 244/300:  27%|██▋       | 6/22 [00:00<00:00, 25.97it/s]\u001b[A\n",
      "Epoch 244/300:  41%|████      | 9/22 [00:00<00:00, 25.44it/s]\u001b[A\n",
      "Epoch 244/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 244/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "Epoch 244/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 244/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      " 81%|████████▏ | 244/300 [04:12<00:57,  1.02s/it]             \u001b[A\n",
      "Epoch 245/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 245/300:  14%|█▎        | 3/22 [00:00<00:00, 26.34it/s]\u001b[A\n",
      "Epoch 245/300:  27%|██▋       | 6/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 245/300:  41%|████      | 9/22 [00:00<00:00, 25.70it/s]\u001b[A\n",
      "Epoch 245/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.46it/s]\u001b[A\n",
      "Epoch 245/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 245/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 245/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      " 82%|████████▏ | 245/300 [04:13<00:56,  1.02s/it]             \u001b[A\n",
      "Epoch 246/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 246/300:  14%|█▎        | 3/22 [00:00<00:00, 23.84it/s]\u001b[A\n",
      "Epoch 246/300:  27%|██▋       | 6/22 [00:00<00:00, 24.15it/s]\u001b[A\n",
      "Epoch 246/300:  41%|████      | 9/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 246/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 246/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 246/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 246/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      " 82%|████████▏ | 246/300 [04:14<00:55,  1.02s/it]             \u001b[A\n",
      "Epoch 247/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 247/300:  14%|█▎        | 3/22 [00:00<00:00, 23.76it/s]\u001b[A\n",
      "Epoch 247/300:  27%|██▋       | 6/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 247/300:  41%|████      | 9/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 247/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 247/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 247/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 247/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      " 82%|████████▏ | 247/300 [04:15<00:54,  1.02s/it]             \u001b[A\n",
      "Epoch 248/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 248/300:  14%|█▎        | 3/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 248/300:  27%|██▋       | 6/22 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 248/300:  41%|████      | 9/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 248/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 248/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 248/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 248/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      " 83%|████████▎ | 248/300 [04:16<00:53,  1.02s/it]             \u001b[A\n",
      "Epoch 249/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 249/300:  14%|█▎        | 3/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 249/300:  27%|██▋       | 6/22 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "Epoch 249/300:  41%|████      | 9/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 249/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 249/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 249/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 249/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      " 83%|████████▎ | 249/300 [04:17<00:52,  1.02s/it]             \u001b[A\n",
      "Epoch 250/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 250/300:  14%|█▎        | 3/22 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "Epoch 250/300:  27%|██▋       | 6/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 250/300:  41%|████      | 9/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 250/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 250/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "Epoch 250/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 250/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 83%|████████▎ | 250/300 [04:18<00:51,  1.02s/it]             \u001b[A\n",
      "Epoch 251/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 251/300:  14%|█▎        | 3/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 251/300:  27%|██▋       | 6/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 251/300:  41%|████      | 9/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 251/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 251/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 251/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 251/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      " 84%|████████▎ | 251/300 [04:19<00:50,  1.02s/it]             \u001b[A\n",
      "Epoch 252/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 252/300:  14%|█▎        | 3/22 [00:00<00:00, 26.01it/s]\u001b[A\n",
      "Epoch 252/300:  27%|██▋       | 6/22 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Epoch 252/300:  41%|████      | 9/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 252/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 252/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 252/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 252/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      " 84%|████████▍ | 252/300 [04:20<00:49,  1.02s/it]             \u001b[A\n",
      "Epoch 253/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 253/300:  14%|█▎        | 3/22 [00:00<00:00, 25.82it/s]\u001b[A\n",
      "Epoch 253/300:  27%|██▋       | 6/22 [00:00<00:00, 24.46it/s]\u001b[A\n",
      "Epoch 253/300:  41%|████      | 9/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 253/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 253/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 253/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 253/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 84%|████████▍ | 253/300 [04:21<00:48,  1.02s/it]             \u001b[A\n",
      "Epoch 254/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 254/300:  14%|█▎        | 3/22 [00:00<00:00, 25.75it/s]\u001b[A\n",
      "Epoch 254/300:  27%|██▋       | 6/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 254/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 254/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 254/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 254/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 254/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      " 85%|████████▍ | 254/300 [04:22<00:47,  1.03s/it]             \u001b[A\n",
      "Epoch 255/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 255/300:  14%|█▎        | 3/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 255/300:  27%|██▋       | 6/22 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "Epoch 255/300:  41%|████      | 9/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      "Epoch 255/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "Epoch 255/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 255/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 255/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      " 85%|████████▌ | 255/300 [04:23<00:46,  1.03s/it]             \u001b[A\n",
      "Epoch 256/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 256/300:  14%|█▎        | 3/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 256/300:  27%|██▋       | 6/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 256/300:  41%|████      | 9/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 256/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 256/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 256/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 256/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      " 85%|████████▌ | 256/300 [04:24<00:45,  1.03s/it]             \u001b[A\n",
      "Epoch 257/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 257/300:  14%|█▎        | 3/22 [00:00<00:00, 25.75it/s]\u001b[A\n",
      "Epoch 257/300:  27%|██▋       | 6/22 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "Epoch 257/300:  41%|████      | 9/22 [00:00<00:00, 24.66it/s]\u001b[A\n",
      "Epoch 257/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 257/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 257/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 257/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      " 86%|████████▌ | 257/300 [04:25<00:44,  1.02s/it]             \u001b[A\n",
      "Epoch 258/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 258/300:  14%|█▎        | 3/22 [00:00<00:00, 26.69it/s]\u001b[A\n",
      "Epoch 258/300:  27%|██▋       | 6/22 [00:00<00:00, 25.55it/s]\u001b[A\n",
      "Epoch 258/300:  41%|████      | 9/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 258/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 258/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      "Epoch 258/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 258/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 86%|████████▌ | 258/300 [04:26<00:42,  1.02s/it]             \u001b[A\n",
      "Epoch 259/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 259/300:  14%|█▎        | 3/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 259/300:  27%|██▋       | 6/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 259/300:  41%|████      | 9/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 259/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 259/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "Epoch 259/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 259/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      " 86%|████████▋ | 259/300 [04:27<00:41,  1.02s/it]             \u001b[A\n",
      "Epoch 260/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 260/300:  14%|█▎        | 3/22 [00:00<00:00, 24.07it/s]\u001b[A\n",
      "Epoch 260/300:  27%|██▋       | 6/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 260/300:  41%|████      | 9/22 [00:00<00:00, 25.54it/s]\u001b[A\n",
      "Epoch 260/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 260/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 260/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 260/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.59it/s]\u001b[A\n",
      " 87%|████████▋ | 260/300 [04:28<00:40,  1.02s/it]             \u001b[A\n",
      "Epoch 261/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 261/300:  14%|█▎        | 3/22 [00:00<00:00, 23.89it/s]\u001b[A\n",
      "Epoch 261/300:  27%|██▋       | 6/22 [00:00<00:00, 24.00it/s]\u001b[A\n",
      "Epoch 261/300:  41%|████      | 9/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 261/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 261/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 261/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 261/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      " 87%|████████▋ | 261/300 [04:29<00:39,  1.03s/it]             \u001b[A\n",
      "Epoch 262/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 262/300:  14%|█▎        | 3/22 [00:00<00:00, 25.83it/s]\u001b[A\n",
      "Epoch 262/300:  27%|██▋       | 6/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 262/300:  41%|████      | 9/22 [00:00<00:00, 25.47it/s]\u001b[A\n",
      "Epoch 262/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 262/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 262/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 262/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 87%|████████▋ | 262/300 [04:30<00:38,  1.02s/it]             \u001b[A\n",
      "Epoch 263/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 263/300:  14%|█▎        | 3/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 263/300:  27%|██▋       | 6/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 263/300:  41%|████      | 9/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 263/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 263/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 263/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 263/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      " 88%|████████▊ | 263/300 [04:31<00:37,  1.02s/it]             \u001b[A\n",
      "Epoch 264/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 264/300:  14%|█▎        | 3/22 [00:00<00:00, 27.32it/s]\u001b[A\n",
      "Epoch 264/300:  27%|██▋       | 6/22 [00:00<00:00, 25.69it/s]\u001b[A\n",
      "Epoch 264/300:  41%|████      | 9/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 264/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "Epoch 264/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 264/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 264/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      " 88%|████████▊ | 264/300 [04:32<00:36,  1.02s/it]             \u001b[A\n",
      "Epoch 265/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 265/300:  14%|█▎        | 3/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 265/300:  27%|██▋       | 6/22 [00:00<00:00, 25.24it/s]\u001b[A\n",
      "Epoch 265/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 265/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.22it/s]\u001b[A\n",
      "Epoch 265/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 265/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "Epoch 265/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 88%|████████▊ | 265/300 [04:33<00:35,  1.02s/it]             \u001b[A\n",
      "Epoch 266/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 266/300:  14%|█▎        | 3/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      "Epoch 266/300:  27%|██▋       | 6/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 266/300:  41%|████      | 9/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 266/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 266/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 266/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 266/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      " 89%|████████▊ | 266/300 [04:34<00:34,  1.02s/it]             \u001b[A\n",
      "Epoch 267/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 267/300:  14%|█▎        | 3/22 [00:00<00:00, 25.29it/s]\u001b[A\n",
      "Epoch 267/300:  27%|██▋       | 6/22 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "Epoch 267/300:  41%|████      | 9/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 267/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 267/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 267/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 267/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.31it/s]\u001b[A\n",
      " 89%|████████▉ | 267/300 [04:35<00:33,  1.03s/it]             \u001b[A\n",
      "Epoch 268/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 268/300:  14%|█▎        | 3/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      "Epoch 268/300:  27%|██▋       | 6/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 268/300:  41%|████      | 9/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 268/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 268/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 268/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 268/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      " 89%|████████▉ | 268/300 [04:36<00:32,  1.02s/it]             \u001b[A\n",
      "Epoch 269/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 269/300:  14%|█▎        | 3/22 [00:00<00:00, 26.83it/s]\u001b[A\n",
      "Epoch 269/300:  27%|██▋       | 6/22 [00:00<00:00, 25.76it/s]\u001b[A\n",
      "Epoch 269/300:  41%|████      | 9/22 [00:00<00:00, 25.93it/s]\u001b[A\n",
      "Epoch 269/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.62it/s]\u001b[A\n",
      "Epoch 269/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 269/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.38it/s]\u001b[A\n",
      "Epoch 269/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.01it/s]\u001b[A\n",
      " 90%|████████▉ | 269/300 [04:37<00:31,  1.02s/it]             \u001b[A\n",
      "Epoch 270/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 270/300:  14%|█▎        | 3/22 [00:00<00:00, 25.53it/s]\u001b[A\n",
      "Epoch 270/300:  27%|██▋       | 6/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 270/300:  41%|████      | 9/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 270/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 270/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "Epoch 270/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.59it/s]\u001b[A\n",
      "Epoch 270/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.28it/s]\u001b[A\n",
      " 90%|█████████ | 270/300 [04:39<00:30,  1.02s/it]             \u001b[A\n",
      "Epoch 271/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 271/300:  14%|█▎        | 3/22 [00:00<00:00, 25.88it/s]\u001b[A\n",
      "Epoch 271/300:  27%|██▋       | 6/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 271/300:  41%|████      | 9/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 271/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.26it/s]\u001b[A\n",
      "Epoch 271/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 271/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 271/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.62it/s]\u001b[A\n",
      " 90%|█████████ | 271/300 [04:40<00:29,  1.02s/it]             \u001b[A\n",
      "Epoch 272/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 272/300:  14%|█▎        | 3/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      "Epoch 272/300:  27%|██▋       | 6/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 272/300:  41%|████      | 9/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      "Epoch 272/300:  55%|█████▍    | 12/22 [00:00<00:00, 23.61it/s]\u001b[A\n",
      "Epoch 272/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.07it/s]\u001b[A\n",
      "Epoch 272/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.60it/s]\u001b[A\n",
      "Epoch 272/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 91%|█████████ | 272/300 [04:41<00:28,  1.02s/it]             \u001b[A\n",
      "Epoch 273/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 273/300:  14%|█▎        | 3/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 273/300:  27%|██▋       | 6/22 [00:00<00:00, 25.40it/s]\u001b[A\n",
      "Epoch 273/300:  41%|████      | 9/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 273/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 273/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 273/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 273/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 91%|█████████ | 273/300 [04:42<00:27,  1.03s/it]             \u001b[A\n",
      "Epoch 274/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 274/300:  14%|█▎        | 3/22 [00:00<00:00, 25.49it/s]\u001b[A\n",
      "Epoch 274/300:  27%|██▋       | 6/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 274/300:  41%|████      | 9/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 274/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.35it/s]\u001b[A\n",
      "Epoch 274/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 274/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.93it/s]\u001b[A\n",
      "Epoch 274/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.79it/s]\u001b[A\n",
      " 91%|█████████▏| 274/300 [04:43<00:26,  1.02s/it]             \u001b[A\n",
      "Epoch 275/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 275/300:  14%|█▎        | 3/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 275/300:  27%|██▋       | 6/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 275/300:  41%|████      | 9/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 275/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 275/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 275/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 275/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      " 92%|█████████▏| 275/300 [04:44<00:25,  1.03s/it]             \u001b[A\n",
      "Epoch 276/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 276/300:  14%|█▎        | 3/22 [00:00<00:00, 24.03it/s]\u001b[A\n",
      "Epoch 276/300:  27%|██▋       | 6/22 [00:00<00:00, 24.25it/s]\u001b[A\n",
      "Epoch 276/300:  41%|████      | 9/22 [00:00<00:00, 24.27it/s]\u001b[A\n",
      "Epoch 276/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.09it/s]\u001b[A\n",
      "Epoch 276/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 276/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "Epoch 276/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 92%|█████████▏| 276/300 [04:45<00:24,  1.03s/it]             \u001b[A\n",
      "Epoch 277/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 277/300:  14%|█▎        | 3/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 277/300:  27%|██▋       | 6/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 277/300:  41%|████      | 9/22 [00:00<00:00, 24.69it/s]\u001b[A\n",
      "Epoch 277/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.72it/s]\u001b[A\n",
      "Epoch 277/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.74it/s]\u001b[A\n",
      "Epoch 277/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 277/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      " 92%|█████████▏| 277/300 [04:46<00:23,  1.03s/it]             \u001b[A\n",
      "Epoch 278/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 278/300:  14%|█▎        | 3/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 278/300:  27%|██▋       | 6/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 278/300:  41%|████      | 9/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 278/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 278/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 278/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.64it/s]\u001b[A\n",
      "Epoch 278/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      " 93%|█████████▎| 278/300 [04:47<00:22,  1.03s/it]             \u001b[A\n",
      "Epoch 279/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 279/300:  14%|█▎        | 3/22 [00:00<00:00, 25.34it/s]\u001b[A\n",
      "Epoch 279/300:  27%|██▋       | 6/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "Epoch 279/300:  41%|████      | 9/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 279/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 279/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.08it/s]\u001b[A\n",
      "Epoch 279/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 279/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.75it/s]\u001b[A\n",
      " 93%|█████████▎| 279/300 [04:48<00:21,  1.03s/it]             \u001b[A\n",
      "Epoch 280/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 280/300:  14%|█▎        | 3/22 [00:00<00:00, 26.78it/s]\u001b[A\n",
      "Epoch 280/300:  27%|██▋       | 6/22 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "Epoch 280/300:  41%|████      | 9/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 280/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 280/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 280/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 280/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      " 93%|█████████▎| 280/300 [04:49<00:20,  1.02s/it]             \u001b[A\n",
      "Epoch 281/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 281/300:  14%|█▎        | 3/22 [00:00<00:00, 25.96it/s]\u001b[A\n",
      "Epoch 281/300:  27%|██▋       | 6/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 281/300:  41%|████      | 9/22 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "Epoch 281/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 281/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 281/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "Epoch 281/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      " 94%|█████████▎| 281/300 [04:50<00:19,  1.02s/it]             \u001b[A\n",
      "Epoch 282/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 282/300:  14%|█▎        | 3/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 282/300:  27%|██▋       | 6/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 282/300:  41%|████      | 9/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 282/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 282/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 282/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 282/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.70it/s]\u001b[A\n",
      " 94%|█████████▍| 282/300 [04:51<00:18,  1.02s/it]             \u001b[A\n",
      "Epoch 283/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 283/300:  14%|█▎        | 3/22 [00:00<00:00, 23.39it/s]\u001b[A\n",
      "Epoch 283/300:  27%|██▋       | 6/22 [00:00<00:00, 23.85it/s]\u001b[A\n",
      "Epoch 283/300:  41%|████      | 9/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      "Epoch 283/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "Epoch 283/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 283/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 283/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      " 94%|█████████▍| 283/300 [04:52<00:17,  1.03s/it]             \u001b[A\n",
      "Epoch 284/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 284/300:  14%|█▎        | 3/22 [00:00<00:00, 24.13it/s]\u001b[A\n",
      "Epoch 284/300:  27%|██▋       | 6/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 284/300:  41%|████      | 9/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 284/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      "Epoch 284/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "Epoch 284/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 284/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      " 95%|█████████▍| 284/300 [04:53<00:16,  1.03s/it]             \u001b[A\n",
      "Epoch 285/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 285/300:  14%|█▎        | 3/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 285/300:  27%|██▋       | 6/22 [00:00<00:00, 25.30it/s]\u001b[A\n",
      "Epoch 285/300:  41%|████      | 9/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      "Epoch 285/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 285/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 285/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.27it/s]\u001b[A\n",
      "Epoch 285/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      " 95%|█████████▌| 285/300 [04:54<00:15,  1.03s/it]             \u001b[A\n",
      "Epoch 286/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 286/300:  14%|█▎        | 3/22 [00:00<00:00, 26.65it/s]\u001b[A\n",
      "Epoch 286/300:  27%|██▋       | 6/22 [00:00<00:00, 25.22it/s]\u001b[A\n",
      "Epoch 286/300:  41%|████      | 9/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 286/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.37it/s]\u001b[A\n",
      "Epoch 286/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.05it/s]\u001b[A\n",
      "Epoch 286/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 286/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.67it/s]\u001b[A\n",
      " 95%|█████████▌| 286/300 [04:55<00:14,  1.02s/it]             \u001b[A\n",
      "Epoch 287/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 287/300:  14%|█▎        | 3/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      "Epoch 287/300:  27%|██▋       | 6/22 [00:00<00:00, 24.92it/s]\u001b[A\n",
      "Epoch 287/300:  41%|████      | 9/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 287/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.09it/s]\u001b[A\n",
      "Epoch 287/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 287/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 287/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.36it/s]\u001b[A\n",
      " 96%|█████████▌| 287/300 [04:56<00:13,  1.03s/it]             \u001b[A\n",
      "Epoch 288/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 288/300:  14%|█▎        | 3/22 [00:00<00:00, 24.54it/s]\u001b[A\n",
      "Epoch 288/300:  27%|██▋       | 6/22 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "Epoch 288/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 288/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "Epoch 288/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 288/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 288/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.78it/s]\u001b[A\n",
      " 96%|█████████▌| 288/300 [04:57<00:12,  1.02s/it]             \u001b[A\n",
      "Epoch 289/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 289/300:  14%|█▎        | 3/22 [00:00<00:00, 24.09it/s]\u001b[A\n",
      "Epoch 289/300:  27%|██▋       | 6/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 289/300:  41%|████      | 9/22 [00:00<00:00, 24.84it/s]\u001b[A\n",
      "Epoch 289/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.19it/s]\u001b[A\n",
      "Epoch 289/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.71it/s]\u001b[A\n",
      "Epoch 289/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.86it/s]\u001b[A\n",
      "Epoch 289/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.94it/s]\u001b[A\n",
      " 96%|█████████▋| 289/300 [04:58<00:11,  1.02s/it]             \u001b[A\n",
      "Epoch 290/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 290/300:  14%|█▎        | 3/22 [00:00<00:00, 26.05it/s]\u001b[A\n",
      "Epoch 290/300:  27%|██▋       | 6/22 [00:00<00:00, 25.52it/s]\u001b[A\n",
      "Epoch 290/300:  41%|████      | 9/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 290/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      "Epoch 290/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 290/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 290/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.48it/s]\u001b[A\n",
      " 97%|█████████▋| 290/300 [04:59<00:10,  1.03s/it]             \u001b[A\n",
      "Epoch 291/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 291/300:  14%|█▎        | 3/22 [00:00<00:00, 26.04it/s]\u001b[A\n",
      "Epoch 291/300:  27%|██▋       | 6/22 [00:00<00:00, 25.56it/s]\u001b[A\n",
      "Epoch 291/300:  41%|████      | 9/22 [00:00<00:00, 25.07it/s]\u001b[A\n",
      "Epoch 291/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "Epoch 291/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.38it/s]\u001b[A\n",
      "Epoch 291/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "Epoch 291/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 97%|█████████▋| 291/300 [05:00<00:09,  1.03s/it]             \u001b[A\n",
      "Epoch 292/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 292/300:  14%|█▎        | 3/22 [00:00<00:00, 25.47it/s]\u001b[A\n",
      "Epoch 292/300:  27%|██▋       | 6/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 292/300:  41%|████      | 9/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 292/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.95it/s]\u001b[A\n",
      "Epoch 292/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 292/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.33it/s]\u001b[A\n",
      "Epoch 292/300:  95%|█████████▌| 21/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      " 97%|█████████▋| 292/300 [05:01<00:08,  1.02s/it]             \u001b[A\n",
      "Epoch 293/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 293/300:  14%|█▎        | 3/22 [00:00<00:00, 23.93it/s]\u001b[A\n",
      "Epoch 293/300:  27%|██▋       | 6/22 [00:00<00:00, 24.18it/s]\u001b[A\n",
      "Epoch 293/300:  41%|████      | 9/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 293/300:  55%|█████▍    | 12/22 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "Epoch 293/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 293/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "Epoch 293/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.76it/s]\u001b[A\n",
      " 98%|█████████▊| 293/300 [05:02<00:07,  1.02s/it]             \u001b[A\n",
      "Epoch 294/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 294/300:  14%|█▎        | 3/22 [00:00<00:00, 26.02it/s]\u001b[A\n",
      "Epoch 294/300:  27%|██▋       | 6/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 294/300:  41%|████      | 9/22 [00:00<00:00, 24.50it/s]\u001b[A\n",
      "Epoch 294/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.49it/s]\u001b[A\n",
      "Epoch 294/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 294/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.13it/s]\u001b[A\n",
      "Epoch 294/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      " 98%|█████████▊| 294/300 [05:03<00:06,  1.02s/it]             \u001b[A\n",
      "Epoch 295/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 295/300:  14%|█▎        | 3/22 [00:00<00:00, 25.93it/s]\u001b[A\n",
      "Epoch 295/300:  27%|██▋       | 6/22 [00:00<00:00, 25.39it/s]\u001b[A\n",
      "Epoch 295/300:  41%|████      | 9/22 [00:00<00:00, 25.04it/s]\u001b[A\n",
      "Epoch 295/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.98it/s]\u001b[A\n",
      "Epoch 295/300:  68%|██████▊   | 15/22 [00:00<00:00, 25.20it/s]\u001b[A\n",
      "Epoch 295/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.43it/s]\u001b[A\n",
      "Epoch 295/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      " 98%|█████████▊| 295/300 [05:04<00:05,  1.02s/it]             \u001b[A\n",
      "Epoch 296/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 296/300:  14%|█▎        | 3/22 [00:00<00:00, 24.04it/s]\u001b[A\n",
      "Epoch 296/300:  27%|██▋       | 6/22 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "Epoch 296/300:  41%|████      | 9/22 [00:00<00:00, 24.83it/s]\u001b[A\n",
      "Epoch 296/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 296/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.68it/s]\u001b[A\n",
      "Epoch 296/300:  82%|████████▏ | 18/22 [00:00<00:00, 25.00it/s]\u001b[A\n",
      "Epoch 296/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.90it/s]\u001b[A\n",
      " 99%|█████████▊| 296/300 [05:05<00:04,  1.02s/it]             \u001b[A\n",
      "Epoch 297/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 297/300:  14%|█▎        | 3/22 [00:00<00:00, 25.16it/s]\u001b[A\n",
      "Epoch 297/300:  27%|██▋       | 6/22 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "Epoch 297/300:  41%|████      | 9/22 [00:00<00:00, 24.77it/s]\u001b[A\n",
      "Epoch 297/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.81it/s]\u001b[A\n",
      "Epoch 297/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "Epoch 297/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.99it/s]\u001b[A\n",
      "Epoch 297/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.27it/s]\u001b[A\n",
      " 99%|█████████▉| 297/300 [05:06<00:03,  1.02s/it]             \u001b[A\n",
      "Epoch 298/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 298/300:  14%|█▎        | 3/22 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "Epoch 298/300:  27%|██▋       | 6/22 [00:00<00:00, 24.73it/s]\u001b[A\n",
      "Epoch 298/300:  41%|████      | 9/22 [00:00<00:00, 24.80it/s]\u001b[A\n",
      "Epoch 298/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.63it/s]\u001b[A\n",
      "Epoch 298/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "Epoch 298/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "Epoch 298/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.85it/s]\u001b[A\n",
      " 99%|█████████▉| 298/300 [05:07<00:02,  1.03s/it]             \u001b[A\n",
      "Epoch 299/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299/300:  14%|█▎        | 3/22 [00:00<00:00, 25.06it/s]\u001b[A\n",
      "Epoch 299/300:  27%|██▋       | 6/22 [00:00<00:00, 25.02it/s]\u001b[A\n",
      "Epoch 299/300:  41%|████      | 9/22 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "Epoch 299/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.55it/s]\u001b[A\n",
      "Epoch 299/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 299/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "Epoch 299/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.57it/s]\u001b[A\n",
      "100%|█████████▉| 299/300 [05:08<00:01,  1.03s/it]             \u001b[A\n",
      "Epoch 300/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 300/300:  14%|█▎        | 3/22 [00:00<00:00, 25.73it/s]\u001b[A\n",
      "Epoch 300/300:  27%|██▋       | 6/22 [00:00<00:00, 25.10it/s]\u001b[A\n",
      "Epoch 300/300:  41%|████      | 9/22 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "Epoch 300/300:  55%|█████▍    | 12/22 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "Epoch 300/300:  68%|██████▊   | 15/22 [00:00<00:00, 24.12it/s]\u001b[A\n",
      "Epoch 300/300:  82%|████████▏ | 18/22 [00:00<00:00, 24.52it/s]\u001b[A\n",
      "Epoch 300/300:  95%|█████████▌| 21/22 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "100%|██████████| 300/300 [05:09<00:00,  1.03s/it]             \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 Results ---\n",
      "Hyperparameters:\n",
      "  hidden_channels: 64\n",
      "  num_layers: 2\n",
      "  dropout_rate: 0.3449171314300249\n",
      "  lr: 1.2194748129279183e-05\n",
      "  weight_decay: 9.74954930786132e-05\n",
      "  optimizer: Adam\n",
      "  activation: LeakyReLU\n",
      "  skip_connection: True\n",
      "  grad_clip: 0.5671836123912031\n",
      "  self_loops: True\n",
      "  k_val: 12\n",
      "  use_augmentation: False\n",
      "\n",
      "Current Model Architecture:\n",
      "HeteroMetaLayerGNN(\n",
      "  (activation): LeakyReLU(negative_slope=0.01)\n",
      "  (node_encoders): ModuleDict(\n",
      "    (A): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (B): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (C): Linear(in_features=6, out_features=64, bias=True)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x MetaLayer(\n",
      "      edge_model=EdgeModel(\n",
      "      (edge_mlp): Sequential(\n",
      "        (0): Linear(in_features=129, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (5): LeakyReLU(negative_slope=0.01)\n",
      "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    ),\n",
      "      node_model=NodeModel(\n",
      "      (node_mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    ),\n",
      "      global_model=GlobalModel(\n",
      "      (global_mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Dropout(p=0.3449171314300249, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Best Validation Loss for this trial: -6.89247e+00\n",
      "Best Overall Validation Loss: -6.89247e+00 (Trial 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-21 20:42:48,553] Trial 0 finished with value: -6.892471027374268 and parameters: {'hidden_channels': 64, 'num_layers': 2, 'dropout_rate': 0.3449171314300249, 'lr': 1.2194748129279183e-05, 'weight_decay': 9.74954930786132e-05, 'optimizer': 'Adam', 'activation': 'LeakyReLU', 'skip_connection': True, 'grad_clip': 0.5671836123912031, 'self_loops': True, 'k_val': 12, 'use_augmentation': False}. Best is trial 0 with value: -6.892471027374268.\n",
      "Loading graphs: 100%|██████████| 1000/1000 [03:28<00:00,  4.79it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\n",
      "Epoch 1/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1/300:   5%|▍         | 1/22 [00:00<00:08,  2.54it/s]\u001b[A\n",
      "Epoch 1/300:  14%|█▎        | 3/22 [00:00<00:02,  6.34it/s]\u001b[A\n",
      "Epoch 1/300:  23%|██▎       | 5/22 [00:00<00:01,  8.50it/s]\u001b[A\n",
      "Epoch 1/300:  32%|███▏      | 7/22 [00:00<00:01,  9.93it/s]\u001b[A\n",
      "Epoch 1/300:  41%|████      | 9/22 [00:01<00:01, 10.84it/s]\u001b[A\n",
      "Epoch 1/300:  50%|█████     | 11/22 [00:01<00:00, 11.19it/s]\u001b[A\n",
      "Epoch 1/300:  59%|█████▉    | 13/22 [00:01<00:00, 11.29it/s]\u001b[A\n",
      "Epoch 1/300:  68%|██████▊   | 15/22 [00:01<00:00, 11.82it/s]\u001b[A\n",
      "Epoch 1/300:  77%|███████▋  | 17/22 [00:01<00:00, 12.31it/s]\u001b[A\n",
      "Epoch 1/300:  86%|████████▋ | 19/22 [00:01<00:00, 12.05it/s]\u001b[A\n",
      "Epoch 1/300:  95%|█████████▌| 21/22 [00:01<00:00, 12.41it/s]\u001b[A\n",
      "  0%|          | 1/300 [00:02<11:29,  2.31s/it]             \u001b[A\n",
      "Epoch 2/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2/300:   9%|▉         | 2/22 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "Epoch 2/300:  18%|█▊        | 4/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 2/300:  27%|██▋       | 6/22 [00:00<00:01, 12.96it/s]\u001b[A\n",
      "Epoch 2/300:  36%|███▋      | 8/22 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "Epoch 2/300:  45%|████▌     | 10/22 [00:00<00:00, 12.91it/s]\u001b[A\n",
      "Epoch 2/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.12it/s]\u001b[A\n",
      "Epoch 2/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 2/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 2/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 2/300:  91%|█████████ | 20/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 2/300: 100%|██████████| 22/22 [00:01<00:00, 13.88it/s]\u001b[A\n",
      "  1%|          | 2/300 [00:04<10:07,  2.04s/it]             \u001b[A\n",
      "Epoch 3/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3/300:   9%|▉         | 2/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 3/300:  18%|█▊        | 4/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
      "Epoch 3/300:  27%|██▋       | 6/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 3/300:  36%|███▋      | 8/22 [00:00<00:01, 13.50it/s]\u001b[A\n",
      "Epoch 3/300:  45%|████▌     | 10/22 [00:00<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 3/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 3/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 3/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 3/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 3/300:  91%|█████████ | 20/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 3/300: 100%|██████████| 22/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "  1%|          | 3/300 [00:05<09:37,  1.95s/it]             \u001b[A\n",
      "Epoch 4/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4/300:   9%|▉         | 2/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 4/300:  18%|█▊        | 4/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 4/300:  27%|██▋       | 6/22 [00:00<00:01, 12.99it/s]\u001b[A\n",
      "Epoch 4/300:  36%|███▋      | 8/22 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Epoch 4/300:  45%|████▌     | 10/22 [00:00<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 4/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 4/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 4/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.17it/s]\u001b[A\n",
      "Epoch 4/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.42it/s]\u001b[A\n",
      "Epoch 4/300:  91%|█████████ | 20/22 [00:01<00:00, 14.41it/s]\u001b[A\n",
      "Epoch 4/300: 100%|██████████| 22/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "  1%|▏         | 4/300 [00:07<09:19,  1.89s/it]             \u001b[A\n",
      "Epoch 5/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5/300:   9%|▉         | 2/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 5/300:  18%|█▊        | 4/22 [00:00<00:01, 13.56it/s]\u001b[A\n",
      "Epoch 5/300:  27%|██▋       | 6/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 5/300:  36%|███▋      | 8/22 [00:00<00:00, 14.11it/s]\u001b[A\n",
      "Epoch 5/300:  45%|████▌     | 10/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 5/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 5/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 5/300:  73%|███████▎  | 16/22 [00:01<00:00, 12.83it/s]\u001b[A\n",
      "Epoch 5/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.20it/s]\u001b[A\n",
      "Epoch 5/300:  91%|█████████ | 20/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 5/300: 100%|██████████| 22/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "  2%|▏         | 5/300 [00:09<09:12,  1.87s/it]             \u001b[A\n",
      "Epoch 6/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6/300:   9%|▉         | 2/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 6/300:  18%|█▊        | 4/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 6/300:  27%|██▋       | 6/22 [00:00<00:01, 12.71it/s]\u001b[A\n",
      "Epoch 6/300:  36%|███▋      | 8/22 [00:00<00:01, 13.16it/s]\u001b[A\n",
      "Epoch 6/300:  45%|████▌     | 10/22 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 6/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 6/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 6/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 6/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.28it/s]\u001b[A\n",
      "Epoch 6/300:  91%|█████████ | 20/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 6/300: 100%|██████████| 22/22 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "  2%|▏         | 6/300 [00:11<09:06,  1.86s/it]             \u001b[A\n",
      "Epoch 7/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7/300:   9%|▉         | 2/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 7/300:  18%|█▊        | 4/22 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "Epoch 7/300:  27%|██▋       | 6/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 7/300:  36%|███▋      | 8/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 7/300:  45%|████▌     | 10/22 [00:00<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 7/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 7/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 7/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 7/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 7/300:  91%|█████████ | 20/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 7/300: 100%|██████████| 22/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "  2%|▏         | 7/300 [00:13<09:01,  1.85s/it]             \u001b[A\n",
      "Epoch 8/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8/300:   9%|▉         | 2/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 8/300:  18%|█▊        | 4/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 8/300:  27%|██▋       | 6/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 8/300:  36%|███▋      | 8/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 8/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 8/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 8/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 8/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 8/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 8/300:  91%|█████████ | 20/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 8/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "  3%|▎         | 8/300 [00:15<08:57,  1.84s/it]             \u001b[A\n",
      "Epoch 9/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9/300:   9%|▉         | 2/22 [00:00<00:01, 12.43it/s]\u001b[A\n",
      "Epoch 9/300:  18%|█▊        | 4/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 9/300:  27%|██▋       | 6/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 9/300:  36%|███▋      | 8/22 [00:00<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 9/300:  45%|████▌     | 10/22 [00:00<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 9/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 9/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 9/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 9/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 9/300:  91%|█████████ | 20/22 [00:01<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 9/300: 100%|██████████| 22/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "  3%|▎         | 9/300 [00:16<08:53,  1.83s/it]             \u001b[A\n",
      "Epoch 10/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10/300:   9%|▉         | 2/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 10/300:  18%|█▊        | 4/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 10/300:  27%|██▋       | 6/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 10/300:  36%|███▋      | 8/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 10/300:  45%|████▌     | 10/22 [00:00<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 10/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 10/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 10/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 10/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 10/300:  91%|█████████ | 20/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 10/300: 100%|██████████| 22/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "  3%|▎         | 10/300 [00:18<08:50,  1.83s/it]             \u001b[A\n",
      "Epoch 11/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11/300:   9%|▉         | 2/22 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "Epoch 11/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 11/300:  27%|██▋       | 6/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 11/300:  36%|███▋      | 8/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 11/300:  45%|████▌     | 10/22 [00:00<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 11/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.32it/s]\u001b[A\n",
      "Epoch 11/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 11/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 11/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 11/300:  91%|█████████ | 20/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 11/300: 100%|██████████| 22/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "  4%|▎         | 11/300 [00:20<08:47,  1.83s/it]             \u001b[A\n",
      "Epoch 12/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12/300:   9%|▉         | 2/22 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "Epoch 12/300:  18%|█▊        | 4/22 [00:00<00:01, 14.24it/s]\u001b[A\n",
      "Epoch 12/300:  27%|██▋       | 6/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 12/300:  36%|███▋      | 8/22 [00:00<00:00, 14.28it/s]\u001b[A\n",
      "Epoch 12/300:  45%|████▌     | 10/22 [00:00<00:00, 14.14it/s]\u001b[A\n",
      "Epoch 12/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 12/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 12/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 12/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 12/300:  91%|█████████ | 20/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 12/300: 100%|██████████| 22/22 [00:01<00:00, 14.07it/s]\u001b[A\n",
      "  4%|▍         | 12/300 [00:22<08:43,  1.82s/it]             \u001b[A\n",
      "Epoch 13/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13/300:   9%|▉         | 2/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 13/300:  18%|█▊        | 4/22 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "Epoch 13/300:  27%|██▋       | 6/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 13/300:  36%|███▋      | 8/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 13/300:  45%|████▌     | 10/22 [00:00<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 13/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 13/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 13/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 13/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 13/300:  91%|█████████ | 20/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 13/300: 100%|██████████| 22/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      "  4%|▍         | 13/300 [00:24<08:40,  1.81s/it]             \u001b[A\n",
      "Epoch 14/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14/300:   9%|▉         | 2/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 14/300:  18%|█▊        | 4/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 14/300:  27%|██▋       | 6/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 14/300:  36%|███▋      | 8/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 14/300:  45%|████▌     | 10/22 [00:00<00:00, 14.44it/s]\u001b[A\n",
      "Epoch 14/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 14/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 14/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 14/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 14/300:  91%|█████████ | 20/22 [00:01<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 14/300: 100%|██████████| 22/22 [00:01<00:00, 14.02it/s]\u001b[A\n",
      "  5%|▍         | 14/300 [00:25<08:37,  1.81s/it]             \u001b[A\n",
      "Epoch 15/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15/300:   9%|▉         | 2/22 [00:00<00:01, 13.03it/s]\u001b[A\n",
      "Epoch 15/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 15/300:  27%|██▋       | 6/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 15/300:  36%|███▋      | 8/22 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "Epoch 15/300:  45%|████▌     | 10/22 [00:00<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 15/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.00it/s]\u001b[A\n",
      "Epoch 15/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.00it/s]\u001b[A\n",
      "Epoch 15/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.06it/s]\u001b[A\n",
      "Epoch 15/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.30it/s]\u001b[A\n",
      "Epoch 15/300:  91%|█████████ | 20/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 15/300: 100%|██████████| 22/22 [00:01<00:00, 14.04it/s]\u001b[A\n",
      "  5%|▌         | 15/300 [00:27<08:35,  1.81s/it]             \u001b[A\n",
      "Epoch 16/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16/300:   9%|▉         | 2/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 16/300:  18%|█▊        | 4/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 16/300:  27%|██▋       | 6/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 16/300:  36%|███▋      | 8/22 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "Epoch 16/300:  45%|████▌     | 10/22 [00:00<00:00, 13.03it/s]\u001b[A\n",
      "Epoch 16/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 16/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 16/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 16/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.16it/s]\u001b[A\n",
      "Epoch 16/300:  91%|█████████ | 20/22 [00:01<00:00, 14.04it/s]\u001b[A\n",
      "Epoch 16/300: 100%|██████████| 22/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "  5%|▌         | 16/300 [00:29<08:34,  1.81s/it]             \u001b[A\n",
      "Epoch 17/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17/300:   9%|▉         | 2/22 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "Epoch 17/300:  18%|█▊        | 4/22 [00:00<00:01, 14.43it/s]\u001b[A\n",
      "Epoch 17/300:  27%|██▋       | 6/22 [00:00<00:01, 14.16it/s]\u001b[A\n",
      "Epoch 17/300:  36%|███▋      | 8/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 17/300:  45%|████▌     | 10/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 17/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 17/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 17/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 17/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 17/300:  91%|█████████ | 20/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 17/300: 100%|██████████| 22/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "  6%|▌         | 17/300 [00:31<08:34,  1.82s/it]             \u001b[A\n",
      "Epoch 18/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18/300:   9%|▉         | 2/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 18/300:  18%|█▊        | 4/22 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "Epoch 18/300:  27%|██▋       | 6/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 18/300:  36%|███▋      | 8/22 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "Epoch 18/300:  45%|████▌     | 10/22 [00:00<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 18/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 18/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 18/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 18/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 18/300:  91%|█████████ | 20/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 18/300: 100%|██████████| 22/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "  6%|▌         | 18/300 [00:33<08:34,  1.82s/it]             \u001b[A\n",
      "Epoch 19/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19/300:   9%|▉         | 2/22 [00:00<00:01, 12.87it/s]\u001b[A\n",
      "Epoch 19/300:  18%|█▊        | 4/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
      "Epoch 19/300:  27%|██▋       | 6/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 19/300:  36%|███▋      | 8/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 19/300:  45%|████▌     | 10/22 [00:00<00:00, 14.31it/s]\u001b[A\n",
      "Epoch 19/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 19/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 19/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 19/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 19/300:  91%|█████████ | 20/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 19/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "  6%|▋         | 19/300 [00:35<08:31,  1.82s/it]             \u001b[A\n",
      "Epoch 20/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20/300:   9%|▉         | 2/22 [00:00<00:01, 14.54it/s]\u001b[A\n",
      "Epoch 20/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 20/300:  27%|██▋       | 6/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 20/300:  36%|███▋      | 8/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 20/300:  45%|████▌     | 10/22 [00:00<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 20/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 20/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.30it/s]\u001b[A\n",
      "Epoch 20/300:  73%|███████▎  | 16/22 [00:01<00:00, 12.86it/s]\u001b[A\n",
      "Epoch 20/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 20/300:  91%|█████████ | 20/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 20/300: 100%|██████████| 22/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "  7%|▋         | 20/300 [00:36<08:31,  1.83s/it]             \u001b[A\n",
      "Epoch 21/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21/300:   9%|▉         | 2/22 [00:00<00:01, 14.36it/s]\u001b[A\n",
      "Epoch 21/300:  18%|█▊        | 4/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "Epoch 21/300:  27%|██▋       | 6/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 21/300:  36%|███▋      | 8/22 [00:00<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 21/300:  45%|████▌     | 10/22 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 21/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 21/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 21/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 21/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 21/300:  91%|█████████ | 20/22 [00:01<00:00, 13.10it/s]\u001b[A\n",
      "Epoch 21/300: 100%|██████████| 22/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "  7%|▋         | 21/300 [00:38<08:29,  1.83s/it]             \u001b[A\n",
      "Epoch 22/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22/300:   9%|▉         | 2/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 22/300:  18%|█▊        | 4/22 [00:00<00:01, 13.56it/s]\u001b[A\n",
      "Epoch 22/300:  27%|██▋       | 6/22 [00:00<00:01, 13.23it/s]\u001b[A\n",
      "Epoch 22/300:  36%|███▋      | 8/22 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "Epoch 22/300:  45%|████▌     | 10/22 [00:00<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 22/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 22/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.26it/s]\u001b[A\n",
      "Epoch 22/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 22/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 22/300:  91%|█████████ | 20/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 22/300: 100%|██████████| 22/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "  7%|▋         | 22/300 [00:40<08:28,  1.83s/it]             \u001b[A\n",
      "Epoch 23/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23/300:   9%|▉         | 2/22 [00:00<00:01, 12.78it/s]\u001b[A\n",
      "Epoch 23/300:  18%|█▊        | 4/22 [00:00<00:01, 13.51it/s]\u001b[A\n",
      "Epoch 23/300:  27%|██▋       | 6/22 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "Epoch 23/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 23/300:  45%|████▌     | 10/22 [00:00<00:00, 14.16it/s]\u001b[A\n",
      "Epoch 23/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.23it/s]\u001b[A\n",
      "Epoch 23/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      "Epoch 23/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 23/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 23/300:  91%|█████████ | 20/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 23/300: 100%|██████████| 22/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "  8%|▊         | 23/300 [00:42<08:24,  1.82s/it]             \u001b[A\n",
      "Epoch 24/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24/300:   9%|▉         | 2/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 24/300:  18%|█▊        | 4/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 24/300:  27%|██▋       | 6/22 [00:00<00:01, 14.16it/s]\u001b[A\n",
      "Epoch 24/300:  36%|███▋      | 8/22 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "Epoch 24/300:  45%|████▌     | 10/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 24/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 24/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 24/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.13it/s]\u001b[A\n",
      "Epoch 24/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 24/300:  91%|█████████ | 20/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 24/300: 100%|██████████| 22/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "  8%|▊         | 24/300 [00:44<08:22,  1.82s/it]             \u001b[A\n",
      "Epoch 25/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25/300:   9%|▉         | 2/22 [00:00<00:01, 11.92it/s]\u001b[A\n",
      "Epoch 25/300:  18%|█▊        | 4/22 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "Epoch 25/300:  27%|██▋       | 6/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 25/300:  36%|███▋      | 8/22 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "Epoch 25/300:  45%|████▌     | 10/22 [00:00<00:00, 14.12it/s]\u001b[A\n",
      "Epoch 25/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.04it/s]\u001b[A\n",
      "Epoch 25/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.07it/s]\u001b[A\n",
      "Epoch 25/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 25/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 25/300:  91%|█████████ | 20/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 25/300: 100%|██████████| 22/22 [00:01<00:00, 14.26it/s]\u001b[A\n",
      "  8%|▊         | 25/300 [00:46<08:19,  1.82s/it]             \u001b[A\n",
      "Epoch 26/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26/300:   9%|▉         | 2/22 [00:00<00:01, 13.23it/s]\u001b[A\n",
      "Epoch 26/300:  18%|█▊        | 4/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 26/300:  27%|██▋       | 6/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 26/300:  36%|███▋      | 8/22 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "Epoch 26/300:  45%|████▌     | 10/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 26/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 26/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 26/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 26/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 26/300:  91%|█████████ | 20/22 [00:01<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 26/300: 100%|██████████| 22/22 [00:01<00:00, 14.25it/s]\u001b[A\n",
      "  9%|▊         | 26/300 [00:47<08:16,  1.81s/it]             \u001b[A\n",
      "Epoch 27/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27/300:   9%|▉         | 2/22 [00:00<00:01, 15.00it/s]\u001b[A\n",
      "Epoch 27/300:  18%|█▊        | 4/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 27/300:  27%|██▋       | 6/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 27/300:  36%|███▋      | 8/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 27/300:  45%|████▌     | 10/22 [00:00<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 27/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 27/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 27/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 27/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 27/300:  91%|█████████ | 20/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 27/300: 100%|██████████| 22/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "  9%|▉         | 27/300 [00:49<08:14,  1.81s/it]             \u001b[A\n",
      "Epoch 28/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28/300:   9%|▉         | 2/22 [00:00<00:01, 15.63it/s]\u001b[A\n",
      "Epoch 28/300:  18%|█▊        | 4/22 [00:00<00:01, 14.12it/s]\u001b[A\n",
      "Epoch 28/300:  27%|██▋       | 6/22 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "Epoch 28/300:  36%|███▋      | 8/22 [00:00<00:00, 14.12it/s]\u001b[A\n",
      "Epoch 28/300:  45%|████▌     | 10/22 [00:00<00:00, 14.26it/s]\u001b[A\n",
      "Epoch 28/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.26it/s]\u001b[A\n",
      "Epoch 28/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 28/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 28/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 28/300:  91%|█████████ | 20/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 28/300: 100%|██████████| 22/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "  9%|▉         | 28/300 [00:51<08:12,  1.81s/it]             \u001b[A\n",
      "Epoch 29/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29/300:   9%|▉         | 2/22 [00:00<00:01, 14.24it/s]\u001b[A\n",
      "Epoch 29/300:  18%|█▊        | 4/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 29/300:  27%|██▋       | 6/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 29/300:  36%|███▋      | 8/22 [00:00<00:00, 14.06it/s]\u001b[A\n",
      "Epoch 29/300:  45%|████▌     | 10/22 [00:00<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 29/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 29/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 29/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 29/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 29/300:  91%|█████████ | 20/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 29/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 10%|▉         | 29/300 [00:53<08:11,  1.81s/it]             \u001b[A\n",
      "Epoch 30/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30/300:   9%|▉         | 2/22 [00:00<00:01, 14.06it/s]\u001b[A\n",
      "Epoch 30/300:  18%|█▊        | 4/22 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "Epoch 30/300:  27%|██▋       | 6/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 30/300:  36%|███▋      | 8/22 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "Epoch 30/300:  45%|████▌     | 10/22 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 30/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 30/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 30/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 30/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 30/300:  91%|█████████ | 20/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 30/300: 100%|██████████| 22/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      " 10%|█         | 30/300 [00:55<08:10,  1.82s/it]             \u001b[A\n",
      "Epoch 31/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31/300:   9%|▉         | 2/22 [00:00<00:01, 12.68it/s]\u001b[A\n",
      "Epoch 31/300:  18%|█▊        | 4/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 31/300:  27%|██▋       | 6/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 31/300:  36%|███▋      | 8/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 31/300:  45%|████▌     | 10/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 31/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 31/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 31/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 31/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.20it/s]\u001b[A\n",
      "Epoch 31/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 31/300: 100%|██████████| 22/22 [00:01<00:00, 14.06it/s]\u001b[A\n",
      " 10%|█         | 31/300 [00:56<08:08,  1.82s/it]             \u001b[A\n",
      "Epoch 32/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32/300:   9%|▉         | 2/22 [00:00<00:01, 14.01it/s]\u001b[A\n",
      "Epoch 32/300:  18%|█▊        | 4/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 32/300:  27%|██▋       | 6/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 32/300:  36%|███▋      | 8/22 [00:00<00:01, 13.31it/s]\u001b[A\n",
      "Epoch 32/300:  45%|████▌     | 10/22 [00:00<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 32/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 32/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 32/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 32/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 32/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 32/300: 100%|██████████| 22/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      " 11%|█         | 32/300 [00:58<08:07,  1.82s/it]             \u001b[A\n",
      "Epoch 33/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33/300:   9%|▉         | 2/22 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "Epoch 33/300:  18%|█▊        | 4/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 33/300:  27%|██▋       | 6/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 33/300:  36%|███▋      | 8/22 [00:00<00:01, 13.27it/s]\u001b[A\n",
      "Epoch 33/300:  45%|████▌     | 10/22 [00:00<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 33/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 33/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 33/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 33/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 33/300:  91%|█████████ | 20/22 [00:01<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 33/300: 100%|██████████| 22/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      " 11%|█         | 33/300 [01:00<08:06,  1.82s/it]             \u001b[A\n",
      "Epoch 34/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34/300:   9%|▉         | 2/22 [00:00<00:01, 13.04it/s]\u001b[A\n",
      "Epoch 34/300:  18%|█▊        | 4/22 [00:00<00:01, 12.57it/s]\u001b[A\n",
      "Epoch 34/300:  27%|██▋       | 6/22 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "Epoch 34/300:  36%|███▋      | 8/22 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "Epoch 34/300:  45%|████▌     | 10/22 [00:00<00:00, 13.09it/s]\u001b[A\n",
      "Epoch 34/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 34/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 34/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 34/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 34/300:  91%|█████████ | 20/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 34/300: 100%|██████████| 22/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      " 11%|█▏        | 34/300 [01:02<08:06,  1.83s/it]             \u001b[A\n",
      "Epoch 35/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35/300:   9%|▉         | 2/22 [00:00<00:01, 12.61it/s]\u001b[A\n",
      "Epoch 35/300:  18%|█▊        | 4/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 35/300:  27%|██▋       | 6/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 35/300:  36%|███▋      | 8/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 35/300:  45%|████▌     | 10/22 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 35/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 35/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 35/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 35/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 35/300:  91%|█████████ | 20/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 35/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 12%|█▏        | 35/300 [01:04<08:04,  1.83s/it]             \u001b[A\n",
      "Epoch 36/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36/300:   9%|▉         | 2/22 [00:00<00:01, 13.93it/s]\u001b[A\n",
      "Epoch 36/300:  18%|█▊        | 4/22 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "Epoch 36/300:  27%|██▋       | 6/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 36/300:  36%|███▋      | 8/22 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "Epoch 36/300:  45%|████▌     | 10/22 [00:00<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 36/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 36/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 36/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 36/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 36/300:  91%|█████████ | 20/22 [00:01<00:00, 13.23it/s]\u001b[A\n",
      "Epoch 36/300: 100%|██████████| 22/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      " 12%|█▏        | 36/300 [01:06<08:04,  1.83s/it]             \u001b[A\n",
      "Epoch 37/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37/300:   9%|▉         | 2/22 [00:00<00:01, 14.26it/s]\u001b[A\n",
      "Epoch 37/300:  18%|█▊        | 4/22 [00:00<00:01, 14.74it/s]\u001b[A\n",
      "Epoch 37/300:  27%|██▋       | 6/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 37/300:  36%|███▋      | 8/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 37/300:  45%|████▌     | 10/22 [00:00<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 37/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 37/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 37/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 37/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 37/300:  91%|█████████ | 20/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 37/300: 100%|██████████| 22/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      " 12%|█▏        | 37/300 [01:07<08:01,  1.83s/it]             \u001b[A\n",
      "Epoch 38/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38/300:   9%|▉         | 2/22 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "Epoch 38/300:  18%|█▊        | 4/22 [00:00<00:01, 14.15it/s]\u001b[A\n",
      "Epoch 38/300:  27%|██▋       | 6/22 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "Epoch 38/300:  36%|███▋      | 8/22 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "Epoch 38/300:  45%|████▌     | 10/22 [00:00<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 38/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 38/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 38/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 38/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 38/300:  91%|█████████ | 20/22 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 38/300: 100%|██████████| 22/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      " 13%|█▎        | 38/300 [01:09<07:58,  1.83s/it]             \u001b[A\n",
      "Epoch 39/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39/300:   9%|▉         | 2/22 [00:00<00:01, 12.96it/s]\u001b[A\n",
      "Epoch 39/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 39/300:  27%|██▋       | 6/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 39/300:  36%|███▋      | 8/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 39/300:  45%|████▌     | 10/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 39/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 39/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 39/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 39/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 39/300:  91%|█████████ | 20/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 39/300: 100%|██████████| 22/22 [00:01<00:00, 14.34it/s]\u001b[A\n",
      " 13%|█▎        | 39/300 [01:11<07:56,  1.83s/it]             \u001b[A\n",
      "Epoch 40/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40/300:   9%|▉         | 2/22 [00:00<00:01, 12.76it/s]\u001b[A\n",
      "Epoch 40/300:  18%|█▊        | 4/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 40/300:  27%|██▋       | 6/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 40/300:  36%|███▋      | 8/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 40/300:  45%|████▌     | 10/22 [00:00<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 40/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 40/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 40/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 40/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 40/300:  91%|█████████ | 20/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 40/300: 100%|██████████| 22/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      " 13%|█▎        | 40/300 [01:13<07:55,  1.83s/it]             \u001b[A\n",
      "Epoch 41/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41/300:   9%|▉         | 2/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 41/300:  18%|█▊        | 4/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 41/300:  27%|██▋       | 6/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 41/300:  36%|███▋      | 8/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 41/300:  45%|████▌     | 10/22 [00:00<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 41/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 41/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 41/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 41/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.30it/s]\u001b[A\n",
      "Epoch 41/300:  91%|█████████ | 20/22 [00:01<00:00, 12.98it/s]\u001b[A\n",
      "Epoch 41/300: 100%|██████████| 22/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      " 14%|█▎        | 41/300 [01:15<07:55,  1.84s/it]             \u001b[A\n",
      "Epoch 42/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42/300:   9%|▉         | 2/22 [00:00<00:01, 13.21it/s]\u001b[A\n",
      "Epoch 42/300:  18%|█▊        | 4/22 [00:00<00:01, 13.06it/s]\u001b[A\n",
      "Epoch 42/300:  27%|██▋       | 6/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 42/300:  36%|███▋      | 8/22 [00:00<00:01, 12.97it/s]\u001b[A\n",
      "Epoch 42/300:  45%|████▌     | 10/22 [00:00<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 42/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 42/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 42/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 42/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.19it/s]\u001b[A\n",
      "Epoch 42/300:  91%|█████████ | 20/22 [00:01<00:00, 14.42it/s]\u001b[A\n",
      "Epoch 42/300: 100%|██████████| 22/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      " 14%|█▍        | 42/300 [01:17<07:52,  1.83s/it]             \u001b[A\n",
      "Epoch 43/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43/300:   9%|▉         | 2/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 43/300:  18%|█▊        | 4/22 [00:00<00:01, 13.23it/s]\u001b[A\n",
      "Epoch 43/300:  27%|██▋       | 6/22 [00:00<00:01, 13.10it/s]\u001b[A\n",
      "Epoch 43/300:  36%|███▋      | 8/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 43/300:  45%|████▌     | 10/22 [00:00<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 43/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 43/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 43/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 43/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 43/300:  91%|█████████ | 20/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 43/300: 100%|██████████| 22/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      " 14%|█▍        | 43/300 [01:18<07:50,  1.83s/it]             \u001b[A\n",
      "Epoch 44/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44/300:   9%|▉         | 2/22 [00:00<00:01, 13.91it/s]\u001b[A\n",
      "Epoch 44/300:  18%|█▊        | 4/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 44/300:  27%|██▋       | 6/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 44/300:  36%|███▋      | 8/22 [00:00<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 44/300:  45%|████▌     | 10/22 [00:00<00:00, 14.18it/s]\u001b[A\n",
      "Epoch 44/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.88it/s]\u001b[A\n",
      "Epoch 44/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 44/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 44/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 44/300:  91%|█████████ | 20/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 44/300: 100%|██████████| 22/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      " 15%|█▍        | 44/300 [01:20<07:46,  1.82s/it]             \u001b[A\n",
      "Epoch 45/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45/300:   9%|▉         | 2/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 45/300:  18%|█▊        | 4/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 45/300:  27%|██▋       | 6/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 45/300:  36%|███▋      | 8/22 [00:00<00:01, 13.60it/s]\u001b[A\n",
      "Epoch 45/300:  45%|████▌     | 10/22 [00:00<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 45/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 45/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 45/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 45/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.12it/s]\u001b[A\n",
      "Epoch 45/300:  91%|█████████ | 20/22 [00:01<00:00, 13.11it/s]\u001b[A\n",
      "Epoch 45/300: 100%|██████████| 22/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      " 15%|█▌        | 45/300 [01:22<07:46,  1.83s/it]             \u001b[A\n",
      "Epoch 46/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46/300:   9%|▉         | 2/22 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Epoch 46/300:  18%|█▊        | 4/22 [00:00<00:01, 13.51it/s]\u001b[A\n",
      "Epoch 46/300:  27%|██▋       | 6/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 46/300:  36%|███▋      | 8/22 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "Epoch 46/300:  45%|████▌     | 10/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 46/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 46/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 46/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 46/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 46/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 46/300: 100%|██████████| 22/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      " 15%|█▌        | 46/300 [01:24<07:44,  1.83s/it]             \u001b[A\n",
      "Epoch 47/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47/300:   9%|▉         | 2/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 47/300:  18%|█▊        | 4/22 [00:00<00:01, 13.21it/s]\u001b[A\n",
      "Epoch 47/300:  27%|██▋       | 6/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 47/300:  36%|███▋      | 8/22 [00:00<00:01, 13.24it/s]\u001b[A\n",
      "Epoch 47/300:  45%|████▌     | 10/22 [00:00<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 47/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 47/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 47/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 47/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 47/300:  91%|█████████ | 20/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 47/300: 100%|██████████| 22/22 [00:01<00:00, 14.20it/s]\u001b[A\n",
      " 16%|█▌        | 47/300 [01:26<07:41,  1.82s/it]             \u001b[A\n",
      "Epoch 48/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48/300:   9%|▉         | 2/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 48/300:  18%|█▊        | 4/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 48/300:  27%|██▋       | 6/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 48/300:  36%|███▋      | 8/22 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "Epoch 48/300:  45%|████▌     | 10/22 [00:00<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 48/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 48/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 48/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 48/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 48/300:  91%|█████████ | 20/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 48/300: 100%|██████████| 22/22 [00:01<00:00, 14.30it/s]\u001b[A\n",
      " 16%|█▌        | 48/300 [01:27<07:38,  1.82s/it]             \u001b[A\n",
      "Epoch 49/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49/300:   9%|▉         | 2/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 49/300:  18%|█▊        | 4/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 49/300:  27%|██▋       | 6/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 49/300:  36%|███▋      | 8/22 [00:00<00:01, 13.59it/s]\u001b[A\n",
      "Epoch 49/300:  45%|████▌     | 10/22 [00:00<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 49/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 49/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 49/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 49/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.18it/s]\u001b[A\n",
      "Epoch 49/300:  91%|█████████ | 20/22 [00:01<00:00, 14.14it/s]\u001b[A\n",
      "Epoch 49/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      " 16%|█▋        | 49/300 [01:29<07:35,  1.81s/it]             \u001b[A\n",
      "Epoch 50/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50/300:   9%|▉         | 2/22 [00:00<00:01, 13.19it/s]\u001b[A\n",
      "Epoch 50/300:  18%|█▊        | 4/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 50/300:  27%|██▋       | 6/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 50/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 50/300:  45%|████▌     | 10/22 [00:00<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 50/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.36it/s]\u001b[A\n",
      "Epoch 50/300:  64%|██████▎   | 14/22 [00:00<00:00, 14.64it/s]\u001b[A\n",
      "Epoch 50/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 50/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.12it/s]\u001b[A\n",
      "Epoch 50/300:  91%|█████████ | 20/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 50/300: 100%|██████████| 22/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      " 17%|█▋        | 50/300 [01:31<07:32,  1.81s/it]             \u001b[A\n",
      "Epoch 51/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51/300:   9%|▉         | 2/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 51/300:  18%|█▊        | 4/22 [00:00<00:01, 14.84it/s]\u001b[A\n",
      "Epoch 51/300:  27%|██▋       | 6/22 [00:00<00:01, 14.49it/s]\u001b[A\n",
      "Epoch 51/300:  36%|███▋      | 8/22 [00:00<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 51/300:  45%|████▌     | 10/22 [00:00<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 51/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.42it/s]\u001b[A\n",
      "Epoch 51/300:  64%|██████▎   | 14/22 [00:00<00:00, 14.56it/s]\u001b[A\n",
      "Epoch 51/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.07it/s]\u001b[A\n",
      "Epoch 51/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 51/300:  91%|█████████ | 20/22 [00:01<00:00, 12.99it/s]\u001b[A\n",
      "Epoch 51/300: 100%|██████████| 22/22 [00:01<00:00, 13.14it/s]\u001b[A\n",
      " 17%|█▋        | 51/300 [01:33<07:30,  1.81s/it]             \u001b[A\n",
      "Epoch 52/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52/300:   9%|▉         | 2/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 52/300:  18%|█▊        | 4/22 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "Epoch 52/300:  27%|██▋       | 6/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 52/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 52/300:  45%|████▌     | 10/22 [00:00<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 52/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 52/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 52/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 52/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 52/300:  91%|█████████ | 20/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 52/300: 100%|██████████| 22/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      " 17%|█▋        | 52/300 [01:35<07:29,  1.81s/it]             \u001b[A\n",
      "Epoch 53/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53/300:   9%|▉         | 2/22 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "Epoch 53/300:  18%|█▊        | 4/22 [00:00<00:01, 14.49it/s]\u001b[A\n",
      "Epoch 53/300:  27%|██▋       | 6/22 [00:00<00:01, 14.31it/s]\u001b[A\n",
      "Epoch 53/300:  36%|███▋      | 8/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 53/300:  45%|████▌     | 10/22 [00:00<00:00, 13.10it/s]\u001b[A\n",
      "Epoch 53/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 53/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 53/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 53/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 53/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 53/300: 100%|██████████| 22/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      " 18%|█▊        | 53/300 [01:37<07:28,  1.81s/it]             \u001b[A\n",
      "Epoch 54/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54/300:   9%|▉         | 2/22 [00:00<00:01, 14.63it/s]\u001b[A\n",
      "Epoch 54/300:  18%|█▊        | 4/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 54/300:  27%|██▋       | 6/22 [00:00<00:01, 14.62it/s]\u001b[A\n",
      "Epoch 54/300:  36%|███▋      | 8/22 [00:00<00:00, 14.50it/s]\u001b[A\n",
      "Epoch 54/300:  45%|████▌     | 10/22 [00:00<00:00, 14.04it/s]\u001b[A\n",
      "Epoch 54/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 54/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 54/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.00it/s]\u001b[A\n",
      "Epoch 54/300:  82%|████████▏ | 18/22 [00:01<00:00, 12.94it/s]\u001b[A\n",
      "Epoch 54/300:  91%|█████████ | 20/22 [00:01<00:00, 13.17it/s]\u001b[A\n",
      "Epoch 54/300: 100%|██████████| 22/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      " 18%|█▊        | 54/300 [01:38<07:28,  1.82s/it]             \u001b[A\n",
      "Epoch 55/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55/300:   9%|▉         | 2/22 [00:00<00:01, 14.58it/s]\u001b[A\n",
      "Epoch 55/300:  18%|█▊        | 4/22 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "Epoch 55/300:  27%|██▋       | 6/22 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "Epoch 55/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 55/300:  45%|████▌     | 10/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 55/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 55/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 55/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 55/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 55/300:  91%|█████████ | 20/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 55/300: 100%|██████████| 22/22 [00:01<00:00, 14.15it/s]\u001b[A\n",
      " 18%|█▊        | 55/300 [01:40<07:25,  1.82s/it]             \u001b[A\n",
      "Epoch 56/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56/300:   9%|▉         | 2/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 56/300:  18%|█▊        | 4/22 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "Epoch 56/300:  27%|██▋       | 6/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 56/300:  36%|███▋      | 8/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 56/300:  45%|████▌     | 10/22 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 56/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 56/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 56/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 56/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.00it/s]\u001b[A\n",
      "Epoch 56/300:  91%|█████████ | 20/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 56/300: 100%|██████████| 22/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      " 19%|█▊        | 56/300 [01:42<07:23,  1.82s/it]             \u001b[A\n",
      "Epoch 57/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57/300:   9%|▉         | 2/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 57/300:  18%|█▊        | 4/22 [00:00<00:01, 13.96it/s]\u001b[A\n",
      "Epoch 57/300:  27%|██▋       | 6/22 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "Epoch 57/300:  36%|███▋      | 8/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 57/300:  45%|████▌     | 10/22 [00:00<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 57/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 57/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 57/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 57/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 57/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 57/300: 100%|██████████| 22/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      " 19%|█▉        | 57/300 [01:44<07:21,  1.82s/it]             \u001b[A\n",
      "Epoch 58/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58/300:   9%|▉         | 2/22 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "Epoch 58/300:  18%|█▊        | 4/22 [00:00<00:01, 14.41it/s]\u001b[A\n",
      "Epoch 58/300:  27%|██▋       | 6/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 58/300:  36%|███▋      | 8/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 58/300:  45%|████▌     | 10/22 [00:00<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 58/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 58/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 58/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 58/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 58/300:  91%|█████████ | 20/22 [00:01<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 58/300: 100%|██████████| 22/22 [00:01<00:00, 14.09it/s]\u001b[A\n",
      " 19%|█▉        | 58/300 [01:46<07:18,  1.81s/it]             \u001b[A\n",
      "Epoch 59/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59/300:   9%|▉         | 2/22 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "Epoch 59/300:  18%|█▊        | 4/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 59/300:  27%|██▋       | 6/22 [00:00<00:01, 13.56it/s]\u001b[A\n",
      "Epoch 59/300:  36%|███▋      | 8/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 59/300:  45%|████▌     | 10/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 59/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.24it/s]\u001b[A\n",
      "Epoch 59/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 59/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 59/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 59/300:  91%|█████████ | 20/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 59/300: 100%|██████████| 22/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      " 20%|█▉        | 59/300 [01:47<07:18,  1.82s/it]             \u001b[A\n",
      "Epoch 60/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60/300:   9%|▉         | 2/22 [00:00<00:01, 14.77it/s]\u001b[A\n",
      "Epoch 60/300:  18%|█▊        | 4/22 [00:00<00:01, 13.27it/s]\u001b[A\n",
      "Epoch 60/300:  27%|██▋       | 6/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 60/300:  36%|███▋      | 8/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 60/300:  45%|████▌     | 10/22 [00:00<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 60/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 60/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 60/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 60/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 60/300:  91%|█████████ | 20/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 60/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      " 20%|██        | 60/300 [01:49<07:16,  1.82s/it]             \u001b[A\n",
      "Epoch 61/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61/300:   9%|▉         | 2/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 61/300:  18%|█▊        | 4/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 61/300:  27%|██▋       | 6/22 [00:00<00:01, 13.27it/s]\u001b[A\n",
      "Epoch 61/300:  36%|███▋      | 8/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 61/300:  45%|████▌     | 10/22 [00:00<00:00, 13.26it/s]\u001b[A\n",
      "Epoch 61/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 61/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 61/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 61/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 61/300:  91%|█████████ | 20/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 61/300: 100%|██████████| 22/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      " 20%|██        | 61/300 [01:51<07:15,  1.82s/it]             \u001b[A\n",
      "Epoch 62/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62/300:   9%|▉         | 2/22 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "Epoch 62/300:  18%|█▊        | 4/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "Epoch 62/300:  27%|██▋       | 6/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 62/300:  36%|███▋      | 8/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 62/300:  45%|████▌     | 10/22 [00:00<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 62/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.30it/s]\u001b[A\n",
      "Epoch 62/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 62/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 62/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 62/300:  91%|█████████ | 20/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 62/300: 100%|██████████| 22/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      " 21%|██        | 62/300 [01:53<07:15,  1.83s/it]             \u001b[A\n",
      "Epoch 63/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63/300:   9%|▉         | 2/22 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "Epoch 63/300:  18%|█▊        | 4/22 [00:00<00:01, 14.10it/s]\u001b[A\n",
      "Epoch 63/300:  27%|██▋       | 6/22 [00:00<00:01, 13.59it/s]\u001b[A\n",
      "Epoch 63/300:  36%|███▋      | 8/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 63/300:  45%|████▌     | 10/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 63/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 63/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.00it/s]\u001b[A\n",
      "Epoch 63/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 63/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 63/300:  91%|█████████ | 20/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 63/300: 100%|██████████| 22/22 [00:01<00:00, 13.95it/s]\u001b[A\n",
      " 21%|██        | 63/300 [01:55<07:11,  1.82s/it]             \u001b[A\n",
      "Epoch 64/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64/300:   9%|▉         | 2/22 [00:00<00:01, 14.17it/s]\u001b[A\n",
      "Epoch 64/300:  18%|█▊        | 4/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 64/300:  27%|██▋       | 6/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 64/300:  36%|███▋      | 8/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 64/300:  45%|████▌     | 10/22 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 64/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 64/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 64/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 64/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 64/300:  91%|█████████ | 20/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 64/300: 100%|██████████| 22/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      " 21%|██▏       | 64/300 [01:57<07:10,  1.82s/it]             \u001b[A\n",
      "Epoch 65/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65/300:   9%|▉         | 2/22 [00:00<00:01, 14.62it/s]\u001b[A\n",
      "Epoch 65/300:  18%|█▊        | 4/22 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "Epoch 65/300:  27%|██▋       | 6/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 65/300:  36%|███▋      | 8/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 65/300:  45%|████▌     | 10/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 65/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 65/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 65/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 65/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 65/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 65/300: 100%|██████████| 22/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      " 22%|██▏       | 65/300 [01:58<07:07,  1.82s/it]             \u001b[A\n",
      "Epoch 66/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66/300:   9%|▉         | 2/22 [00:00<00:01, 13.13it/s]\u001b[A\n",
      "Epoch 66/300:  18%|█▊        | 4/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 66/300:  27%|██▋       | 6/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 66/300:  36%|███▋      | 8/22 [00:00<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 66/300:  45%|████▌     | 10/22 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 66/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 66/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 66/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 66/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 66/300:  91%|█████████ | 20/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 66/300: 100%|██████████| 22/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      " 22%|██▏       | 66/300 [02:00<07:06,  1.82s/it]             \u001b[A\n",
      "Epoch 67/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67/300:   9%|▉         | 2/22 [00:00<00:01, 14.52it/s]\u001b[A\n",
      "Epoch 67/300:  18%|█▊        | 4/22 [00:00<00:01, 13.30it/s]\u001b[A\n",
      "Epoch 67/300:  27%|██▋       | 6/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
      "Epoch 67/300:  36%|███▋      | 8/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 67/300:  45%|████▌     | 10/22 [00:00<00:00, 12.87it/s]\u001b[A\n",
      "Epoch 67/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.15it/s]\u001b[A\n",
      "Epoch 67/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 67/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 67/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 67/300:  91%|█████████ | 20/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 67/300: 100%|██████████| 22/22 [00:01<00:00, 14.30it/s]\u001b[A\n",
      " 22%|██▏       | 67/300 [02:02<07:03,  1.82s/it]             \u001b[A\n",
      "Epoch 68/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68/300:   9%|▉         | 2/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 68/300:  18%|█▊        | 4/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 68/300:  27%|██▋       | 6/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 68/300:  36%|███▋      | 8/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
      "Epoch 68/300:  45%|████▌     | 10/22 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 68/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 68/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 68/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 68/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 68/300:  91%|█████████ | 20/22 [00:01<00:00, 13.10it/s]\u001b[A\n",
      "Epoch 68/300: 100%|██████████| 22/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      " 23%|██▎       | 68/300 [02:04<07:04,  1.83s/it]             \u001b[A\n",
      "Epoch 69/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69/300:   9%|▉         | 2/22 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "Epoch 69/300:  18%|█▊        | 4/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 69/300:  27%|██▋       | 6/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 69/300:  36%|███▋      | 8/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 69/300:  45%|████▌     | 10/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 69/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 69/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 69/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 69/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 69/300:  91%|█████████ | 20/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 69/300: 100%|██████████| 22/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      " 23%|██▎       | 69/300 [02:06<07:02,  1.83s/it]             \u001b[A\n",
      "Epoch 70/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70/300:   9%|▉         | 2/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 70/300:  18%|█▊        | 4/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 70/300:  27%|██▋       | 6/22 [00:00<00:01, 13.50it/s]\u001b[A\n",
      "Epoch 70/300:  36%|███▋      | 8/22 [00:00<00:01, 13.25it/s]\u001b[A\n",
      "Epoch 70/300:  45%|████▌     | 10/22 [00:00<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 70/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 70/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 70/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 70/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 70/300:  91%|█████████ | 20/22 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 70/300: 100%|██████████| 22/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      " 23%|██▎       | 70/300 [02:08<07:00,  1.83s/it]             \u001b[A\n",
      "Epoch 71/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71/300:   9%|▉         | 2/22 [00:00<00:01, 13.09it/s]\u001b[A\n",
      "Epoch 71/300:  18%|█▊        | 4/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "Epoch 71/300:  27%|██▋       | 6/22 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "Epoch 71/300:  36%|███▋      | 8/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 71/300:  45%|████▌     | 10/22 [00:00<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 71/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 71/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 71/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 71/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 71/300:  91%|█████████ | 20/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 71/300: 100%|██████████| 22/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      " 24%|██▎       | 71/300 [02:09<06:59,  1.83s/it]             \u001b[A\n",
      "Epoch 72/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72/300:   9%|▉         | 2/22 [00:00<00:01, 13.26it/s]\u001b[A\n",
      "Epoch 72/300:  18%|█▊        | 4/22 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "Epoch 72/300:  27%|██▋       | 6/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 72/300:  36%|███▋      | 8/22 [00:00<00:00, 14.22it/s]\u001b[A\n",
      "Epoch 72/300:  45%|████▌     | 10/22 [00:00<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 72/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 72/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 72/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 72/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 72/300:  91%|█████████ | 20/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 72/300: 100%|██████████| 22/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      " 24%|██▍       | 72/300 [02:11<06:56,  1.83s/it]             \u001b[A\n",
      "Epoch 73/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73/300:   9%|▉         | 2/22 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "Epoch 73/300:  18%|█▊        | 4/22 [00:00<00:01, 13.31it/s]\u001b[A\n",
      "Epoch 73/300:  27%|██▋       | 6/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 73/300:  36%|███▋      | 8/22 [00:00<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 73/300:  45%|████▌     | 10/22 [00:00<00:00, 14.14it/s]\u001b[A\n",
      "Epoch 73/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 73/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 73/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 73/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 73/300:  91%|█████████ | 20/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 73/300: 100%|██████████| 22/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      " 24%|██▍       | 73/300 [02:13<06:53,  1.82s/it]             \u001b[A\n",
      "Epoch 74/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74/300:   9%|▉         | 2/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 74/300:  18%|█▊        | 4/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "Epoch 74/300:  27%|██▋       | 6/22 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "Epoch 74/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 74/300:  45%|████▌     | 10/22 [00:00<00:00, 14.14it/s]\u001b[A\n",
      "Epoch 74/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 74/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 74/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 74/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 74/300:  91%|█████████ | 20/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 74/300: 100%|██████████| 22/22 [00:01<00:00, 14.15it/s]\u001b[A\n",
      " 25%|██▍       | 74/300 [02:15<06:50,  1.81s/it]             \u001b[A\n",
      "Epoch 75/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75/300:   9%|▉         | 2/22 [00:00<00:01, 14.19it/s]\u001b[A\n",
      "Epoch 75/300:  18%|█▊        | 4/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 75/300:  27%|██▋       | 6/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 75/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 75/300:  45%|████▌     | 10/22 [00:00<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 75/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 75/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 75/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 75/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 75/300:  91%|█████████ | 20/22 [00:01<00:00, 13.07it/s]\u001b[A\n",
      "Epoch 75/300: 100%|██████████| 22/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      " 25%|██▌       | 75/300 [02:17<06:49,  1.82s/it]             \u001b[A\n",
      "Epoch 76/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76/300:   9%|▉         | 2/22 [00:00<00:01, 14.71it/s]\u001b[A\n",
      "Epoch 76/300:  18%|█▊        | 4/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 76/300:  27%|██▋       | 6/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 76/300:  36%|███▋      | 8/22 [00:00<00:00, 14.06it/s]\u001b[A\n",
      "Epoch 76/300:  45%|████▌     | 10/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 76/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 76/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 76/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 76/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 76/300:  91%|█████████ | 20/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 76/300: 100%|██████████| 22/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      " 25%|██▌       | 76/300 [02:18<06:47,  1.82s/it]             \u001b[A\n",
      "Epoch 77/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77/300:   9%|▉         | 2/22 [00:00<00:01, 14.58it/s]\u001b[A\n",
      "Epoch 77/300:  18%|█▊        | 4/22 [00:00<00:01, 13.49it/s]\u001b[A\n",
      "Epoch 77/300:  27%|██▋       | 6/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 77/300:  36%|███▋      | 8/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 77/300:  45%|████▌     | 10/22 [00:00<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 77/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 77/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.06it/s]\u001b[A\n",
      "Epoch 77/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 77/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 77/300:  91%|█████████ | 20/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 77/300: 100%|██████████| 22/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      " 26%|██▌       | 77/300 [02:20<06:46,  1.82s/it]             \u001b[A\n",
      "Epoch 78/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78/300:   9%|▉         | 2/22 [00:00<00:01, 12.90it/s]\u001b[A\n",
      "Epoch 78/300:  18%|█▊        | 4/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 78/300:  27%|██▋       | 6/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 78/300:  36%|███▋      | 8/22 [00:00<00:01, 13.25it/s]\u001b[A\n",
      "Epoch 78/300:  45%|████▌     | 10/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 78/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 78/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 78/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 78/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 78/300:  91%|█████████ | 20/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 78/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 26%|██▌       | 78/300 [02:22<06:44,  1.82s/it]             \u001b[A\n",
      "Epoch 79/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79/300:   9%|▉         | 2/22 [00:00<00:01, 13.27it/s]\u001b[A\n",
      "Epoch 79/300:  18%|█▊        | 4/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 79/300:  27%|██▋       | 6/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 79/300:  36%|███▋      | 8/22 [00:00<00:01, 13.60it/s]\u001b[A\n",
      "Epoch 79/300:  45%|████▌     | 10/22 [00:00<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 79/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.27it/s]\u001b[A\n",
      "Epoch 79/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 79/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 79/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 79/300:  91%|█████████ | 20/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 79/300: 100%|██████████| 22/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      " 26%|██▋       | 79/300 [02:24<06:41,  1.82s/it]             \u001b[A\n",
      "Epoch 80/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80/300:   9%|▉         | 2/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "Epoch 80/300:  18%|█▊        | 4/22 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "Epoch 80/300:  27%|██▋       | 6/22 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "Epoch 80/300:  36%|███▋      | 8/22 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "Epoch 80/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 80/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 80/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 80/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 80/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 80/300:  91%|█████████ | 20/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 80/300: 100%|██████████| 22/22 [00:01<00:00, 14.07it/s]\u001b[A\n",
      " 27%|██▋       | 80/300 [02:26<06:39,  1.82s/it]             \u001b[A\n",
      "Epoch 81/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81/300:   9%|▉         | 2/22 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Epoch 81/300:  18%|█▊        | 4/22 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "Epoch 81/300:  27%|██▋       | 6/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 81/300:  36%|███▋      | 8/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 81/300:  45%|████▌     | 10/22 [00:00<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 81/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 81/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 81/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 81/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 81/300:  91%|█████████ | 20/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 81/300: 100%|██████████| 22/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      " 27%|██▋       | 81/300 [02:28<06:38,  1.82s/it]             \u001b[A\n",
      "Epoch 82/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82/300:   9%|▉         | 2/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 82/300:  18%|█▊        | 4/22 [00:00<00:01, 13.55it/s]\u001b[A\n",
      "Epoch 82/300:  27%|██▋       | 6/22 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "Epoch 82/300:  36%|███▋      | 8/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 82/300:  45%|████▌     | 10/22 [00:00<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 82/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.11it/s]\u001b[A\n",
      "Epoch 82/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.23it/s]\u001b[A\n",
      "Epoch 82/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.16it/s]\u001b[A\n",
      "Epoch 82/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 82/300:  91%|█████████ | 20/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 82/300: 100%|██████████| 22/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      " 27%|██▋       | 82/300 [02:29<06:38,  1.83s/it]             \u001b[A\n",
      "Epoch 83/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83/300:   9%|▉         | 2/22 [00:00<00:01, 14.85it/s]\u001b[A\n",
      "Epoch 83/300:  18%|█▊        | 4/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 83/300:  27%|██▋       | 6/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 83/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 83/300:  45%|████▌     | 10/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 83/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 83/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 83/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 83/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 83/300:  91%|█████████ | 20/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 83/300: 100%|██████████| 22/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      " 28%|██▊       | 83/300 [02:31<06:36,  1.83s/it]             \u001b[A\n",
      "Epoch 84/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84/300:   9%|▉         | 2/22 [00:00<00:01, 15.15it/s]\u001b[A\n",
      "Epoch 84/300:  18%|█▊        | 4/22 [00:00<00:01, 15.01it/s]\u001b[A\n",
      "Epoch 84/300:  27%|██▋       | 6/22 [00:00<00:01, 14.19it/s]\u001b[A\n",
      "Epoch 84/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 84/300:  45%|████▌     | 10/22 [00:00<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 84/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 84/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 84/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 84/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 84/300:  91%|█████████ | 20/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 84/300: 100%|██████████| 22/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      " 28%|██▊       | 84/300 [02:33<06:33,  1.82s/it]             \u001b[A\n",
      "Epoch 85/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85/300:   9%|▉         | 2/22 [00:00<00:01, 13.43it/s]\u001b[A\n",
      "Epoch 85/300:  18%|█▊        | 4/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 85/300:  27%|██▋       | 6/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 85/300:  36%|███▋      | 8/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 85/300:  45%|████▌     | 10/22 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 85/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.10it/s]\u001b[A\n",
      "Epoch 85/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 85/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 85/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 85/300:  91%|█████████ | 20/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 85/300: 100%|██████████| 22/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      " 28%|██▊       | 85/300 [02:35<06:32,  1.82s/it]             \u001b[A\n",
      "Epoch 86/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 86/300:   9%|▉         | 2/22 [00:00<00:01, 14.33it/s]\u001b[A\n",
      "Epoch 86/300:  18%|█▊        | 4/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 86/300:  27%|██▋       | 6/22 [00:00<00:01, 13.90it/s]\u001b[A\n",
      "Epoch 86/300:  36%|███▋      | 8/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 86/300:  45%|████▌     | 10/22 [00:00<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 86/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 86/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 86/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 86/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 86/300:  91%|█████████ | 20/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 86/300: 100%|██████████| 22/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      " 29%|██▊       | 86/300 [02:37<06:30,  1.82s/it]             \u001b[A\n",
      "Epoch 87/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87/300:   9%|▉         | 2/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 87/300:  18%|█▊        | 4/22 [00:00<00:01, 12.86it/s]\u001b[A\n",
      "Epoch 87/300:  27%|██▋       | 6/22 [00:00<00:01, 13.14it/s]\u001b[A\n",
      "Epoch 87/300:  36%|███▋      | 8/22 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "Epoch 87/300:  45%|████▌     | 10/22 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 87/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.16it/s]\u001b[A\n",
      "Epoch 87/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.14it/s]\u001b[A\n",
      "Epoch 87/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 87/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 87/300:  91%|█████████ | 20/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 87/300: 100%|██████████| 22/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      " 29%|██▉       | 87/300 [02:39<06:30,  1.83s/it]             \u001b[A\n",
      "Epoch 88/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88/300:   9%|▉         | 2/22 [00:00<00:01, 12.46it/s]\u001b[A\n",
      "Epoch 88/300:  18%|█▊        | 4/22 [00:00<00:01, 12.63it/s]\u001b[A\n",
      "Epoch 88/300:  27%|██▋       | 6/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 88/300:  36%|███▋      | 8/22 [00:00<00:01, 13.60it/s]\u001b[A\n",
      "Epoch 88/300:  45%|████▌     | 10/22 [00:00<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 88/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 88/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.06it/s]\u001b[A\n",
      "Epoch 88/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 88/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 88/300:  91%|█████████ | 20/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 88/300: 100%|██████████| 22/22 [00:01<00:00, 13.88it/s]\u001b[A\n",
      " 29%|██▉       | 88/300 [02:40<06:29,  1.84s/it]             \u001b[A\n",
      "Epoch 89/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89/300:   9%|▉         | 2/22 [00:00<00:01, 13.02it/s]\u001b[A\n",
      "Epoch 89/300:  18%|█▊        | 4/22 [00:00<00:01, 13.17it/s]\u001b[A\n",
      "Epoch 89/300:  27%|██▋       | 6/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 89/300:  36%|███▋      | 8/22 [00:00<00:01, 13.71it/s]\u001b[A\n",
      "Epoch 89/300:  45%|████▌     | 10/22 [00:00<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 89/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 89/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 89/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 89/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 89/300:  91%|█████████ | 20/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 89/300: 100%|██████████| 22/22 [00:01<00:00, 14.15it/s]\u001b[A\n",
      " 30%|██▉       | 89/300 [02:42<06:26,  1.83s/it]             \u001b[A\n",
      "Epoch 90/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90/300:   9%|▉         | 2/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 90/300:  18%|█▊        | 4/22 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "Epoch 90/300:  27%|██▋       | 6/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 90/300:  36%|███▋      | 8/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 90/300:  45%|████▌     | 10/22 [00:00<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 90/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 90/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 90/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 90/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 90/300:  91%|█████████ | 20/22 [00:01<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 90/300: 100%|██████████| 22/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      " 30%|███       | 90/300 [02:44<06:24,  1.83s/it]             \u001b[A\n",
      "Epoch 91/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91/300:   9%|▉         | 2/22 [00:00<00:01, 12.61it/s]\u001b[A\n",
      "Epoch 91/300:  18%|█▊        | 4/22 [00:00<00:01, 12.92it/s]\u001b[A\n",
      "Epoch 91/300:  27%|██▋       | 6/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 91/300:  36%|███▋      | 8/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 91/300:  45%|████▌     | 10/22 [00:00<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 91/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 91/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 91/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "Epoch 91/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.12it/s]\u001b[A\n",
      "Epoch 91/300:  91%|█████████ | 20/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 91/300: 100%|██████████| 22/22 [00:01<00:00, 13.99it/s]\u001b[A\n",
      " 30%|███       | 91/300 [02:46<06:21,  1.83s/it]             \u001b[A\n",
      "Epoch 92/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92/300:   9%|▉         | 2/22 [00:00<00:01, 13.25it/s]\u001b[A\n",
      "Epoch 92/300:  18%|█▊        | 4/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 92/300:  27%|██▋       | 6/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 92/300:  36%|███▋      | 8/22 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "Epoch 92/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 92/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 92/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 92/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 92/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 92/300:  91%|█████████ | 20/22 [00:01<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 92/300: 100%|██████████| 22/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      " 31%|███       | 92/300 [02:48<06:19,  1.82s/it]             \u001b[A\n",
      "Epoch 93/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93/300:   9%|▉         | 2/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 93/300:  18%|█▊        | 4/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 93/300:  27%|██▋       | 6/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 93/300:  36%|███▋      | 8/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 93/300:  45%|████▌     | 10/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 93/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 93/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 93/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.15it/s]\u001b[A\n",
      "Epoch 93/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 93/300:  91%|█████████ | 20/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 93/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 31%|███       | 93/300 [02:49<06:17,  1.82s/it]             \u001b[A\n",
      "Epoch 94/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94/300:   9%|▉         | 2/22 [00:00<00:01, 14.17it/s]\u001b[A\n",
      "Epoch 94/300:  18%|█▊        | 4/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 94/300:  27%|██▋       | 6/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 94/300:  36%|███▋      | 8/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 94/300:  45%|████▌     | 10/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 94/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 94/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 94/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.88it/s]\u001b[A\n",
      "Epoch 94/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 94/300:  91%|█████████ | 20/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 94/300: 100%|██████████| 22/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      " 31%|███▏      | 94/300 [02:51<06:15,  1.82s/it]             \u001b[A\n",
      "Epoch 95/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95/300:   9%|▉         | 2/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 95/300:  18%|█▊        | 4/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 95/300:  27%|██▋       | 6/22 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "Epoch 95/300:  36%|███▋      | 8/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 95/300:  45%|████▌     | 10/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 95/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 95/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 95/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 95/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 95/300:  91%|█████████ | 20/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 95/300: 100%|██████████| 22/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      " 32%|███▏      | 95/300 [02:53<06:14,  1.83s/it]             \u001b[A\n",
      "Epoch 96/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96/300:   9%|▉         | 2/22 [00:00<00:01, 12.45it/s]\u001b[A\n",
      "Epoch 96/300:  18%|█▊        | 4/22 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Epoch 96/300:  27%|██▋       | 6/22 [00:00<00:01, 13.12it/s]\u001b[A\n",
      "Epoch 96/300:  36%|███▋      | 8/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 96/300:  45%|████▌     | 10/22 [00:00<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 96/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 96/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 96/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 96/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 96/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 96/300: 100%|██████████| 22/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      " 32%|███▏      | 96/300 [02:55<06:13,  1.83s/it]             \u001b[A\n",
      "Epoch 97/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97/300:   9%|▉         | 2/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 97/300:  18%|█▊        | 4/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 97/300:  27%|██▋       | 6/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 97/300:  36%|███▋      | 8/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 97/300:  45%|████▌     | 10/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 97/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 97/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 97/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 97/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 97/300:  91%|█████████ | 20/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 97/300: 100%|██████████| 22/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      " 32%|███▏      | 97/300 [02:57<06:11,  1.83s/it]             \u001b[A\n",
      "Epoch 98/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98/300:   9%|▉         | 2/22 [00:00<00:01, 14.07it/s]\u001b[A\n",
      "Epoch 98/300:  18%|█▊        | 4/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 98/300:  27%|██▋       | 6/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 98/300:  36%|███▋      | 8/22 [00:00<00:00, 14.15it/s]\u001b[A\n",
      "Epoch 98/300:  45%|████▌     | 10/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 98/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 98/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 98/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.24it/s]\u001b[A\n",
      "Epoch 98/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 98/300:  91%|█████████ | 20/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 98/300: 100%|██████████| 22/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      " 33%|███▎      | 98/300 [02:59<06:09,  1.83s/it]             \u001b[A\n",
      "Epoch 99/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99/300:   9%|▉         | 2/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 99/300:  18%|█▊        | 4/22 [00:00<00:01, 13.22it/s]\u001b[A\n",
      "Epoch 99/300:  27%|██▋       | 6/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 99/300:  36%|███▋      | 8/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 99/300:  45%|████▌     | 10/22 [00:00<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 99/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 99/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 99/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 99/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 99/300:  91%|█████████ | 20/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 99/300: 100%|██████████| 22/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      " 33%|███▎      | 99/300 [03:00<06:06,  1.82s/it]             \u001b[A\n",
      "Epoch 100/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100/300:   9%|▉         | 2/22 [00:00<00:01, 12.47it/s]\u001b[A\n",
      "Epoch 100/300:  18%|█▊        | 4/22 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "Epoch 100/300:  27%|██▋       | 6/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 100/300:  36%|███▋      | 8/22 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "Epoch 100/300:  45%|████▌     | 10/22 [00:00<00:00, 12.94it/s]\u001b[A\n",
      "Epoch 100/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 100/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 100/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 100/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      "Epoch 100/300:  91%|█████████ | 20/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 100/300: 100%|██████████| 22/22 [00:01<00:00, 14.04it/s]\u001b[A\n",
      " 33%|███▎      | 100/300 [03:02<06:04,  1.82s/it]             \u001b[A\n",
      "Epoch 101/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101/300:   9%|▉         | 2/22 [00:00<00:01, 14.90it/s]\u001b[A\n",
      "Epoch 101/300:  18%|█▊        | 4/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 101/300:  27%|██▋       | 6/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 101/300:  36%|███▋      | 8/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 101/300:  45%|████▌     | 10/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 101/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 101/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 101/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 101/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.30it/s]\u001b[A\n",
      "Epoch 101/300:  91%|█████████ | 20/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 101/300: 100%|██████████| 22/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      " 34%|███▎      | 101/300 [03:04<06:02,  1.82s/it]             \u001b[A\n",
      "Epoch 102/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102/300:   9%|▉         | 2/22 [00:00<00:01, 14.40it/s]\u001b[A\n",
      "Epoch 102/300:  18%|█▊        | 4/22 [00:00<00:01, 14.15it/s]\u001b[A\n",
      "Epoch 102/300:  27%|██▋       | 6/22 [00:00<00:01, 14.44it/s]\u001b[A\n",
      "Epoch 102/300:  36%|███▋      | 8/22 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "Epoch 102/300:  45%|████▌     | 10/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 102/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 102/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 102/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 102/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 102/300:  91%|█████████ | 20/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 102/300: 100%|██████████| 22/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      " 34%|███▍      | 102/300 [03:06<06:01,  1.82s/it]             \u001b[A\n",
      "Epoch 103/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103/300:   9%|▉         | 2/22 [00:00<00:01, 13.12it/s]\u001b[A\n",
      "Epoch 103/300:  18%|█▊        | 4/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 103/300:  27%|██▋       | 6/22 [00:00<00:01, 13.51it/s]\u001b[A\n",
      "Epoch 139/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.14it/s]\u001b[A\n",
      "Epoch 139/300:  91%|█████████ | 20/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 139/300: 100%|██████████| 22/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      " 46%|████▋     | 139/300 [04:14<04:54,  1.83s/it]             \u001b[A\n",
      "Epoch 140/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 140/300:   9%|▉         | 2/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 140/300:  18%|█▊        | 4/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 140/300:  27%|██▋       | 6/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 140/300:  36%|███▋      | 8/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 140/300:  45%|████▌     | 10/22 [00:00<00:00, 13.02it/s]\u001b[A\n",
      "Epoch 140/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 140/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 140/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 140/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 140/300:  91%|█████████ | 20/22 [00:01<00:00, 14.11it/s]\u001b[A\n",
      "Epoch 140/300: 100%|██████████| 22/22 [00:01<00:00, 14.43it/s]\u001b[A\n",
      " 47%|████▋     | 140/300 [04:15<04:51,  1.82s/it]             \u001b[A\n",
      "Epoch 141/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 141/300:   9%|▉         | 2/22 [00:00<00:01, 14.37it/s]\u001b[A\n",
      "Epoch 141/300:  18%|█▊        | 4/22 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "Epoch 141/300:  27%|██▋       | 6/22 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "Epoch 141/300:  36%|███▋      | 8/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 141/300:  45%|████▌     | 10/22 [00:00<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 141/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 141/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 141/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 141/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 141/300:  91%|█████████ | 20/22 [00:01<00:00, 13.11it/s]\u001b[A\n",
      "Epoch 141/300: 100%|██████████| 22/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      " 47%|████▋     | 141/300 [04:17<04:50,  1.82s/it]             \u001b[A\n",
      "Epoch 142/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 142/300:   9%|▉         | 2/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 142/300:  18%|█▊        | 4/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 142/300:  27%|██▋       | 6/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 142/300:  36%|███▋      | 8/22 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "Epoch 142/300:  45%|████▌     | 10/22 [00:00<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 142/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 142/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 142/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 142/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 142/300:  91%|█████████ | 20/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 142/300: 100%|██████████| 22/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      " 47%|████▋     | 142/300 [04:19<04:48,  1.82s/it]             \u001b[A\n",
      "Epoch 143/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 143/300:   9%|▉         | 2/22 [00:00<00:01, 14.71it/s]\u001b[A\n",
      "Epoch 143/300:  18%|█▊        | 4/22 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "Epoch 143/300:  27%|██▋       | 6/22 [00:00<00:01, 14.11it/s]\u001b[A\n",
      "Epoch 143/300:  36%|███▋      | 8/22 [00:00<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 143/300:  45%|████▌     | 10/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 143/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 143/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 143/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 143/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 143/300:  91%|█████████ | 20/22 [00:01<00:00, 13.13it/s]\u001b[A\n",
      "Epoch 143/300: 100%|██████████| 22/22 [00:01<00:00, 13.07it/s]\u001b[A\n",
      " 48%|████▊     | 143/300 [04:21<04:47,  1.83s/it]             \u001b[A\n",
      "Epoch 144/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 144/300:   9%|▉         | 2/22 [00:00<00:01, 14.69it/s]\u001b[A\n",
      "Epoch 144/300:  18%|█▊        | 4/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 144/300:  27%|██▋       | 6/22 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "Epoch 144/300:  36%|███▋      | 8/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 144/300:  45%|████▌     | 10/22 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 144/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 144/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 144/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 144/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 144/300:  91%|█████████ | 20/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 144/300: 100%|██████████| 22/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      " 48%|████▊     | 144/300 [04:23<04:43,  1.82s/it]             \u001b[A\n",
      "Epoch 145/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 145/300:   9%|▉         | 2/22 [00:00<00:01, 14.56it/s]\u001b[A\n",
      "Epoch 145/300:  18%|█▊        | 4/22 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "Epoch 145/300:  27%|██▋       | 6/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 145/300:  36%|███▋      | 8/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 145/300:  45%|████▌     | 10/22 [00:00<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 145/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 145/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 145/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 145/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 145/300:  91%|█████████ | 20/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 145/300: 100%|██████████| 22/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      " 48%|████▊     | 145/300 [04:24<04:42,  1.82s/it]             \u001b[A\n",
      "Epoch 146/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 146/300:   9%|▉         | 2/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 146/300:  18%|█▊        | 4/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 146/300:  27%|██▋       | 6/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 146/300:  36%|███▋      | 8/22 [00:00<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 146/300:  45%|████▌     | 10/22 [00:00<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 146/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 146/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 146/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 146/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 146/300:  91%|█████████ | 20/22 [00:01<00:00, 13.13it/s]\u001b[A\n",
      "Epoch 146/300: 100%|██████████| 22/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      " 49%|████▊     | 146/300 [04:26<04:41,  1.83s/it]             \u001b[A\n",
      "Epoch 147/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 147/300:   9%|▉         | 2/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 147/300:  18%|█▊        | 4/22 [00:00<00:01, 13.52it/s]\u001b[A\n",
      "Epoch 147/300:  27%|██▋       | 6/22 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "Epoch 147/300:  36%|███▋      | 8/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 147/300:  45%|████▌     | 10/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 147/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 147/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 147/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 147/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 147/300:  91%|█████████ | 20/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 147/300: 100%|██████████| 22/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      " 49%|████▉     | 147/300 [04:28<04:38,  1.82s/it]             \u001b[A\n",
      "Epoch 148/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 148/300:   9%|▉         | 2/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 148/300:  18%|█▊        | 4/22 [00:00<00:01, 13.49it/s]\u001b[A\n",
      "Epoch 148/300:  27%|██▋       | 6/22 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "Epoch 148/300:  36%|███▋      | 8/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 148/300:  45%|████▌     | 10/22 [00:00<00:00, 13.91it/s]\u001b[A\n",
      "Epoch 148/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 148/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.22it/s]\u001b[A\n",
      "Epoch 148/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 148/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 148/300:  91%|█████████ | 20/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 148/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      " 49%|████▉     | 148/300 [04:30<04:36,  1.82s/it]             \u001b[A\n",
      "Epoch 149/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149/300:   9%|▉         | 2/22 [00:00<00:01, 14.48it/s]\u001b[A\n",
      "Epoch 149/300:  18%|█▊        | 4/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 149/300:  27%|██▋       | 6/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 149/300:  36%|███▋      | 8/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 149/300:  45%|████▌     | 10/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 149/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 149/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 149/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 149/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 149/300:  91%|█████████ | 20/22 [00:01<00:00, 13.07it/s]\u001b[A\n",
      "Epoch 149/300: 100%|██████████| 22/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      " 50%|████▉     | 149/300 [04:32<04:35,  1.83s/it]             \u001b[A\n",
      "Epoch 150/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 150/300:   9%|▉         | 2/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "Epoch 150/300:  18%|█▊        | 4/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 150/300:  27%|██▋       | 6/22 [00:00<00:01, 13.34it/s]\u001b[A\n",
      "Epoch 150/300:  36%|███▋      | 8/22 [00:00<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 150/300:  45%|████▌     | 10/22 [00:00<00:00, 14.22it/s]\u001b[A\n",
      "Epoch 150/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 150/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.12it/s]\u001b[A\n",
      "Epoch 150/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 150/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 150/300:  91%|█████████ | 20/22 [00:01<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 150/300: 100%|██████████| 22/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      " 50%|█████     | 150/300 [04:34<04:33,  1.82s/it]             \u001b[A\n",
      "Epoch 151/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 151/300:   9%|▉         | 2/22 [00:00<00:01, 13.50it/s]\u001b[A\n",
      "Epoch 151/300:  18%|█▊        | 4/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 151/300:  27%|██▋       | 6/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 151/300:  36%|███▋      | 8/22 [00:00<00:00, 14.04it/s]\u001b[A\n",
      "Epoch 151/300:  45%|████▌     | 10/22 [00:00<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 151/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 151/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 151/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 151/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 151/300:  91%|█████████ | 20/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 151/300: 100%|██████████| 22/22 [00:01<00:00, 13.95it/s]\u001b[A\n",
      " 50%|█████     | 151/300 [04:35<04:31,  1.82s/it]             \u001b[A\n",
      "Epoch 152/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 152/300:   9%|▉         | 2/22 [00:00<00:01, 13.56it/s]\u001b[A\n",
      "Epoch 152/300:  18%|█▊        | 4/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 152/300:  27%|██▋       | 6/22 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "Epoch 152/300:  36%|███▋      | 8/22 [00:00<00:00, 14.17it/s]\u001b[A\n",
      "Epoch 152/300:  45%|████▌     | 10/22 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 152/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 152/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 152/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 152/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 152/300:  91%|█████████ | 20/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 152/300: 100%|██████████| 22/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      " 51%|█████     | 152/300 [04:37<04:29,  1.82s/it]             \u001b[A\n",
      "Epoch 153/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 153/300:   9%|▉         | 2/22 [00:00<00:01, 14.41it/s]\u001b[A\n",
      "Epoch 153/300:  18%|█▊        | 4/22 [00:00<00:01, 14.16it/s]\u001b[A\n",
      "Epoch 153/300:  27%|██▋       | 6/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 153/300:  36%|███▋      | 8/22 [00:00<00:01, 13.97it/s]\u001b[A\n",
      "Epoch 153/300:  45%|████▌     | 10/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 153/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 153/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 153/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 153/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 153/300:  91%|█████████ | 20/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 153/300: 100%|██████████| 22/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      " 51%|█████     | 153/300 [04:39<04:27,  1.82s/it]             \u001b[A\n",
      "Epoch 154/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 154/300:   9%|▉         | 2/22 [00:00<00:01, 14.57it/s]\u001b[A\n",
      "Epoch 154/300:  18%|█▊        | 4/22 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "Epoch 154/300:  27%|██▋       | 6/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "Epoch 154/300:  36%|███▋      | 8/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 154/300:  45%|████▌     | 10/22 [00:00<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 154/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 154/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 154/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.23it/s]\u001b[A\n",
      "Epoch 154/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 154/300:  91%|█████████ | 20/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 154/300: 100%|██████████| 22/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      " 51%|█████▏    | 154/300 [04:41<04:26,  1.82s/it]             \u001b[A\n",
      "Epoch 155/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 155/300:   9%|▉         | 2/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 155/300:  18%|█▊        | 4/22 [00:00<00:01, 14.01it/s]\u001b[A\n",
      "Epoch 155/300:  27%|██▋       | 6/22 [00:00<00:01, 14.06it/s]\u001b[A\n",
      "Epoch 155/300:  36%|███▋      | 8/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 155/300:  45%|████▌     | 10/22 [00:00<00:00, 13.28it/s]\u001b[A\n",
      "Epoch 155/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.19it/s]\u001b[A\n",
      "Epoch 155/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 155/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 155/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 155/300:  91%|█████████ | 20/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 155/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 52%|█████▏    | 155/300 [04:43<04:25,  1.83s/it]             \u001b[A\n",
      "Epoch 156/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 156/300:   9%|▉         | 2/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 156/300:  18%|█▊        | 4/22 [00:00<00:01, 14.16it/s]\u001b[A\n",
      "Epoch 156/300:  27%|██▋       | 6/22 [00:00<00:01, 14.37it/s]\u001b[A\n",
      "Epoch 156/300:  36%|███▋      | 8/22 [00:00<00:01, 13.91it/s]\u001b[A\n",
      "Epoch 156/300:  45%|████▌     | 10/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 156/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 156/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 156/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 156/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 156/300:  91%|█████████ | 20/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 156/300: 100%|██████████| 22/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      " 52%|█████▏    | 156/300 [04:45<04:23,  1.83s/it]             \u001b[A\n",
      "Epoch 157/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 157/300:   9%|▉         | 2/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 157/300:  18%|█▊        | 4/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 157/300:  27%|██▋       | 6/22 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "Epoch 157/300:  36%|███▋      | 8/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 157/300:  45%|████▌     | 10/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 157/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 157/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 157/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 157/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 157/300:  91%|█████████ | 20/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 157/300: 100%|██████████| 22/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      " 52%|█████▏    | 157/300 [04:46<04:21,  1.83s/it]             \u001b[A\n",
      "Epoch 158/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 158/300:   9%|▉         | 2/22 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "Epoch 158/300:  18%|█▊        | 4/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 158/300:  27%|██▋       | 6/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 158/300:  36%|███▋      | 8/22 [00:00<00:01, 13.66it/s]\u001b[A\n",
      "Epoch 158/300:  45%|████▌     | 10/22 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 158/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 158/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 158/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 158/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 158/300:  91%|█████████ | 20/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 158/300: 100%|██████████| 22/22 [00:01<00:00, 13.88it/s]\u001b[A\n",
      " 53%|█████▎    | 158/300 [04:48<04:19,  1.83s/it]             \u001b[A\n",
      "Epoch 159/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 159/300:   9%|▉         | 2/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "Epoch 159/300:  18%|█▊        | 4/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 159/300:  27%|██▋       | 6/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 159/300:  36%|███▋      | 8/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 159/300:  45%|████▌     | 10/22 [00:00<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 159/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 159/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 159/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 159/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 159/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 159/300: 100%|██████████| 22/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      " 53%|█████▎    | 159/300 [04:50<04:17,  1.83s/it]             \u001b[A\n",
      "Epoch 160/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 160/300:   9%|▉         | 2/22 [00:00<00:01, 14.09it/s]\u001b[A\n",
      "Epoch 160/300:  18%|█▊        | 4/22 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "Epoch 160/300:  27%|██▋       | 6/22 [00:00<00:01, 14.26it/s]\u001b[A\n",
      "Epoch 160/300:  36%|███▋      | 8/22 [00:00<00:00, 14.42it/s]\u001b[A\n",
      "Epoch 160/300:  45%|████▌     | 10/22 [00:00<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 160/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.30it/s]\u001b[A\n",
      "Epoch 160/300:  64%|██████▎   | 14/22 [00:00<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 160/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 160/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 160/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 160/300: 100%|██████████| 22/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      " 53%|█████▎    | 160/300 [04:52<04:14,  1.82s/it]             \u001b[A\n",
      "Epoch 161/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 161/300:   9%|▉         | 2/22 [00:00<00:01, 15.08it/s]\u001b[A\n",
      "Epoch 161/300:  18%|█▊        | 4/22 [00:00<00:01, 14.26it/s]\u001b[A\n",
      "Epoch 161/300:  27%|██▋       | 6/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 161/300:  36%|███▋      | 8/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 161/300:  45%|████▌     | 10/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 161/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 161/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 161/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 161/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 161/300:  91%|█████████ | 20/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 161/300: 100%|██████████| 22/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      " 54%|█████▎    | 161/300 [04:54<04:12,  1.82s/it]             \u001b[A\n",
      "Epoch 162/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 162/300:   9%|▉         | 2/22 [00:00<00:01, 13.79it/s]\u001b[A\n",
      "Epoch 162/300:  18%|█▊        | 4/22 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "Epoch 162/300:  27%|██▋       | 6/22 [00:00<00:01, 13.42it/s]\u001b[A\n",
      "Epoch 162/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 162/300:  45%|████▌     | 10/22 [00:00<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 162/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 162/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 162/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 162/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 162/300:  91%|█████████ | 20/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 162/300: 100%|██████████| 22/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      " 54%|█████▍    | 162/300 [04:55<04:11,  1.82s/it]             \u001b[A\n",
      "Epoch 163/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 163/300:   9%|▉         | 2/22 [00:00<00:01, 13.01it/s]\u001b[A\n",
      "Epoch 163/300:  18%|█▊        | 4/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 163/300:  27%|██▋       | 6/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 163/300:  36%|███▋      | 8/22 [00:00<00:00, 14.17it/s]\u001b[A\n",
      "Epoch 163/300:  45%|████▌     | 10/22 [00:00<00:00, 14.27it/s]\u001b[A\n",
      "Epoch 163/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.06it/s]\u001b[A\n",
      "Epoch 163/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 163/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 163/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 163/300:  91%|█████████ | 20/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 163/300: 100%|██████████| 22/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      " 54%|█████▍    | 163/300 [04:57<04:09,  1.82s/it]             \u001b[A\n",
      "Epoch 164/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 164/300:   9%|▉         | 2/22 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "Epoch 164/300:  18%|█▊        | 4/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 164/300:  27%|██▋       | 6/22 [00:00<00:01, 14.17it/s]\u001b[A\n",
      "Epoch 164/300:  36%|███▋      | 8/22 [00:00<00:00, 14.07it/s]\u001b[A\n",
      "Epoch 164/300:  45%|████▌     | 10/22 [00:00<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 164/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 164/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 164/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 164/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 164/300:  91%|█████████ | 20/22 [00:01<00:00, 13.30it/s]\u001b[A\n",
      "Epoch 164/300: 100%|██████████| 22/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      " 55%|█████▍    | 164/300 [04:59<04:07,  1.82s/it]             \u001b[A\n",
      "Epoch 165/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 165/300:   9%|▉         | 2/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 165/300:  18%|█▊        | 4/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 165/300:  27%|██▋       | 6/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 165/300:  36%|███▋      | 8/22 [00:00<00:01, 13.59it/s]\u001b[A\n",
      "Epoch 165/300:  45%|████▌     | 10/22 [00:00<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 165/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 165/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.15it/s]\u001b[A\n",
      "Epoch 165/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 165/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 165/300:  91%|█████████ | 20/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 165/300: 100%|██████████| 22/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      " 55%|█████▌    | 165/300 [05:01<04:06,  1.82s/it]             \u001b[A\n",
      "Epoch 166/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 166/300:   9%|▉         | 2/22 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "Epoch 166/300:  18%|█▊        | 4/22 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "Epoch 166/300:  27%|██▋       | 6/22 [00:00<00:01, 13.49it/s]\u001b[A\n",
      "Epoch 166/300:  36%|███▋      | 8/22 [00:00<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 166/300:  45%|████▌     | 10/22 [00:00<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 166/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 166/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 166/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 166/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 166/300:  91%|█████████ | 20/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 166/300: 100%|██████████| 22/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      " 55%|█████▌    | 166/300 [05:03<04:04,  1.82s/it]             \u001b[A\n",
      "Epoch 167/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 167/300:   9%|▉         | 2/22 [00:00<00:01, 14.28it/s]\u001b[A\n",
      "Epoch 167/300:  18%|█▊        | 4/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 167/300:  27%|██▋       | 6/22 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "Epoch 167/300:  36%|███▋      | 8/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 167/300:  45%|████▌     | 10/22 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 167/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 167/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 167/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 167/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.19it/s]\u001b[A\n",
      "Epoch 167/300:  91%|█████████ | 20/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 167/300: 100%|██████████| 22/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      " 56%|█████▌    | 167/300 [05:05<04:03,  1.83s/it]             \u001b[A\n",
      "Epoch 168/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 168/300:   9%|▉         | 2/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 168/300:  18%|█▊        | 4/22 [00:00<00:01, 13.57it/s]\u001b[A\n",
      "Epoch 168/300:  27%|██▋       | 6/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 168/300:  36%|███▋      | 8/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 168/300:  45%|████▌     | 10/22 [00:00<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 168/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 168/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 168/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 168/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 168/300:  91%|█████████ | 20/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 168/300: 100%|██████████| 22/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      " 56%|█████▌    | 168/300 [05:06<04:01,  1.83s/it]             \u001b[A\n",
      "Epoch 169/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 169/300:   9%|▉         | 2/22 [00:00<00:01, 13.56it/s]\u001b[A\n",
      "Epoch 169/300:  18%|█▊        | 4/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 169/300:  27%|██▋       | 6/22 [00:00<00:01, 14.19it/s]\u001b[A\n",
      "Epoch 169/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 169/300:  45%|████▌     | 10/22 [00:00<00:00, 13.88it/s]\u001b[A\n",
      "Epoch 169/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 169/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 169/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 169/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 169/300:  91%|█████████ | 20/22 [00:01<00:00, 13.08it/s]\u001b[A\n",
      "Epoch 169/300: 100%|██████████| 22/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      " 56%|█████▋    | 169/300 [05:08<04:00,  1.83s/it]             \u001b[A\n",
      "Epoch 170/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 170/300:   9%|▉         | 2/22 [00:00<00:01, 14.08it/s]\u001b[A\n",
      "Epoch 170/300:  18%|█▊        | 4/22 [00:00<00:01, 13.51it/s]\u001b[A\n",
      "Epoch 170/300:  27%|██▋       | 6/22 [00:00<00:01, 13.00it/s]\u001b[A\n",
      "Epoch 170/300:  36%|███▋      | 8/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 170/300:  45%|████▌     | 10/22 [00:00<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 170/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 170/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 170/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 170/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 170/300:  91%|█████████ | 20/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 170/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 57%|█████▋    | 170/300 [05:10<03:58,  1.83s/it]             \u001b[A\n",
      "Epoch 171/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 171/300:   9%|▉         | 2/22 [00:00<00:01, 12.42it/s]\u001b[A\n",
      "Epoch 171/300:  18%|█▊        | 4/22 [00:00<00:01, 13.26it/s]\u001b[A\n",
      "Epoch 171/300:  27%|██▋       | 6/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 171/300:  36%|███▋      | 8/22 [00:00<00:00, 14.11it/s]\u001b[A\n",
      "Epoch 171/300:  45%|████▌     | 10/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 171/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.33it/s]\u001b[A\n",
      "Epoch 171/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 171/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 171/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 171/300:  91%|█████████ | 20/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 171/300: 100%|██████████| 22/22 [00:01<00:00, 13.90it/s]\u001b[A\n",
      " 57%|█████▋    | 171/300 [05:12<03:56,  1.83s/it]             \u001b[A\n",
      "Epoch 172/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 172/300:   9%|▉         | 2/22 [00:00<00:01, 12.67it/s]\u001b[A\n",
      "Epoch 172/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 172/300:  27%|██▋       | 6/22 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "Epoch 172/300:  36%|███▋      | 8/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 172/300:  45%|████▌     | 10/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 172/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 172/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 172/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 172/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 172/300:  91%|█████████ | 20/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 172/300: 100%|██████████| 22/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      " 57%|█████▋    | 172/300 [05:14<03:54,  1.83s/it]             \u001b[A\n",
      "Epoch 173/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 173/300:   9%|▉         | 2/22 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "Epoch 173/300:  18%|█▊        | 4/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 173/300:  27%|██▋       | 6/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 173/300:  36%|███▋      | 8/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 173/300:  45%|████▌     | 10/22 [00:00<00:00, 13.77it/s]\u001b[A\n",
      "Epoch 173/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.90it/s]\u001b[A\n",
      "Epoch 173/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 173/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 173/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 173/300:  91%|█████████ | 20/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 173/300: 100%|██████████| 22/22 [00:01<00:00, 13.30it/s]\u001b[A\n",
      " 58%|█████▊    | 173/300 [05:16<03:53,  1.84s/it]             \u001b[A\n",
      "Epoch 174/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 174/300:   9%|▉         | 2/22 [00:00<00:01, 13.54it/s]\u001b[A\n",
      "Epoch 174/300:  18%|█▊        | 4/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 174/300:  27%|██▋       | 6/22 [00:00<00:01, 13.88it/s]\u001b[A\n",
      "Epoch 174/300:  36%|███▋      | 8/22 [00:00<00:00, 14.13it/s]\u001b[A\n",
      "Epoch 174/300:  45%|████▌     | 10/22 [00:00<00:00, 13.88it/s]\u001b[A\n",
      "Epoch 174/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 174/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 174/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 174/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 174/300:  91%|█████████ | 20/22 [00:01<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 174/300: 100%|██████████| 22/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      " 58%|█████▊    | 174/300 [05:17<03:50,  1.83s/it]             \u001b[A\n",
      "Epoch 175/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 175/300:   9%|▉         | 2/22 [00:00<00:01, 12.91it/s]\u001b[A\n",
      "Epoch 175/300:  18%|█▊        | 4/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 175/300:  27%|██▋       | 6/22 [00:00<00:01, 13.25it/s]\u001b[A\n",
      "Epoch 175/300:  36%|███▋      | 8/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 175/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 175/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.16it/s]\u001b[A\n",
      "Epoch 175/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 175/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 175/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 175/300:  91%|█████████ | 20/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 175/300: 100%|██████████| 22/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      " 58%|█████▊    | 175/300 [05:19<03:49,  1.83s/it]             \u001b[A\n",
      "Epoch 176/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 176/300:   9%|▉         | 2/22 [00:00<00:01, 13.01it/s]\u001b[A\n",
      "Epoch 176/300:  18%|█▊        | 4/22 [00:00<00:01, 13.39it/s]\u001b[A\n",
      "Epoch 176/300:  27%|██▋       | 6/22 [00:00<00:01, 13.45it/s]\u001b[A\n",
      "Epoch 176/300:  36%|███▋      | 8/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 176/300:  45%|████▌     | 10/22 [00:00<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 176/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 176/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.17it/s]\u001b[A\n",
      "Epoch 176/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 176/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 176/300:  91%|█████████ | 20/22 [00:01<00:00, 13.39it/s]\u001b[A\n",
      "Epoch 176/300: 100%|██████████| 22/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      " 59%|█████▊    | 176/300 [05:21<03:48,  1.84s/it]             \u001b[A\n",
      "Epoch 177/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 177/300:   9%|▉         | 2/22 [00:00<00:01, 12.14it/s]\u001b[A\n",
      "Epoch 177/300:  18%|█▊        | 4/22 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "Epoch 177/300:  27%|██▋       | 6/22 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "Epoch 177/300:  36%|███▋      | 8/22 [00:00<00:01, 12.80it/s]\u001b[A\n",
      "Epoch 177/300:  45%|████▌     | 10/22 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 177/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 177/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 177/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 177/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 177/300:  91%|█████████ | 20/22 [00:01<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 177/300: 100%|██████████| 22/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      " 59%|█████▉    | 177/300 [05:23<03:46,  1.84s/it]             \u001b[A\n",
      "Epoch 178/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 178/300:   9%|▉         | 2/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 178/300:  18%|█▊        | 4/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 178/300:  27%|██▋       | 6/22 [00:00<00:01, 13.59it/s]\u001b[A\n",
      "Epoch 178/300:  36%|███▋      | 8/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 178/300:  45%|████▌     | 10/22 [00:00<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 178/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 178/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 178/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 178/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 178/300:  91%|█████████ | 20/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 178/300: 100%|██████████| 22/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      " 59%|█████▉    | 178/300 [05:25<03:44,  1.84s/it]             \u001b[A\n",
      "Epoch 179/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 179/300:   9%|▉         | 2/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 179/300:  18%|█▊        | 4/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 179/300:  27%|██▋       | 6/22 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "Epoch 179/300:  36%|███▋      | 8/22 [00:00<00:01, 13.62it/s]\u001b[A\n",
      "Epoch 179/300:  45%|████▌     | 10/22 [00:00<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 179/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 179/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.03it/s]\u001b[A\n",
      "Epoch 179/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 179/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 179/300:  91%|█████████ | 20/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 179/300: 100%|██████████| 22/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      " 60%|█████▉    | 179/300 [05:27<03:41,  1.83s/it]             \u001b[A\n",
      "Epoch 180/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 180/300:   9%|▉         | 2/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 180/300:  18%|█▊        | 4/22 [00:00<00:01, 13.16it/s]\u001b[A\n",
      "Epoch 180/300:  27%|██▋       | 6/22 [00:00<00:01, 13.01it/s]\u001b[A\n",
      "Epoch 180/300:  36%|███▋      | 8/22 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "Epoch 180/300:  45%|████▌     | 10/22 [00:00<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 180/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 180/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 180/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 180/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 180/300:  91%|█████████ | 20/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 180/300: 100%|██████████| 22/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      " 60%|██████    | 180/300 [05:28<03:39,  1.83s/it]             \u001b[A\n",
      "Epoch 181/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 181/300:   9%|▉         | 2/22 [00:00<00:01, 12.74it/s]\u001b[A\n",
      "Epoch 181/300:  18%|█▊        | 4/22 [00:00<00:01, 13.05it/s]\u001b[A\n",
      "Epoch 181/300:  27%|██▋       | 6/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 181/300:  36%|███▋      | 8/22 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "Epoch 181/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 181/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 181/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 181/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 181/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 181/300:  91%|█████████ | 20/22 [00:01<00:00, 13.20it/s]\u001b[A\n",
      "Epoch 181/300: 100%|██████████| 22/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      " 60%|██████    | 181/300 [05:30<03:38,  1.84s/it]             \u001b[A\n",
      "Epoch 182/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 182/300:   9%|▉         | 2/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 182/300:  18%|█▊        | 4/22 [00:00<00:01, 12.94it/s]\u001b[A\n",
      "Epoch 182/300:  27%|██▋       | 6/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 182/300:  36%|███▋      | 8/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 182/300:  45%|████▌     | 10/22 [00:00<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 182/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 182/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 182/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 182/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 182/300:  91%|█████████ | 20/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 182/300: 100%|██████████| 22/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      " 61%|██████    | 182/300 [05:32<03:36,  1.83s/it]             \u001b[A\n",
      "Epoch 183/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 183/300:   9%|▉         | 2/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 183/300:  18%|█▊        | 4/22 [00:00<00:01, 14.00it/s]\u001b[A\n",
      "Epoch 183/300:  27%|██▋       | 6/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 183/300:  36%|███▋      | 8/22 [00:00<00:01, 13.60it/s]\u001b[A\n",
      "Epoch 183/300:  45%|████▌     | 10/22 [00:00<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 183/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 183/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 183/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 183/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 183/300:  91%|█████████ | 20/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 183/300: 100%|██████████| 22/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      " 61%|██████    | 183/300 [05:34<03:34,  1.84s/it]             \u001b[A\n",
      "Epoch 184/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 184/300:   9%|▉         | 2/22 [00:00<00:01, 12.67it/s]\u001b[A\n",
      "Epoch 184/300:  18%|█▊        | 4/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 184/300:  27%|██▋       | 6/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 184/300:  36%|███▋      | 8/22 [00:00<00:01, 13.36it/s]\u001b[A\n",
      "Epoch 184/300:  45%|████▌     | 10/22 [00:00<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 184/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 184/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 184/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 184/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 184/300:  91%|█████████ | 20/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 184/300: 100%|██████████| 22/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      " 61%|██████▏   | 184/300 [05:36<03:32,  1.83s/it]             \u001b[A\n",
      "Epoch 185/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 185/300:   9%|▉         | 2/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 185/300:  18%|█▊        | 4/22 [00:00<00:01, 14.07it/s]\u001b[A\n",
      "Epoch 185/300:  27%|██▋       | 6/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 185/300:  36%|███▋      | 8/22 [00:00<00:01, 13.96it/s]\u001b[A\n",
      "Epoch 185/300:  45%|████▌     | 10/22 [00:00<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 185/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 185/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 185/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.26it/s]\u001b[A\n",
      "Epoch 185/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 185/300:  91%|█████████ | 20/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 185/300: 100%|██████████| 22/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      " 62%|██████▏   | 185/300 [05:38<03:30,  1.83s/it]             \u001b[A\n",
      "Epoch 186/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 186/300:   9%|▉         | 2/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 186/300:  18%|█▊        | 4/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 186/300:  27%|██▋       | 6/22 [00:00<00:01, 13.84it/s]\u001b[A\n",
      "Epoch 186/300:  36%|███▋      | 8/22 [00:00<00:01, 13.92it/s]\u001b[A\n",
      "Epoch 186/300:  45%|████▌     | 10/22 [00:00<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 186/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 186/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 186/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 186/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 186/300:  91%|█████████ | 20/22 [00:01<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 186/300: 100%|██████████| 22/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      " 62%|██████▏   | 186/300 [05:39<03:27,  1.82s/it]             \u001b[A\n",
      "Epoch 187/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 187/300:   9%|▉         | 2/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 187/300:  18%|█▊        | 4/22 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "Epoch 187/300:  27%|██▋       | 6/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 187/300:  36%|███▋      | 8/22 [00:00<00:01, 12.89it/s]\u001b[A\n",
      "Epoch 187/300:  45%|████▌     | 10/22 [00:00<00:00, 12.84it/s]\u001b[A\n",
      "Epoch 187/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.17it/s]\u001b[A\n",
      "Epoch 187/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 187/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 187/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 187/300:  91%|█████████ | 20/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 187/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      " 62%|██████▏   | 187/300 [05:41<03:26,  1.83s/it]             \u001b[A\n",
      "Epoch 188/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 188/300:   9%|▉         | 2/22 [00:00<00:01, 13.31it/s]\u001b[A\n",
      "Epoch 188/300:  18%|█▊        | 4/22 [00:00<00:01, 12.78it/s]\u001b[A\n",
      "Epoch 188/300:  27%|██▋       | 6/22 [00:00<00:01, 13.19it/s]\u001b[A\n",
      "Epoch 188/300:  36%|███▋      | 8/22 [00:00<00:01, 12.99it/s]\u001b[A\n",
      "Epoch 188/300:  45%|████▌     | 10/22 [00:00<00:00, 13.15it/s]\u001b[A\n",
      "Epoch 188/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.23it/s]\u001b[A\n",
      "Epoch 188/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 188/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 188/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 188/300:  91%|█████████ | 20/22 [00:01<00:00, 14.27it/s]\u001b[A\n",
      "Epoch 188/300: 100%|██████████| 22/22 [00:01<00:00, 14.21it/s]\u001b[A\n",
      " 63%|██████▎   | 188/300 [05:43<03:24,  1.83s/it]             \u001b[A\n",
      "Epoch 189/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 189/300:   9%|▉         | 2/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 189/300:  18%|█▊        | 4/22 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "Epoch 189/300:  27%|██▋       | 6/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 189/300:  36%|███▋      | 8/22 [00:00<00:01, 13.72it/s]\u001b[A\n",
      "Epoch 189/300:  45%|████▌     | 10/22 [00:00<00:00, 13.88it/s]\u001b[A\n",
      "Epoch 189/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 189/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 189/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 189/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 189/300:  91%|█████████ | 20/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 189/300: 100%|██████████| 22/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      " 63%|██████▎   | 189/300 [05:45<03:22,  1.83s/it]             \u001b[A\n",
      "Epoch 190/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 190/300:   9%|▉         | 2/22 [00:00<00:01, 15.01it/s]\u001b[A\n",
      "Epoch 190/300:  18%|█▊        | 4/22 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "Epoch 190/300:  27%|██▋       | 6/22 [00:00<00:01, 13.49it/s]\u001b[A\n",
      "Epoch 190/300:  36%|███▋      | 8/22 [00:00<00:01, 12.83it/s]\u001b[A\n",
      "Epoch 190/300:  45%|████▌     | 10/22 [00:00<00:00, 13.28it/s]\n",
      "Epoch 233/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 233/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 233/300:  91%|█████████ | 20/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 233/300: 100%|██████████| 22/22 [00:01<00:00, 14.17it/s]\u001b[A\n",
      " 78%|███████▊  | 233/300 [07:06<02:02,  1.83s/it]             \u001b[A\n",
      "Epoch 234/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 234/300:   9%|▉         | 2/22 [00:00<00:01, 14.02it/s]\u001b[A\n",
      "Epoch 234/300:  18%|█▊        | 4/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 234/300:  27%|██▋       | 6/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 234/300:  36%|███▋      | 8/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 234/300:  45%|████▌     | 10/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 234/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 234/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 234/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 234/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.58it/s]\u001b[A\n",
      "Epoch 234/300:  91%|█████████ | 20/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 234/300: 100%|██████████| 22/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      " 78%|███████▊  | 234/300 [07:07<02:00,  1.83s/it]             \u001b[A\n",
      "Epoch 235/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 235/300:   9%|▉         | 2/22 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "Epoch 235/300:  18%|█▊        | 4/22 [00:00<00:01, 14.57it/s]\u001b[A\n",
      "Epoch 235/300:  27%|██▋       | 6/22 [00:00<00:01, 14.14it/s]\u001b[A\n",
      "Epoch 235/300:  36%|███▋      | 8/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 235/300:  45%|████▌     | 10/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 235/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 235/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 235/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 235/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 235/300:  91%|█████████ | 20/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 235/300: 100%|██████████| 22/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      " 78%|███████▊  | 235/300 [07:09<01:58,  1.83s/it]             \u001b[A\n",
      "Epoch 236/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 236/300:   9%|▉         | 2/22 [00:00<00:01, 14.66it/s]\u001b[A\n",
      "Epoch 236/300:  18%|█▊        | 4/22 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "Epoch 236/300:  27%|██▋       | 6/22 [00:00<00:01, 13.00it/s]\u001b[A\n",
      "Epoch 236/300:  36%|███▋      | 8/22 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "Epoch 236/300:  45%|████▌     | 10/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 236/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 236/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 236/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 236/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 236/300:  91%|█████████ | 20/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 236/300: 100%|██████████| 22/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      " 79%|███████▊  | 236/300 [07:11<01:56,  1.83s/it]             \u001b[A\n",
      "Epoch 237/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 237/300:   9%|▉         | 2/22 [00:00<00:01, 14.15it/s]\u001b[A\n",
      "Epoch 237/300:  18%|█▊        | 4/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 237/300:  27%|██▋       | 6/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 237/300:  36%|███▋      | 8/22 [00:00<00:01, 13.63it/s]\u001b[A\n",
      "Epoch 237/300:  45%|████▌     | 10/22 [00:00<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 237/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 237/300:  64%|██████▎   | 14/22 [00:01<00:00, 14.16it/s]\u001b[A\n",
      "Epoch 237/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.75it/s]\u001b[A\n",
      "Epoch 237/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 237/300:  91%|█████████ | 20/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 237/300: 100%|██████████| 22/22 [00:01<00:00, 13.31it/s]\u001b[A\n",
      " 79%|███████▉  | 237/300 [07:13<01:55,  1.83s/it]             \u001b[A\n",
      "Epoch 238/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 238/300:   9%|▉         | 2/22 [00:00<00:01, 14.92it/s]\u001b[A\n",
      "Epoch 238/300:  18%|█▊        | 4/22 [00:00<00:01, 14.02it/s]\u001b[A\n",
      "Epoch 238/300:  27%|██▋       | 6/22 [00:00<00:01, 14.35it/s]\u001b[A\n",
      "Epoch 238/300:  36%|███▋      | 8/22 [00:00<00:01, 13.81it/s]\u001b[A\n",
      "Epoch 238/300:  45%|████▌     | 10/22 [00:00<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 238/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.64it/s]\u001b[A\n",
      "Epoch 238/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 238/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 238/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 238/300:  91%|█████████ | 20/22 [00:01<00:00, 13.65it/s]\u001b[A\n",
      "Epoch 238/300: 100%|██████████| 22/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      " 79%|███████▉  | 238/300 [07:15<01:52,  1.82s/it]             \u001b[A\n",
      "Epoch 239/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 239/300:   9%|▉         | 2/22 [00:00<00:01, 13.15it/s]\u001b[A\n",
      "Epoch 239/300:  18%|█▊        | 4/22 [00:00<00:01, 13.46it/s]\u001b[A\n",
      "Epoch 239/300:  27%|██▋       | 6/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 239/300:  36%|███▋      | 8/22 [00:00<00:01, 13.78it/s]\u001b[A\n",
      "Epoch 239/300:  45%|████▌     | 10/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 239/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 239/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 239/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.44it/s]\u001b[A\n",
      "Epoch 239/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.22it/s]\u001b[A\n",
      "Epoch 239/300:  91%|█████████ | 20/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 239/300: 100%|██████████| 22/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      " 80%|███████▉  | 239/300 [07:16<01:50,  1.82s/it]             \u001b[A\n",
      "Epoch 240/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 240/300:   9%|▉         | 2/22 [00:00<00:01, 15.96it/s]\u001b[A\n",
      "Epoch 240/300:  18%|█▊        | 4/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 240/300:  27%|██▋       | 6/22 [00:00<00:01, 14.58it/s]\u001b[A\n",
      "Epoch 240/300:  36%|███▋      | 8/22 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "Epoch 240/300:  45%|████▌     | 10/22 [00:00<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 240/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 240/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.24it/s]\u001b[A\n",
      "Epoch 240/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.40it/s]\u001b[A\n",
      "Epoch 240/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 240/300:  91%|█████████ | 20/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 240/300: 100%|██████████| 22/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      " 80%|████████  | 240/300 [07:18<01:49,  1.82s/it]             \u001b[A\n",
      "Epoch 241/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 241/300:   9%|▉         | 2/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 241/300:  18%|█▊        | 4/22 [00:00<00:01, 13.35it/s]\u001b[A\n",
      "Epoch 241/300:  27%|██▋       | 6/22 [00:00<00:01, 13.65it/s]\u001b[A\n",
      "Epoch 241/300:  36%|███▋      | 8/22 [00:00<00:01, 13.70it/s]\u001b[A\n",
      "Epoch 241/300:  45%|████▌     | 10/22 [00:00<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 241/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 241/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 241/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 241/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 241/300:  91%|█████████ | 20/22 [00:01<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 241/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 80%|████████  | 241/300 [07:20<01:47,  1.82s/it]             \u001b[A\n",
      "Epoch 242/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 242/300:   9%|▉         | 2/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 242/300:  18%|█▊        | 4/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 242/300:  27%|██▋       | 6/22 [00:00<00:01, 13.67it/s]\u001b[A\n",
      "Epoch 242/300:  36%|███▋      | 8/22 [00:00<00:01, 13.64it/s]\u001b[A\n",
      "Epoch 242/300:  45%|████▌     | 10/22 [00:00<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 242/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 242/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 242/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 242/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 242/300:  91%|█████████ | 20/22 [00:01<00:00, 13.41it/s]\u001b[A\n",
      "Epoch 242/300: 100%|██████████| 22/22 [00:01<00:00, 13.44it/s]\u001b[A\n",
      " 81%|████████  | 242/300 [07:22<01:46,  1.83s/it]             \u001b[A\n",
      "Epoch 243/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 243/300:   9%|▉         | 2/22 [00:00<00:01, 13.26it/s]\u001b[A\n",
      "Epoch 243/300:  18%|█▊        | 4/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 243/300:  27%|██▋       | 6/22 [00:00<00:01, 12.93it/s]\u001b[A\n",
      "Epoch 243/300:  36%|███▋      | 8/22 [00:00<00:01, 13.21it/s]\u001b[A\n",
      "Epoch 243/300:  45%|████▌     | 10/22 [00:00<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 243/300:  55%|█████▍    | 12/22 [00:00<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 243/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 243/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 243/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.98it/s]\u001b[A\n",
      "Epoch 243/300:  91%|█████████ | 20/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 243/300: 100%|██████████| 22/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      " 81%|████████  | 243/300 [07:24<01:44,  1.83s/it]             \u001b[A\n",
      "Epoch 244/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 244/300:   9%|▉         | 2/22 [00:00<00:01, 14.41it/s]\u001b[A\n",
      "Epoch 244/300:  18%|█▊        | 4/22 [00:00<00:01, 12.98it/s]\u001b[A\n",
      "Epoch 244/300:  27%|██▋       | 6/22 [00:00<00:01, 12.94it/s]\u001b[A\n",
      "Epoch 244/300:  36%|███▋      | 8/22 [00:00<00:01, 13.69it/s]\u001b[A\n",
      "Epoch 244/300:  45%|████▌     | 10/22 [00:00<00:00, 13.42it/s]\u001b[A\n",
      "Epoch 244/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.37it/s]\u001b[A\n",
      "Epoch 244/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 244/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.12it/s]\u001b[A\n",
      "Epoch 244/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 244/300:  91%|█████████ | 20/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 244/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 81%|████████▏ | 244/300 [07:26<01:42,  1.83s/it]             \u001b[A\n",
      "Epoch 245/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 245/300:   9%|▉         | 2/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 245/300:  18%|█▊        | 4/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 245/300:  27%|██▋       | 6/22 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "Epoch 245/300:  36%|███▋      | 8/22 [00:00<00:01, 13.41it/s]\u001b[A\n",
      "Epoch 245/300:  45%|████▌     | 10/22 [00:00<00:00, 13.48it/s]\u001b[A\n",
      "Epoch 245/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 245/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 245/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 245/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 245/300:  91%|█████████ | 20/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 245/300: 100%|██████████| 22/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      " 82%|████████▏ | 245/300 [07:27<01:40,  1.83s/it]             \u001b[A\n",
      "Epoch 246/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 246/300:   9%|▉         | 2/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 246/300:  18%|█▊        | 4/22 [00:00<00:01, 13.38it/s]\u001b[A\n",
      "Epoch 246/300:  27%|██▋       | 6/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 246/300:  36%|███▋      | 8/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 246/300:  45%|████▌     | 10/22 [00:00<00:00, 13.44it/s]\u001b[A\n",
      "Epoch 246/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 246/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 246/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 246/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 246/300:  91%|█████████ | 20/22 [00:01<00:00, 13.36it/s]\u001b[A\n",
      "Epoch 246/300: 100%|██████████| 22/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      " 82%|████████▏ | 246/300 [07:29<01:39,  1.84s/it]             \u001b[A\n",
      "Epoch 247/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 247/300:   9%|▉         | 2/22 [00:00<00:01, 13.08it/s]\u001b[A\n",
      "Epoch 247/300:  18%|█▊        | 4/22 [00:00<00:01, 14.10it/s]\u001b[A\n",
      "Epoch 247/300:  27%|██▋       | 6/22 [00:00<00:01, 13.20it/s]\u001b[A\n",
      "Epoch 247/300:  36%|███▋      | 8/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 247/300:  45%|████▌     | 10/22 [00:00<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 247/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 247/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 247/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 247/300:  82%|████████▏ | 18/22 [00:01<00:00, 12.97it/s]\u001b[A\n",
      "Epoch 247/300:  91%|█████████ | 20/22 [00:01<00:00, 13.35it/s]\u001b[A\n",
      "Epoch 247/300: 100%|██████████| 22/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      " 82%|████████▏ | 247/300 [07:31<01:37,  1.84s/it]             \u001b[A\n",
      "Epoch 248/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 248/300:   9%|▉         | 2/22 [00:00<00:01, 14.86it/s]\u001b[A\n",
      "Epoch 248/300:  18%|█▊        | 4/22 [00:00<00:01, 13.48it/s]\u001b[A\n",
      "Epoch 248/300:  27%|██▋       | 6/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 248/300:  36%|███▋      | 8/22 [00:00<00:01, 13.73it/s]\u001b[A\n",
      "Epoch 248/300:  45%|████▌     | 10/22 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 248/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 248/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 248/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 248/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 248/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 248/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 83%|████████▎ | 248/300 [07:33<01:35,  1.83s/it]             \u001b[A\n",
      "Epoch 249/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 249/300:   9%|▉         | 2/22 [00:00<00:01, 14.42it/s]\u001b[A\n",
      "Epoch 249/300:  18%|█▊        | 4/22 [00:00<00:01, 13.74it/s]\u001b[A\n",
      "Epoch 249/300:  27%|██▋       | 6/22 [00:00<00:01, 14.22it/s]\u001b[A\n",
      "Epoch 249/300:  36%|███▋      | 8/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 249/300:  45%|████▌     | 10/22 [00:00<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 249/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 249/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.38it/s]\u001b[A\n",
      "Epoch 249/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.47it/s]\u001b[A\n",
      "Epoch 249/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 249/300:  91%|█████████ | 20/22 [00:01<00:00, 13.23it/s]\u001b[A\n",
      "Epoch 249/300: 100%|██████████| 22/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      " 83%|████████▎ | 249/300 [07:35<01:33,  1.83s/it]             \u001b[A\n",
      "Epoch 250/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 250/300:   9%|▉         | 2/22 [00:00<00:01, 14.13it/s]\u001b[A\n",
      "Epoch 250/300:  18%|█▊        | 4/22 [00:00<00:01, 13.55it/s]\u001b[A\n",
      "Epoch 250/300:  27%|██▋       | 6/22 [00:00<00:01, 13.32it/s]\u001b[A\n",
      "Epoch 250/300:  36%|███▋      | 8/22 [00:00<00:01, 13.55it/s]\u001b[A\n",
      "Epoch 250/300:  45%|████▌     | 10/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 250/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 250/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.82it/s]\u001b[A\n",
      "Epoch 250/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 250/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      "Epoch 250/300:  91%|█████████ | 20/22 [00:01<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 250/300: 100%|██████████| 22/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      " 83%|████████▎ | 250/300 [07:37<01:31,  1.83s/it]             \u001b[A\n",
      "Epoch 251/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 251/300:   9%|▉         | 2/22 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "Epoch 251/300:  18%|█▊        | 4/22 [00:00<00:01, 14.60it/s]\u001b[A\n",
      "Epoch 251/300:  27%|██▋       | 6/22 [00:00<00:01, 14.11it/s]\u001b[A\n",
      "Epoch 251/300:  36%|███▋      | 8/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 251/300:  45%|████▌     | 10/22 [00:00<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 251/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.27it/s]\u001b[A\n",
      "Epoch 251/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.26it/s]\u001b[A\n",
      "Epoch 251/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 251/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 251/300:  91%|█████████ | 20/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 251/300: 100%|██████████| 22/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      " 84%|████████▎ | 251/300 [07:38<01:29,  1.82s/it]             \u001b[A\n",
      "Epoch 252/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 252/300:   9%|▉         | 2/22 [00:00<00:01, 13.98it/s]\u001b[A\n",
      "Epoch 252/300:  18%|█▊        | 4/22 [00:00<00:01, 14.12it/s]\u001b[A\n",
      "Epoch 252/300:  27%|██▋       | 6/22 [00:00<00:01, 14.03it/s]\u001b[A\n",
      "Epoch 252/300:  36%|███▋      | 8/22 [00:00<00:00, 14.10it/s]\u001b[A\n",
      "Epoch 252/300:  45%|████▌     | 10/22 [00:00<00:00, 13.95it/s]\u001b[A\n",
      "Epoch 252/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 252/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.25it/s]\u001b[A\n",
      "Epoch 252/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 252/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 252/300:  91%|█████████ | 20/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 252/300: 100%|██████████| 22/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      " 84%|████████▍ | 252/300 [07:40<01:27,  1.82s/it]             \u001b[A\n",
      "Epoch 253/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 253/300:   9%|▉         | 2/22 [00:00<00:01, 15.21it/s]\u001b[A\n",
      "Epoch 253/300:  18%|█▊        | 4/22 [00:00<00:01, 14.30it/s]\u001b[A\n",
      "Epoch 253/300:  27%|██▋       | 6/22 [00:00<00:01, 14.05it/s]\u001b[A\n",
      "Epoch 253/300:  36%|███▋      | 8/22 [00:00<00:01, 13.50it/s]\u001b[A\n",
      "Epoch 253/300:  45%|████▌     | 10/22 [00:00<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 253/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.31it/s]\u001b[A\n",
      "Epoch 253/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.79it/s]\u001b[A\n",
      "Epoch 253/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      "Epoch 253/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      "Epoch 253/300:  91%|█████████ | 20/22 [00:01<00:00, 14.21it/s]\u001b[A\n",
      "Epoch 253/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 84%|████████▍ | 253/300 [07:42<01:25,  1.82s/it]             \u001b[A\n",
      "Epoch 254/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 254/300:   9%|▉         | 2/22 [00:00<00:01, 13.03it/s]\u001b[A\n",
      "Epoch 254/300:  18%|█▊        | 4/22 [00:00<00:01, 13.28it/s]\u001b[A\n",
      "Epoch 254/300:  27%|██▋       | 6/22 [00:00<00:01, 14.09it/s]\u001b[A\n",
      "Epoch 254/300:  36%|███▋      | 8/22 [00:00<00:01, 13.94it/s]\u001b[A\n",
      "Epoch 254/300:  45%|████▌     | 10/22 [00:00<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 254/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 254/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 254/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 254/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 254/300:  91%|█████████ | 20/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 254/300: 100%|██████████| 22/22 [00:01<00:00, 13.56it/s]\u001b[A\n",
      " 85%|████████▍ | 254/300 [07:44<01:23,  1.82s/it]             \u001b[A\n",
      "Epoch 255/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 255/300:   9%|▉         | 2/22 [00:00<00:01, 14.13it/s]\u001b[A\n",
      "Epoch 255/300:  18%|█▊        | 4/22 [00:00<00:01, 13.91it/s]\u001b[A\n",
      "Epoch 255/300:  27%|██▋       | 6/22 [00:00<00:01, 13.95it/s]\u001b[A\n",
      "Epoch 255/300:  36%|███▋      | 8/22 [00:00<00:00, 14.26it/s]\u001b[A\n",
      "Epoch 255/300:  45%|████▌     | 10/22 [00:00<00:00, 13.71it/s]\u001b[A\n",
      "Epoch 255/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 255/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      "Epoch 255/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.61it/s]\u001b[A\n",
      "Epoch 255/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.24it/s]\u001b[A\n",
      "Epoch 255/300:  91%|█████████ | 20/22 [00:01<00:00, 13.49it/s]\u001b[A\n",
      "Epoch 255/300: 100%|██████████| 22/22 [00:01<00:00, 13.77it/s]\u001b[A\n",
      " 85%|████████▌ | 255/300 [07:46<01:21,  1.82s/it]             \u001b[A\n",
      "Epoch 256/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 256/300:   9%|▉         | 2/22 [00:00<00:01, 14.78it/s]\u001b[A\n",
      "Epoch 256/300:  18%|█▊        | 4/22 [00:00<00:01, 14.21it/s]\u001b[A\n",
      "Epoch 256/300:  27%|██▋       | 6/22 [00:00<00:01, 14.04it/s]\u001b[A\n",
      "Epoch 256/300:  36%|███▋      | 8/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 256/300:  45%|████▌     | 10/22 [00:00<00:00, 13.92it/s]\u001b[A\n",
      "Epoch 256/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.97it/s]\u001b[A\n",
      "Epoch 256/300:  64%|██████▎   | 14/22 [00:00<00:00, 14.15it/s]\u001b[A\n",
      "Epoch 256/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.02it/s]\u001b[A\n",
      "Epoch 256/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.86it/s]\u001b[A\n",
      "Epoch 256/300:  91%|█████████ | 20/22 [00:01<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 256/300: 100%|██████████| 22/22 [00:01<00:00, 13.89it/s]\u001b[A\n",
      " 85%|████████▌ | 256/300 [07:48<01:19,  1.81s/it]             \u001b[A\n",
      "Epoch 257/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 257/300:   9%|▉         | 2/22 [00:00<00:01, 13.89it/s]\u001b[A\n",
      "Epoch 257/300:  18%|█▊        | 4/22 [00:00<00:01, 13.61it/s]\u001b[A\n",
      "Epoch 257/300:  27%|██▋       | 6/22 [00:00<00:01, 12.74it/s]\u001b[A\n",
      "Epoch 257/300:  36%|███▋      | 8/22 [00:00<00:01, 13.44it/s]\u001b[A\n",
      "Epoch 257/300:  45%|████▌     | 10/22 [00:00<00:00, 13.11it/s]\u001b[A\n",
      "Epoch 257/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 257/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.21it/s]\u001b[A\n",
      "Epoch 257/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 257/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 257/300:  91%|█████████ | 20/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      "Epoch 257/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 86%|████████▌ | 257/300 [07:49<01:18,  1.82s/it]             \u001b[A\n",
      "Epoch 258/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 258/300:   9%|▉         | 2/22 [00:00<00:01, 13.91it/s]\u001b[A\n",
      "Epoch 258/300:  18%|█▊        | 4/22 [00:00<00:01, 13.80it/s]\u001b[A\n",
      "Epoch 258/300:  27%|██▋       | 6/22 [00:00<00:01, 13.99it/s]\u001b[A\n",
      "Epoch 258/300:  36%|███▋      | 8/22 [00:00<00:00, 14.24it/s]\u001b[A\n",
      "Epoch 258/300:  45%|████▌     | 10/22 [00:00<00:00, 13.94it/s]\u001b[A\n",
      "Epoch 258/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.85it/s]\u001b[A\n",
      "Epoch 258/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 258/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.57it/s]\u001b[A\n",
      "Epoch 258/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 258/300:  91%|█████████ | 20/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 258/300: 100%|██████████| 22/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      " 86%|████████▌ | 258/300 [07:51<01:16,  1.82s/it]             \u001b[A\n",
      "Epoch 259/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 259/300:   9%|▉         | 2/22 [00:00<00:01, 14.39it/s]\u001b[A\n",
      "Epoch 259/300:  18%|█▊        | 4/22 [00:00<00:01, 13.83it/s]\u001b[A\n",
      "Epoch 259/300:  27%|██▋       | 6/22 [00:00<00:01, 13.75it/s]\u001b[A\n",
      "Epoch 259/300:  36%|███▋      | 8/22 [00:00<00:01, 13.37it/s]\u001b[A\n",
      "Epoch 259/300:  45%|████▌     | 10/22 [00:00<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 259/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 259/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 259/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 259/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 259/300:  91%|█████████ | 20/22 [00:01<00:00, 13.81it/s]\u001b[A\n",
      "Epoch 259/300: 100%|██████████| 22/22 [00:01<00:00, 13.93it/s]\u001b[A\n",
      " 86%|████████▋ | 259/300 [07:53<01:14,  1.82s/it]             \u001b[A\n",
      "Epoch 260/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 260/300:   9%|▉         | 2/22 [00:00<00:01, 13.10it/s]\u001b[A\n",
      "Epoch 260/300:  18%|█▊        | 4/22 [00:00<00:01, 12.73it/s]\u001b[A\n",
      "Epoch 260/300:  27%|██▋       | 6/22 [00:00<00:01, 13.33it/s]\u001b[A\n",
      "Epoch 260/300:  36%|███▋      | 8/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 260/300:  45%|████▌     | 10/22 [00:00<00:00, 13.74it/s]\u001b[A\n",
      "Epoch 260/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.84it/s]\u001b[A\n",
      "Epoch 260/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 260/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.05it/s]\u001b[A\n",
      "Epoch 260/300:  82%|████████▏ | 18/22 [00:01<00:00, 14.01it/s]\u001b[A\n",
      "Epoch 260/300:  91%|█████████ | 20/22 [00:01<00:00, 13.87it/s]\u001b[A\n",
      "Epoch 260/300: 100%|██████████| 22/22 [00:01<00:00, 13.91it/s]\u001b[A\n",
      " 87%|████████▋ | 260/300 [07:55<01:12,  1.82s/it]             \u001b[A\n",
      "Epoch 261/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 261/300:   9%|▉         | 2/22 [00:00<00:01, 14.29it/s]\u001b[A\n",
      "Epoch 261/300:  18%|█▊        | 4/22 [00:00<00:01, 13.91it/s]\u001b[A\n",
      "Epoch 261/300:  27%|██▋       | 6/22 [00:00<00:01, 14.26it/s]\u001b[A\n",
      "Epoch 261/300:  36%|███▋      | 8/22 [00:00<00:00, 14.08it/s]\u001b[A\n",
      "Epoch 261/300:  45%|████▌     | 10/22 [00:00<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 261/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.51it/s]\u001b[A\n",
      "Epoch 261/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.70it/s]\u001b[A\n",
      "Epoch 261/300:  73%|███████▎  | 16/22 [00:01<00:00, 14.09it/s]\u001b[A\n",
      "Epoch 261/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 261/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 261/300: 100%|██████████| 22/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      " 87%|████████▋ | 261/300 [07:57<01:10,  1.82s/it]             \u001b[A\n",
      "Epoch 262/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 262/300:   9%|▉         | 2/22 [00:00<00:01, 13.29it/s]\u001b[A\n",
      "Epoch 262/300:  18%|█▊        | 4/22 [00:00<00:01, 13.47it/s]\u001b[A\n",
      "Epoch 262/300:  27%|██▋       | 6/22 [00:00<00:01, 13.18it/s]\u001b[A\n",
      "Epoch 262/300:  36%|███▋      | 8/22 [00:00<00:01, 13.85it/s]\u001b[A\n",
      "Epoch 262/300:  45%|████▌     | 10/22 [00:00<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 262/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.60it/s]\u001b[A\n",
      "Epoch 262/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.96it/s]\u001b[A\n",
      "Epoch 262/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      "Epoch 262/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.83it/s]\u001b[A\n",
      "Epoch 262/300:  91%|█████████ | 20/22 [00:01<00:00, 13.73it/s]\u001b[A\n",
      "Epoch 262/300: 100%|██████████| 22/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      " 87%|████████▋ | 262/300 [07:58<01:09,  1.82s/it]             \u001b[A\n",
      "Epoch 263/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 263/300:   9%|▉         | 2/22 [00:00<00:01, 13.86it/s]\u001b[A\n",
      "Epoch 263/300:  18%|█▊        | 4/22 [00:00<00:01, 13.82it/s]\u001b[A\n",
      "Epoch 263/300:  27%|██▋       | 6/22 [00:00<00:01, 13.40it/s]\u001b[A\n",
      "Epoch 263/300:  36%|███▋      | 8/22 [00:00<00:01, 13.17it/s]\u001b[A\n",
      "Epoch 263/300:  45%|████▌     | 10/22 [00:00<00:00, 13.46it/s]\u001b[A\n",
      "Epoch 263/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.69it/s]\u001b[A\n",
      "Epoch 263/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.67it/s]\u001b[A\n",
      "Epoch 263/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.45it/s]\u001b[A\n",
      "Epoch 263/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.34it/s]\u001b[A\n",
      "Epoch 263/300:  91%|█████████ | 20/22 [00:01<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 263/300: 100%|██████████| 22/22 [00:01<00:00, 13.80it/s]\u001b[A\n",
      " 88%|████████▊ | 263/300 [08:00<01:07,  1.82s/it]             \u001b[A\n",
      "Epoch 264/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 264/300:   9%|▉         | 2/22 [00:00<00:01, 14.67it/s]\u001b[A\n",
      "Epoch 264/300:  18%|█▊        | 4/22 [00:00<00:01, 14.66it/s]\u001b[A\n",
      "Epoch 264/300:  27%|██▋       | 6/22 [00:00<00:01, 13.53it/s]\u001b[A\n",
      "Epoch 264/300:  36%|███▋      | 8/22 [00:00<00:01, 13.19it/s]\u001b[A\n",
      "Epoch 264/300:  45%|████▌     | 10/22 [00:00<00:00, 13.02it/s]\u001b[A\n",
      "Epoch 264/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.11it/s]\u001b[A\n",
      "Epoch 264/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 264/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.72it/s]\u001b[A\n",
      "Epoch 264/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.53it/s]\u001b[A\n",
      "Epoch 264/300:  91%|█████████ | 20/22 [00:01<00:00, 13.29it/s]\u001b[A\n",
      "Epoch 264/300: 100%|██████████| 22/22 [00:01<00:00, 13.63it/s]\u001b[A\n",
      " 88%|████████▊ | 264/300 [08:02<01:05,  1.83s/it]             \u001b[A\n",
      "Epoch 265/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 265/300:   9%|▉         | 2/22 [00:00<00:01, 13.68it/s]\u001b[A\n",
      "Epoch 265/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 265/300:  27%|██▋       | 6/22 [00:00<00:01, 13.60it/s]\u001b[A\n",
      "Epoch 265/300:  36%|███▋      | 8/22 [00:00<00:01, 13.90it/s]\u001b[A\n",
      "Epoch 265/300:  45%|████▌     | 10/22 [00:00<00:00, 13.99it/s]\u001b[A\n",
      "Epoch 265/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 265/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.55it/s]\u001b[A\n",
      "Epoch 265/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.32it/s]\u001b[A\n",
      "Epoch 265/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.68it/s]\u001b[A\n",
      "Epoch 265/300:  91%|█████████ | 20/22 [00:01<00:00, 13.54it/s]\u001b[A\n",
      "Epoch 265/300: 100%|██████████| 22/22 [00:01<00:00, 13.43it/s]\u001b[A\n",
      " 88%|████████▊ | 265/300 [08:04<01:04,  1.83s/it]             \u001b[A\n",
      "Epoch 266/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 266/300:   9%|▉         | 2/22 [00:00<00:01, 14.11it/s]\u001b[A\n",
      "Epoch 266/300:  18%|█▊        | 4/22 [00:00<00:01, 13.10it/s]\u001b[A\n",
      "Epoch 266/300:  27%|██▋       | 6/22 [00:00<00:01, 13.77it/s]\u001b[A\n",
      "Epoch 266/300:  36%|███▋      | 8/22 [00:00<00:01, 13.76it/s]\u001b[A\n",
      "Epoch 266/300:  45%|████▌     | 10/22 [00:00<00:00, 13.62it/s]\u001b[A\n",
      "Epoch 266/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.43it/s]\u001b[A\n",
      "Epoch 266/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.76it/s]\u001b[A\n",
      "Epoch 266/300:  73%|███████▎  | 16/22 [00:01<00:00, 13.50it/s]\u001b[A\n",
      "Epoch 266/300:  82%|████████▏ | 18/22 [00:01<00:00, 13.52it/s]\u001b[A\n",
      "Epoch 266/300:  91%|█████████ | 20/22 [00:01<00:00, 13.59it/s]\u001b[A\n",
      "Epoch 266/300: 100%|██████████| 22/22 [00:01<00:00, 13.78it/s]\u001b[A\n",
      " 89%|████████▊ | 266/300 [08:06<01:02,  1.83s/it]             \u001b[A\n",
      "Epoch 267/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 267/300:   9%|▉         | 2/22 [00:00<00:01, 14.18it/s]\u001b[A\n",
      "Epoch 267/300:  18%|█▊        | 4/22 [00:00<00:01, 13.58it/s]\u001b[A\n",
      "Epoch 267/300:  27%|██▋       | 6/22 [00:00<00:01, 13.12it/s]\u001b[A\n",
      "Epoch 267/300:  36%|███▋      | 8/22 [00:00<00:01, 13.07it/s]\u001b[A\n",
      "Epoch 267/300:  45%|████▌     | 10/22 [00:00<00:00, 13.66it/s]\u001b[A\n",
      "Epoch 267/300:  55%|█████▍    | 12/22 [00:00<00:00, 13.93it/s]\u001b[A\n",
      "Epoch 267/300:  64%|██████▎   | 14/22 [00:01<00:00, 13.94it/s]\n",
      "Epoch 128/300:  36%|███▋      | 8/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 128/300:  45%|████▌     | 10/22 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 128/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 128/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 128/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 128/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 128/300:  91%|█████████ | 20/22 [00:01<00:00, 16.36it/s]\u001b[A\n",
      "Epoch 128/300: 100%|██████████| 22/22 [00:01<00:00, 16.14it/s]\u001b[A\n",
      " 43%|████▎     | 128/300 [03:20<04:29,  1.57s/it]             \u001b[A\n",
      "Epoch 129/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 129/300:   9%|▉         | 2/22 [00:00<00:01, 16.44it/s]\u001b[A\n",
      "Epoch 129/300:  18%|█▊        | 4/22 [00:00<00:01, 16.04it/s]\u001b[A\n",
      "Epoch 129/300:  27%|██▋       | 6/22 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 129/300:  36%|███▋      | 8/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 129/300:  45%|████▌     | 10/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 129/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "Epoch 129/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 129/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 129/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 129/300:  91%|█████████ | 20/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 129/300: 100%|██████████| 22/22 [00:01<00:00, 15.67it/s]\u001b[A\n",
      " 43%|████▎     | 129/300 [03:22<04:27,  1.57s/it]             \u001b[A\n",
      "Epoch 130/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 130/300:   9%|▉         | 2/22 [00:00<00:01, 16.51it/s]\u001b[A\n",
      "Epoch 130/300:  18%|█▊        | 4/22 [00:00<00:01, 16.22it/s]\u001b[A\n",
      "Epoch 130/300:  27%|██▋       | 6/22 [00:00<00:01, 15.63it/s]\u001b[A\n",
      "Epoch 130/300:  36%|███▋      | 8/22 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "Epoch 130/300:  45%|████▌     | 10/22 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "Epoch 130/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 130/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 130/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 130/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 130/300:  91%|█████████ | 20/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 130/300: 100%|██████████| 22/22 [00:01<00:00, 15.98it/s]\u001b[A\n",
      " 43%|████▎     | 130/300 [03:23<04:26,  1.57s/it]             \u001b[A\n",
      "Epoch 131/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 131/300:   9%|▉         | 2/22 [00:00<00:01, 16.75it/s]\u001b[A\n",
      "Epoch 131/300:  18%|█▊        | 4/22 [00:00<00:01, 15.64it/s]\u001b[A\n",
      "Epoch 131/300:  27%|██▋       | 6/22 [00:00<00:01, 15.96it/s]\u001b[A\n",
      "Epoch 131/300:  36%|███▋      | 8/22 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 131/300:  45%|████▌     | 10/22 [00:00<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 131/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 131/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 131/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 131/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 131/300:  91%|█████████ | 20/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 131/300: 100%|██████████| 22/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      " 44%|████▎     | 131/300 [03:25<04:24,  1.57s/it]             \u001b[A\n",
      "Epoch 132/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 132/300:   9%|▉         | 2/22 [00:00<00:01, 15.99it/s]\u001b[A\n",
      "Epoch 132/300:  18%|█▊        | 4/22 [00:00<00:01, 16.16it/s]\u001b[A\n",
      "Epoch 132/300:  27%|██▋       | 6/22 [00:00<00:01, 15.76it/s]\u001b[A\n",
      "Epoch 132/300:  36%|███▋      | 8/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 132/300:  45%|████▌     | 10/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 132/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 132/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 132/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.91it/s]\u001b[A\n",
      "Epoch 132/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 132/300:  91%|█████████ | 20/22 [00:01<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 132/300: 100%|██████████| 22/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
      " 44%|████▍     | 132/300 [03:27<04:23,  1.57s/it]             \u001b[A\n",
      "Epoch 133/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 133/300:   9%|▉         | 2/22 [00:00<00:01, 15.20it/s]\u001b[A\n",
      "Epoch 133/300:  18%|█▊        | 4/22 [00:00<00:01, 15.14it/s]\u001b[A\n",
      "Epoch 133/300:  27%|██▋       | 6/22 [00:00<00:01, 15.42it/s]\u001b[A\n",
      "Epoch 133/300:  36%|███▋      | 8/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 133/300:  45%|████▌     | 10/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 133/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 133/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.67it/s]\u001b[A\n",
      "Epoch 133/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.54it/s]\u001b[A\n",
      "Epoch 133/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 133/300:  91%|█████████ | 20/22 [00:01<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 133/300: 100%|██████████| 22/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
      " 44%|████▍     | 133/300 [03:28<04:22,  1.57s/it]             \u001b[A\n",
      "Epoch 134/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 134/300:   9%|▉         | 2/22 [00:00<00:01, 16.05it/s]\u001b[A\n",
      "Epoch 134/300:  18%|█▊        | 4/22 [00:00<00:01, 16.34it/s]\u001b[A\n",
      "Epoch 134/300:  27%|██▋       | 6/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 134/300:  36%|███▋      | 8/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 134/300:  45%|████▌     | 10/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 134/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 134/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 134/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 134/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 134/300:  91%|█████████ | 20/22 [00:01<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 134/300: 100%|██████████| 22/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      " 45%|████▍     | 134/300 [03:30<04:20,  1.57s/it]             \u001b[A\n",
      "Epoch 135/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 135/300:   9%|▉         | 2/22 [00:00<00:01, 15.33it/s]\u001b[A\n",
      "Epoch 135/300:  18%|█▊        | 4/22 [00:00<00:01, 15.42it/s]\u001b[A\n",
      "Epoch 135/300:  27%|██▋       | 6/22 [00:00<00:01, 15.17it/s]\u001b[A\n",
      "Epoch 135/300:  36%|███▋      | 8/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 135/300:  45%|████▌     | 10/22 [00:00<00:00, 15.37it/s]\u001b[A\n",
      "Epoch 135/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "Epoch 135/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 135/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 135/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 135/300:  91%|█████████ | 20/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 135/300: 100%|██████████| 22/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      " 45%|████▌     | 135/300 [03:31<04:19,  1.57s/it]             \u001b[A\n",
      "Epoch 136/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 136/300:   9%|▉         | 2/22 [00:00<00:01, 16.56it/s]\u001b[A\n",
      "Epoch 136/300:  18%|█▊        | 4/22 [00:00<00:01, 16.87it/s]\u001b[A\n",
      "Epoch 136/300:  27%|██▋       | 6/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 136/300:  36%|███▋      | 8/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 136/300:  45%|████▌     | 10/22 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 136/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 136/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 136/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 136/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 136/300:  91%|█████████ | 20/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 136/300: 100%|██████████| 22/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      " 45%|████▌     | 136/300 [03:33<04:17,  1.57s/it]             \u001b[A\n",
      "Epoch 137/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 137/300:   9%|▉         | 2/22 [00:00<00:01, 15.92it/s]\u001b[A\n",
      "Epoch 137/300:  18%|█▊        | 4/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 137/300:  27%|██▋       | 6/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 137/300:  36%|███▋      | 8/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 137/300:  45%|████▌     | 10/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 137/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 137/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 137/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 137/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 137/300:  91%|█████████ | 20/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 137/300: 100%|██████████| 22/22 [00:01<00:00, 15.76it/s]\u001b[A\n",
      " 46%|████▌     | 137/300 [03:34<04:15,  1.57s/it]             \u001b[A\n",
      "Epoch 138/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 138/300:   9%|▉         | 2/22 [00:00<00:01, 15.56it/s]\u001b[A\n",
      "Epoch 138/300:  18%|█▊        | 4/22 [00:00<00:01, 15.73it/s]\u001b[A\n",
      "Epoch 138/300:  27%|██▋       | 6/22 [00:00<00:01, 15.82it/s]\u001b[A\n",
      "Epoch 138/300:  36%|███▋      | 8/22 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 138/300:  45%|████▌     | 10/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 138/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 138/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 138/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 138/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 138/300:  91%|█████████ | 20/22 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 138/300: 100%|██████████| 22/22 [00:01<00:00, 16.22it/s]\u001b[A\n",
      " 46%|████▌     | 138/300 [03:36<04:13,  1.57s/it]             \u001b[A\n",
      "Epoch 139/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 139/300:   9%|▉         | 2/22 [00:00<00:01, 17.23it/s]\u001b[A\n",
      "Epoch 139/300:  18%|█▊        | 4/22 [00:00<00:01, 16.54it/s]\u001b[A\n",
      "Epoch 139/300:  27%|██▋       | 6/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 139/300:  36%|███▋      | 8/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 139/300:  45%|████▌     | 10/22 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "Epoch 139/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 139/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.43it/s]\u001b[A\n",
      "Epoch 139/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 139/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 139/300:  91%|█████████ | 20/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 139/300: 100%|██████████| 22/22 [00:01<00:00, 16.12it/s]\u001b[A\n",
      " 46%|████▋     | 139/300 [03:38<04:11,  1.56s/it]             \u001b[A\n",
      "Epoch 140/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 140/300:   9%|▉         | 2/22 [00:00<00:01, 15.39it/s]\u001b[A\n",
      "Epoch 140/300:  18%|█▊        | 4/22 [00:00<00:01, 15.39it/s]\u001b[A\n",
      "Epoch 140/300:  27%|██▋       | 6/22 [00:00<00:01, 15.74it/s]\u001b[A\n",
      "Epoch 140/300:  36%|███▋      | 8/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 140/300:  45%|████▌     | 10/22 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 140/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 140/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 140/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 140/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 140/300:  91%|█████████ | 20/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 140/300: 100%|██████████| 22/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      " 47%|████▋     | 140/300 [03:39<04:09,  1.56s/it]             \u001b[A\n",
      "Epoch 141/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 141/300:   9%|▉         | 2/22 [00:00<00:01, 18.16it/s]\u001b[A\n",
      "Epoch 141/300:  18%|█▊        | 4/22 [00:00<00:01, 17.48it/s]\u001b[A\n",
      "Epoch 141/300:  27%|██▋       | 6/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 141/300:  36%|███▋      | 8/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 141/300:  45%|████▌     | 10/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 141/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 141/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 141/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 141/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.46it/s]\u001b[A\n",
      "Epoch 141/300:  91%|█████████ | 20/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 141/300: 100%|██████████| 22/22 [00:01<00:00, 15.69it/s]\u001b[A\n",
      " 47%|████▋     | 141/300 [03:41<04:08,  1.56s/it]             \u001b[A\n",
      "Epoch 142/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 142/300:   9%|▉         | 2/22 [00:00<00:01, 16.01it/s]\u001b[A\n",
      "Epoch 142/300:  18%|█▊        | 4/22 [00:00<00:01, 15.78it/s]\u001b[A\n",
      "Epoch 142/300:  27%|██▋       | 6/22 [00:00<00:01, 15.66it/s]\u001b[A\n",
      "Epoch 142/300:  36%|███▋      | 8/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 142/300:  45%|████▌     | 10/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 142/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 142/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.46it/s]\u001b[A\n",
      "Epoch 142/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.65it/s]\u001b[A\n",
      "Epoch 142/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 142/300:  91%|█████████ | 20/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 142/300: 100%|██████████| 22/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      " 47%|████▋     | 142/300 [03:42<04:06,  1.56s/it]             \u001b[A\n",
      "Epoch 143/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 143/300:   9%|▉         | 2/22 [00:00<00:01, 15.49it/s]\u001b[A\n",
      "Epoch 143/300:  18%|█▊        | 4/22 [00:00<00:01, 15.25it/s]\u001b[A\n",
      "Epoch 143/300:  27%|██▋       | 6/22 [00:00<00:01, 15.21it/s]\u001b[A\n",
      "Epoch 143/300:  36%|███▋      | 8/22 [00:00<00:00, 15.38it/s]\u001b[A\n",
      "Epoch 143/300:  45%|████▌     | 10/22 [00:00<00:00, 15.40it/s]\u001b[A\n",
      "Epoch 143/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Epoch 143/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Epoch 143/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 143/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 143/300:  91%|█████████ | 20/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 143/300: 100%|██████████| 22/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      " 48%|████▊     | 143/300 [03:44<04:06,  1.57s/it]             \u001b[A\n",
      "Epoch 144/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 144/300:   9%|▉         | 2/22 [00:00<00:01, 15.90it/s]\u001b[A\n",
      "Epoch 144/300:  18%|█▊        | 4/22 [00:00<00:01, 15.64it/s]\u001b[A\n",
      "Epoch 144/300:  27%|██▋       | 6/22 [00:00<00:01, 15.32it/s]\u001b[A\n",
      "Epoch 144/300:  36%|███▋      | 8/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 144/300:  45%|████▌     | 10/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 144/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 144/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 144/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 144/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.37it/s]\u001b[A\n",
      "Epoch 144/300:  91%|█████████ | 20/22 [00:01<00:00, 15.28it/s]\u001b[A\n",
      "Epoch 144/300: 100%|██████████| 22/22 [00:01<00:00, 15.57it/s]\u001b[A\n",
      " 48%|████▊     | 144/300 [03:45<04:05,  1.57s/it]             \u001b[A\n",
      "Epoch 145/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 145/300:   9%|▉         | 2/22 [00:00<00:01, 16.01it/s]\u001b[A\n",
      "Epoch 145/300:  18%|█▊        | 4/22 [00:00<00:01, 15.83it/s]\u001b[A\n",
      "Epoch 145/300:  27%|██▋       | 6/22 [00:00<00:01, 15.61it/s]\u001b[A\n",
      "Epoch 145/300:  36%|███▋      | 8/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 145/300:  45%|████▌     | 10/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 145/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 145/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 145/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.91it/s]\u001b[A\n",
      "Epoch 145/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 145/300:  91%|█████████ | 20/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 145/300: 100%|██████████| 22/22 [00:01<00:00, 15.66it/s]\u001b[A\n",
      " 48%|████▊     | 145/300 [03:47<04:04,  1.58s/it]             \u001b[A\n",
      "Epoch 146/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 146/300:   9%|▉         | 2/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 146/300:  18%|█▊        | 4/22 [00:00<00:01, 16.35it/s]\u001b[A\n",
      "Epoch 146/300:  27%|██▋       | 6/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 146/300:  36%|███▋      | 8/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 146/300:  45%|████▌     | 10/22 [00:00<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 146/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 146/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 146/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 146/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 146/300:  91%|█████████ | 20/22 [00:01<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 146/300: 100%|██████████| 22/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      " 49%|████▊     | 146/300 [03:49<04:02,  1.58s/it]             \u001b[A\n",
      "Epoch 147/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 147/300:   9%|▉         | 2/22 [00:00<00:01, 16.93it/s]\u001b[A\n",
      "Epoch 147/300:  18%|█▊        | 4/22 [00:00<00:01, 16.60it/s]\u001b[A\n",
      "Epoch 147/300:  27%|██▋       | 6/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 147/300:  36%|███▋      | 8/22 [00:00<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 147/300:  45%|████▌     | 10/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 147/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 147/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 147/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 147/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 147/300:  91%|█████████ | 20/22 [00:01<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 147/300: 100%|██████████| 22/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      " 49%|████▉     | 147/300 [03:50<03:59,  1.57s/it]             \u001b[A\n",
      "Epoch 148/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 148/300:   9%|▉         | 2/22 [00:00<00:01, 16.27it/s]\u001b[A\n",
      "Epoch 148/300:  18%|█▊        | 4/22 [00:00<00:01, 16.26it/s]\u001b[A\n",
      "Epoch 148/300:  27%|██▋       | 6/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 148/300:  36%|███▋      | 8/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 148/300:  45%|████▌     | 10/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 148/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 148/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 148/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 148/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 148/300:  91%|█████████ | 20/22 [00:01<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 148/300: 100%|██████████| 22/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      " 49%|████▉     | 148/300 [03:52<03:57,  1.56s/it]             \u001b[A\n",
      "Epoch 149/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149/300:   9%|▉         | 2/22 [00:00<00:01, 17.20it/s]\u001b[A\n",
      "Epoch 149/300:  18%|█▊        | 4/22 [00:00<00:01, 16.35it/s]\u001b[A\n",
      "Epoch 149/300:  27%|██▋       | 6/22 [00:00<00:01, 15.87it/s]\u001b[A\n",
      "Epoch 149/300:  36%|███▋      | 8/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 149/300:  45%|████▌     | 10/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 149/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 149/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 149/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 149/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 149/300:  91%|█████████ | 20/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 149/300: 100%|██████████| 22/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      " 50%|████▉     | 149/300 [03:53<03:55,  1.56s/it]             \u001b[A\n",
      "Epoch 150/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 150/300:   9%|▉         | 2/22 [00:00<00:01, 16.93it/s]\u001b[A\n",
      "Epoch 150/300:  18%|█▊        | 4/22 [00:00<00:01, 16.74it/s]\u001b[A\n",
      "Epoch 150/300:  27%|██▋       | 6/22 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 150/300:  36%|███▋      | 8/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 150/300:  45%|████▌     | 10/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 150/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 150/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 150/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 150/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 150/300:  91%|█████████ | 20/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 150/300: 100%|██████████| 22/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      " 50%|█████     | 150/300 [03:55<03:53,  1.56s/it]             \u001b[A\n",
      "Epoch 151/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 151/300:   9%|▉         | 2/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 151/300:  18%|█▊        | 4/22 [00:00<00:01, 15.32it/s]\u001b[A\n",
      "Epoch 151/300:  27%|██▋       | 6/22 [00:00<00:01, 15.32it/s]\u001b[A\n",
      "Epoch 151/300:  36%|███▋      | 8/22 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 151/300:  45%|████▌     | 10/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 151/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 151/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 151/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 151/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 151/300:  91%|█████████ | 20/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 151/300: 100%|██████████| 22/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      " 50%|█████     | 151/300 [03:56<03:52,  1.56s/it]             \u001b[A\n",
      "Epoch 152/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 152/300:   9%|▉         | 2/22 [00:00<00:01, 16.28it/s]\u001b[A\n",
      "Epoch 152/300:  18%|█▊        | 4/22 [00:00<00:01, 15.65it/s]\u001b[A\n",
      "Epoch 152/300:  27%|██▋       | 6/22 [00:00<00:01, 15.18it/s]\u001b[A\n",
      "Epoch 152/300:  36%|███▋      | 8/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 152/300:  45%|████▌     | 10/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 152/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 152/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 152/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 152/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 152/300:  91%|█████████ | 20/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 152/300: 100%|██████████| 22/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      " 51%|█████     | 152/300 [03:58<03:51,  1.56s/it]             \u001b[A\n",
      "Epoch 153/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 153/300:   9%|▉         | 2/22 [00:00<00:01, 16.44it/s]\u001b[A\n",
      "Epoch 153/300:  18%|█▊        | 4/22 [00:00<00:01, 16.40it/s]\u001b[A\n",
      "Epoch 153/300:  27%|██▋       | 6/22 [00:00<00:01, 15.96it/s]\u001b[A\n",
      "Epoch 153/300:  36%|███▋      | 8/22 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "Epoch 153/300:  45%|████▌     | 10/22 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "Epoch 153/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 153/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.20it/s]\u001b[A\n",
      "Epoch 153/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.45it/s]\u001b[A\n",
      "Epoch 153/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 153/300:  91%|█████████ | 20/22 [00:01<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 153/300: 100%|██████████| 22/22 [00:01<00:00, 15.81it/s]\u001b[A\n",
      " 51%|█████     | 153/300 [04:00<03:50,  1.57s/it]             \u001b[A\n",
      "Epoch 154/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 154/300:   9%|▉         | 2/22 [00:00<00:01, 16.74it/s]\u001b[A\n",
      "Epoch 154/300:  18%|█▊        | 4/22 [00:00<00:01, 16.97it/s]\u001b[A\n",
      "Epoch 154/300:  27%|██▋       | 6/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 154/300:  36%|███▋      | 8/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 154/300:  45%|████▌     | 10/22 [00:00<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 154/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 154/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 154/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 154/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 154/300:  91%|█████████ | 20/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 154/300: 100%|██████████| 22/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      " 51%|█████▏    | 154/300 [04:01<03:48,  1.56s/it]             \u001b[A\n",
      "Epoch 155/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 155/300:   9%|▉         | 2/22 [00:00<00:01, 16.86it/s]\u001b[A\n",
      "Epoch 155/300:  18%|█▊        | 4/22 [00:00<00:01, 15.59it/s]\u001b[A\n",
      "Epoch 155/300:  27%|██▋       | 6/22 [00:00<00:01, 15.56it/s]\u001b[A\n",
      "Epoch 155/300:  36%|███▋      | 8/22 [00:00<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 155/300:  45%|████▌     | 10/22 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 155/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 155/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 155/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 155/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 155/300:  91%|█████████ | 20/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 155/300: 100%|██████████| 22/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      " 52%|█████▏    | 155/300 [04:03<03:46,  1.56s/it]             \u001b[A\n",
      "Epoch 156/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 156/300:   9%|▉         | 2/22 [00:00<00:01, 15.70it/s]\u001b[A\n",
      "Epoch 156/300:  18%|█▊        | 4/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 156/300:  27%|██▋       | 6/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 156/300:  36%|███▋      | 8/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 156/300:  45%|████▌     | 10/22 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 156/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 156/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 156/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 156/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 156/300:  91%|█████████ | 20/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 156/300: 100%|██████████| 22/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      " 52%|█████▏    | 156/300 [04:04<03:45,  1.56s/it]             \u001b[A\n",
      "Epoch 157/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 157/300:   9%|▉         | 2/22 [00:00<00:01, 15.19it/s]\u001b[A\n",
      "Epoch 157/300:  18%|█▊        | 4/22 [00:00<00:01, 15.67it/s]\u001b[A\n",
      "Epoch 157/300:  27%|██▋       | 6/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 157/300:  36%|███▋      | 8/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 157/300:  45%|████▌     | 10/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 157/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 157/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 157/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 157/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 157/300:  91%|█████████ | 20/22 [00:01<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 157/300: 100%|██████████| 22/22 [00:01<00:00, 15.69it/s]\u001b[A\n",
      " 52%|█████▏    | 157/300 [04:06<03:43,  1.57s/it]             \u001b[A\n",
      "Epoch 158/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 158/300:   9%|▉         | 2/22 [00:00<00:01, 16.39it/s]\u001b[A\n",
      "Epoch 158/300:  18%|█▊        | 4/22 [00:00<00:01, 15.68it/s]\u001b[A\n",
      "Epoch 158/300:  27%|██▋       | 6/22 [00:00<00:01, 15.27it/s]\u001b[A\n",
      "Epoch 158/300:  36%|███▋      | 8/22 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "Epoch 158/300:  45%|████▌     | 10/22 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "Epoch 158/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.67it/s]\u001b[A\n",
      "Epoch 158/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 158/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.62it/s]\u001b[A\n",
      "Epoch 158/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 158/300:  91%|█████████ | 20/22 [00:01<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 158/300: 100%|██████████| 22/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
      " 53%|█████▎    | 158/300 [04:07<03:43,  1.58s/it]             \u001b[A\n",
      "Epoch 159/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 159/300:   9%|▉         | 2/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 159/300:  18%|█▊        | 4/22 [00:00<00:01, 16.12it/s]\u001b[A\n",
      "Epoch 159/300:  27%|██▋       | 6/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 159/300:  36%|███▋      | 8/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 159/300:  45%|████▌     | 10/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 159/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 159/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 159/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 159/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 159/300:  91%|█████████ | 20/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 159/300: 100%|██████████| 22/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      " 53%|█████▎    | 159/300 [04:09<03:41,  1.57s/it]             \u001b[A\n",
      "Epoch 160/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 160/300:   9%|▉         | 2/22 [00:00<00:01, 15.08it/s]\u001b[A\n",
      "Epoch 160/300:  18%|█▊        | 4/22 [00:00<00:01, 15.25it/s]\u001b[A\n",
      "Epoch 160/300:  27%|██▋       | 6/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 160/300:  36%|███▋      | 8/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 160/300:  45%|████▌     | 10/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 160/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 160/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 160/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.39it/s]\u001b[A\n",
      "Epoch 160/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.35it/s]\u001b[A\n",
      "Epoch 160/300:  91%|█████████ | 20/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 160/300: 100%|██████████| 22/22 [00:01<00:00, 15.83it/s]\u001b[A\n",
      " 53%|█████▎    | 160/300 [04:11<03:40,  1.58s/it]             \u001b[A\n",
      "Epoch 161/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 161/300:   9%|▉         | 2/22 [00:00<00:01, 16.37it/s]\u001b[A\n",
      "Epoch 161/300:  18%|█▊        | 4/22 [00:00<00:01, 15.77it/s]\u001b[A\n",
      "Epoch 161/300:  27%|██▋       | 6/22 [00:00<00:01, 15.67it/s]\u001b[A\n",
      "Epoch 161/300:  36%|███▋      | 8/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 161/300:  45%|████▌     | 10/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 161/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 161/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 161/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 161/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 161/300:  91%|█████████ | 20/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 161/300: 100%|██████████| 22/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      " 54%|█████▎    | 161/300 [04:12<03:38,  1.57s/it]             \u001b[A\n",
      "Epoch 162/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 162/300:   9%|▉         | 2/22 [00:00<00:01, 16.88it/s]\u001b[A\n",
      "Epoch 162/300:  18%|█▊        | 4/22 [00:00<00:01, 16.46it/s]\u001b[A\n",
      "Epoch 162/300:  27%|██▋       | 6/22 [00:00<00:00, 17.32it/s]\u001b[A\n",
      "Epoch 162/300:  36%|███▋      | 8/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 162/300:  45%|████▌     | 10/22 [00:00<00:00, 16.63it/s]\u001b[A\n",
      "Epoch 162/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 162/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 162/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 162/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 162/300:  91%|█████████ | 20/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 162/300: 100%|██████████| 22/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      " 54%|█████▍    | 162/300 [04:14<03:35,  1.56s/it]             \u001b[A\n",
      "Epoch 163/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 163/300:   9%|▉         | 2/22 [00:00<00:01, 17.07it/s]\u001b[A\n",
      "Epoch 163/300:  18%|█▊        | 4/22 [00:00<00:01, 16.50it/s]\u001b[A\n",
      "Epoch 163/300:  27%|██▋       | 6/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 163/300:  36%|███▋      | 8/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 163/300:  45%|████▌     | 10/22 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 163/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 163/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 163/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 163/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 163/300:  91%|█████████ | 20/22 [00:01<00:00, 15.42it/s]\u001b[A\n",
      "Epoch 163/300: 100%|██████████| 22/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      " 54%|█████▍    | 163/300 [04:15<03:33,  1.56s/it]             \u001b[A\n",
      "Epoch 164/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 164/300:   9%|▉         | 2/22 [00:00<00:01, 16.66it/s]\u001b[A\n",
      "Epoch 164/300:  18%|█▊        | 4/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 164/300:  27%|██▋       | 6/22 [00:00<00:01, 15.31it/s]\u001b[A\n",
      "Epoch 164/300:  36%|███▋      | 8/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 164/300:  45%|████▌     | 10/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 164/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 164/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 164/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 164/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 164/300:  91%|█████████ | 20/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 164/300: 100%|██████████| 22/22 [00:01<00:00, 16.15it/s]\u001b[A\n",
      " 55%|█████▍    | 164/300 [04:17<03:32,  1.56s/it]             \u001b[A\n",
      "Epoch 165/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 165/300:   9%|▉         | 2/22 [00:00<00:01, 16.50it/s]\u001b[A\n",
      "Epoch 165/300:  18%|█▊        | 4/22 [00:00<00:01, 16.01it/s]\u001b[A\n",
      "Epoch 165/300:  27%|██▋       | 6/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 165/300:  36%|███▋      | 8/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 165/300:  45%|████▌     | 10/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 165/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 165/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 165/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 165/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.48it/s]\u001b[A\n",
      "Epoch 165/300:  91%|█████████ | 20/22 [00:01<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 165/300: 100%|██████████| 22/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      " 55%|█████▌    | 165/300 [04:18<03:30,  1.56s/it]             \u001b[A\n",
      "Epoch 166/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 166/300:   9%|▉         | 2/22 [00:00<00:01, 17.46it/s]\u001b[A\n",
      "Epoch 166/300:  18%|█▊        | 4/22 [00:00<00:01, 16.27it/s]\u001b[A\n",
      "Epoch 166/300:  27%|██▋       | 6/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 166/300:  36%|███▋      | 8/22 [00:00<00:00, 15.91it/s]\u001b[A\n",
      "Epoch 166/300:  45%|████▌     | 10/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 166/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 166/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.39it/s]\u001b[A\n",
      "Epoch 166/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.13it/s]\u001b[A\n",
      "Epoch 166/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.15it/s]\u001b[A\n",
      "Epoch 166/300:  91%|█████████ | 20/22 [00:01<00:00, 15.42it/s]\u001b[A\n",
      "Epoch 166/300: 100%|██████████| 22/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      " 55%|█████▌    | 166/300 [04:20<03:29,  1.57s/it]             \u001b[A\n",
      "Epoch 167/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 167/300:   9%|▉         | 2/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 167/300:  18%|█▊        | 4/22 [00:00<00:01, 15.43it/s]\u001b[A\n",
      "Epoch 167/300:  27%|██▋       | 6/22 [00:00<00:01, 15.94it/s]\u001b[A\n",
      "Epoch 167/300:  36%|███▋      | 8/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 167/300:  45%|████▌     | 10/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 167/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 167/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 167/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 167/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 167/300:  91%|█████████ | 20/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 167/300: 100%|██████████| 22/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      " 56%|█████▌    | 167/300 [04:21<03:28,  1.57s/it]             \u001b[A\n",
      "Epoch 168/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 168/300:   9%|▉         | 2/22 [00:00<00:01, 15.31it/s]\u001b[A\n",
      "Epoch 168/300:  18%|█▊        | 4/22 [00:00<00:01, 16.12it/s]\u001b[A\n",
      "Epoch 168/300:  27%|██▋       | 6/22 [00:00<00:01, 15.89it/s]\u001b[A\n",
      "Epoch 168/300:  36%|███▋      | 8/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 168/300:  45%|████▌     | 10/22 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 168/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 168/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 168/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 168/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 168/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 168/300: 100%|██████████| 22/22 [00:01<00:00, 16.15it/s]\u001b[A\n",
      " 56%|█████▌    | 168/300 [04:23<03:26,  1.56s/it]             \u001b[A\n",
      "Epoch 169/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 169/300:   9%|▉         | 2/22 [00:00<00:01, 16.42it/s]\u001b[A\n",
      "Epoch 169/300:  18%|█▊        | 4/22 [00:00<00:01, 17.07it/s]\u001b[A\n",
      "Epoch 169/300:  27%|██▋       | 6/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 169/300:  36%|███▋      | 8/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 169/300:  45%|████▌     | 10/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 169/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 169/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 169/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 169/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 169/300:  91%|█████████ | 20/22 [00:01<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 169/300: 100%|██████████| 22/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      " 56%|█████▋    | 169/300 [04:25<03:23,  1.56s/it]             \u001b[A\n",
      "Epoch 170/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 170/300:   9%|▉         | 2/22 [00:00<00:01, 15.69it/s]\u001b[A\n",
      "Epoch 170/300:  18%|█▊        | 4/22 [00:00<00:01, 15.13it/s]\u001b[A\n",
      "Epoch 170/300:  27%|██▋       | 6/22 [00:00<00:01, 15.57it/s]\u001b[A\n",
      "Epoch 170/300:  36%|███▋      | 8/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 170/300:  45%|████▌     | 10/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 170/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 170/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "Epoch 170/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 170/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 170/300:  91%|█████████ | 20/22 [00:01<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 170/300: 100%|██████████| 22/22 [00:01<00:00, 15.64it/s]\u001b[A\n",
      " 57%|█████▋    | 170/300 [04:26<03:23,  1.56s/it]             \u001b[A\n",
      "Epoch 171/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 171/300:   9%|▉         | 2/22 [00:00<00:01, 16.01it/s]\u001b[A\n",
      "Epoch 171/300:  18%|█▊        | 4/22 [00:00<00:01, 15.98it/s]\u001b[A\n",
      "Epoch 171/300:  27%|██▋       | 6/22 [00:00<00:01, 15.46it/s]\u001b[A\n",
      "Epoch 171/300:  36%|███▋      | 8/22 [00:00<00:00, 15.48it/s]\u001b[A\n",
      "Epoch 171/300:  45%|████▌     | 10/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 171/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 171/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 171/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 171/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 171/300:  91%|█████████ | 20/22 [00:01<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 171/300: 100%|██████████| 22/22 [00:01<00:00, 15.67it/s]\u001b[A\n",
      " 57%|█████▋    | 171/300 [04:28<03:22,  1.57s/it]             \u001b[A\n",
      "Epoch 172/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 172/300:   9%|▉         | 2/22 [00:00<00:01, 15.83it/s]\u001b[A\n",
      "Epoch 172/300:  18%|█▊        | 4/22 [00:00<00:01, 15.54it/s]\u001b[A\n",
      "Epoch 172/300:  27%|██▋       | 6/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 172/300:  36%|███▋      | 8/22 [00:00<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 172/300:  45%|████▌     | 10/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 172/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "Epoch 172/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 172/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 172/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 172/300:  91%|█████████ | 20/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 172/300: 100%|██████████| 22/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      " 57%|█████▋    | 172/300 [04:29<03:20,  1.57s/it]             \u001b[A\n",
      "Epoch 173/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 173/300:   9%|▉         | 2/22 [00:00<00:01, 15.38it/s]\u001b[A\n",
      "Epoch 173/300:  18%|█▊        | 4/22 [00:00<00:01, 15.59it/s]\u001b[A\n",
      "Epoch 173/300:  27%|██▋       | 6/22 [00:00<00:01, 15.55it/s]\u001b[A\n",
      "Epoch 173/300:  36%|███▋      | 8/22 [00:00<00:00, 15.67it/s]\u001b[A\n",
      "Epoch 173/300:  45%|████▌     | 10/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 173/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 173/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 173/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 173/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 173/300:  91%|█████████ | 20/22 [00:01<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 173/300: 100%|██████████| 22/22 [00:01<00:00, 16.21it/s]\u001b[A\n",
      " 58%|█████▊    | 173/300 [04:31<03:19,  1.57s/it]             \u001b[A\n",
      "Epoch 174/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 174/300:   9%|▉         | 2/22 [00:00<00:01, 15.61it/s]\u001b[A\n",
      "Epoch 174/300:  18%|█▊        | 4/22 [00:00<00:01, 15.97it/s]\u001b[A\n",
      "Epoch 174/300:  27%|██▋       | 6/22 [00:00<00:01, 15.67it/s]\u001b[A\n",
      "Epoch 174/300:  36%|███▋      | 8/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 174/300:  45%|████▌     | 10/22 [00:00<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 174/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 174/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 174/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 174/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 174/300:  91%|█████████ | 20/22 [00:01<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 174/300: 100%|██████████| 22/22 [00:01<00:00, 16.88it/s]\u001b[A\n",
      " 58%|█████▊    | 174/300 [04:32<03:16,  1.56s/it]             \u001b[A\n",
      "Epoch 175/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 175/300:   9%|▉         | 2/22 [00:00<00:01, 16.11it/s]\u001b[A\n",
      "Epoch 175/300:  18%|█▊        | 4/22 [00:00<00:01, 15.40it/s]\u001b[A\n",
      "Epoch 175/300:  27%|██▋       | 6/22 [00:00<00:01, 15.54it/s]\u001b[A\n",
      "Epoch 175/300:  36%|███▋      | 8/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 175/300:  45%|████▌     | 10/22 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 175/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 175/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.28it/s]\u001b[A\n",
      "Epoch 175/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 175/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 175/300:  91%|█████████ | 20/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 175/300: 100%|██████████| 22/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      " 58%|█████▊    | 175/300 [04:34<03:15,  1.57s/it]             \u001b[A\n",
      "Epoch 176/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 176/300:   9%|▉         | 2/22 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "Epoch 176/300:  18%|█▊        | 4/22 [00:00<00:01, 16.08it/s]\u001b[A\n",
      "Epoch 176/300:  27%|██▋       | 6/22 [00:00<00:01, 15.77it/s]\u001b[A\n",
      "Epoch 176/300:  36%|███▋      | 8/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 176/300:  45%|████▌     | 10/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 176/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 176/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 176/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 176/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 176/300:  91%|█████████ | 20/22 [00:01<00:00, 16.23it/s]\u001b[A\n",
      "Epoch 176/300: 100%|██████████| 22/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      " 59%|█████▊    | 176/300 [04:36<03:13,  1.56s/it]             \u001b[A\n",
      "Epoch 177/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 177/300:   9%|▉         | 2/22 [00:00<00:01, 15.11it/s]\u001b[A\n",
      "Epoch 177/300:  18%|█▊        | 4/22 [00:00<00:01, 14.94it/s]\u001b[A\n",
      "Epoch 177/300:  27%|██▋       | 6/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 177/300:  36%|███▋      | 8/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 177/300:  45%|████▌     | 10/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 177/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 177/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 177/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 177/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 177/300:  91%|█████████ | 20/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 177/300: 100%|██████████| 22/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      " 59%|█████▉    | 177/300 [04:37<03:12,  1.56s/it]             \u001b[A\n",
      "Epoch 178/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 178/300:   9%|▉         | 2/22 [00:00<00:01, 16.03it/s]\u001b[A\n",
      "Epoch 178/300:  18%|█▊        | 4/22 [00:00<00:01, 15.70it/s]\u001b[A\n",
      "Epoch 178/300:  27%|██▋       | 6/22 [00:00<00:01, 15.48it/s]\u001b[A\n",
      "Epoch 178/300:  36%|███▋      | 8/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 178/300:  45%|████▌     | 10/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 178/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "Epoch 178/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.91it/s]\u001b[A\n",
      "Epoch 178/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 178/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 178/300:  91%|█████████ | 20/22 [00:01<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 178/300: 100%|██████████| 22/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      " 59%|█████▉    | 178/300 [04:39<03:10,  1.56s/it]             \u001b[A\n",
      "Epoch 179/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 179/300:   9%|▉         | 2/22 [00:00<00:01, 15.87it/s]\u001b[A\n",
      "Epoch 179/300:  18%|█▊        | 4/22 [00:00<00:01, 15.95it/s]\u001b[A\n",
      "Epoch 179/300:  27%|██▋       | 6/22 [00:00<00:01, 15.50it/s]\u001b[A\n",
      "Epoch 179/300:  36%|███▋      | 8/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 179/300:  45%|████▌     | 10/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 179/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 179/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 179/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 179/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 179/300:  91%|█████████ | 20/22 [00:01<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 179/300: 100%|██████████| 22/22 [00:01<00:00, 16.21it/s]\u001b[A\n",
      " 60%|█████▉    | 179/300 [04:40<03:08,  1.56s/it]             \u001b[A\n",
      "Epoch 180/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 180/300:   9%|▉         | 2/22 [00:00<00:01, 15.79it/s]\u001b[A\n",
      "Epoch 180/300:  18%|█▊        | 4/22 [00:00<00:01, 15.81it/s]\u001b[A\n",
      "Epoch 180/300:  27%|██▋       | 6/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 180/300:  36%|███▋      | 8/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 180/300:  45%|████▌     | 10/22 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 180/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.38it/s]\u001b[A\n",
      "Epoch 180/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.50it/s]\u001b[A\n",
      "Epoch 180/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.40it/s]\u001b[A\n",
      "Epoch 180/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 180/300:  91%|█████████ | 20/22 [00:01<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 180/300: 100%|██████████| 22/22 [00:01<00:00, 15.67it/s]\u001b[A\n",
      " 60%|██████    | 180/300 [04:42<03:08,  1.57s/it]             \u001b[A\n",
      "Epoch 181/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 181/300:   9%|▉         | 2/22 [00:00<00:01, 15.82it/s]\u001b[A\n",
      "Epoch 181/300:  18%|█▊        | 4/22 [00:00<00:01, 16.03it/s]\u001b[A\n",
      "Epoch 181/300:  27%|██▋       | 6/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 181/300:  36%|███▋      | 8/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 181/300:  45%|████▌     | 10/22 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "Epoch 181/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 181/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 181/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 181/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 181/300:  91%|█████████ | 20/22 [00:01<00:00, 15.91it/s]\u001b[A\n",
      "Epoch 181/300: 100%|██████████| 22/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      " 60%|██████    | 181/300 [04:43<03:06,  1.57s/it]             \u001b[A\n",
      "Epoch 182/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 182/300:   9%|▉         | 2/22 [00:00<00:01, 15.80it/s]\u001b[A\n",
      "Epoch 182/300:  18%|█▊        | 4/22 [00:00<00:01, 15.87it/s]\u001b[A\n",
      "Epoch 182/300:  27%|██▋       | 6/22 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "Epoch 182/300:  36%|███▋      | 8/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 182/300:  45%|████▌     | 10/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 182/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 182/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 182/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 182/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 182/300:  91%|█████████ | 20/22 [00:01<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 182/300: 100%|██████████| 22/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      " 61%|██████    | 182/300 [04:45<03:04,  1.57s/it]             \u001b[A\n",
      "Epoch 183/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 183/300:   9%|▉         | 2/22 [00:00<00:01, 16.16it/s]\u001b[A\n",
      "Epoch 183/300:  18%|█▊        | 4/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 183/300:  27%|██▋       | 6/22 [00:00<00:01, 15.94it/s]\u001b[A\n",
      "Epoch 183/300:  36%|███▋      | 8/22 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 183/300:  45%|████▌     | 10/22 [00:00<00:00, 15.53it/s]\u001b[A\n",
      "Epoch 183/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 183/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 183/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.59it/s]\u001b[A\n",
      "Epoch 183/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.54it/s]\u001b[A\n",
      "Epoch 183/300:  91%|█████████ | 20/22 [00:01<00:00, 15.43it/s]\u001b[A\n",
      "Epoch 183/300: 100%|██████████| 22/22 [00:01<00:00, 15.78it/s]\u001b[A\n",
      " 61%|██████    | 183/300 [04:46<03:03,  1.57s/it]             \u001b[A\n",
      "Epoch 184/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 184/300:   9%|▉         | 2/22 [00:00<00:01, 16.48it/s]\u001b[A\n",
      "Epoch 184/300:  18%|█▊        | 4/22 [00:00<00:01, 15.97it/s]\u001b[A\n",
      "Epoch 184/300:  27%|██▋       | 6/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 184/300:  36%|███▋      | 8/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 184/300:  45%|████▌     | 10/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 184/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 184/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 184/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.54it/s]\u001b[A\n",
      "Epoch 184/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 184/300:  91%|█████████ | 20/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 184/300: 100%|██████████| 22/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      " 61%|██████▏   | 184/300 [04:48<03:02,  1.57s/it]             \u001b[A\n",
      "Epoch 185/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 185/300:   9%|▉         | 2/22 [00:00<00:01, 16.11it/s]\u001b[A\n",
      "Epoch 185/300:  18%|█▊        | 4/22 [00:00<00:01, 17.22it/s]\u001b[A\n",
      "Epoch 185/300:  27%|██▋       | 6/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 185/300:  36%|███▋      | 8/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 185/300:  45%|████▌     | 10/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 185/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 185/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 185/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 185/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 185/300:  91%|█████████ | 20/22 [00:01<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 185/300: 100%|██████████| 22/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      " 62%|██████▏   | 185/300 [04:50<02:59,  1.56s/it]             \u001b[A\n",
      "Epoch 186/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 186/300:   9%|▉         | 2/22 [00:00<00:01, 15.57it/s]\u001b[A\n",
      "Epoch 186/300:  18%|█▊        | 4/22 [00:00<00:01, 15.62it/s]\u001b[A\n",
      "Epoch 186/300:  27%|██▋       | 6/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 186/300:  36%|███▋      | 8/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 186/300:  45%|████▌     | 10/22 [00:00<00:00, 15.39it/s]\u001b[A\n",
      "Epoch 186/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 186/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 186/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 186/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 186/300:  91%|█████████ | 20/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 186/300: 100%|██████████| 22/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      " 62%|██████▏   | 186/300 [04:51<02:58,  1.57s/it]             \u001b[A\n",
      "Epoch 187/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 187/300:   9%|▉         | 2/22 [00:00<00:01, 15.64it/s]\u001b[A\n",
      "Epoch 187/300:  18%|█▊        | 4/22 [00:00<00:01, 15.81it/s]\u001b[A\n",
      "Epoch 187/300:  27%|██▋       | 6/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 187/300:  36%|███▋      | 8/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 187/300:  45%|████▌     | 10/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 187/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 187/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "Epoch 187/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 187/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 187/300:  91%|█████████ | 20/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 187/300: 100%|██████████| 22/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      " 62%|██████▏   | 187/300 [04:53<02:57,  1.57s/it]             \u001b[A\n",
      "Epoch 188/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 188/300:   9%|▉         | 2/22 [00:00<00:01, 15.75it/s]\u001b[A\n",
      "Epoch 188/300:  18%|█▊        | 4/22 [00:00<00:01, 15.40it/s]\u001b[A\n",
      "Epoch 188/300:  27%|██▋       | 6/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 188/300:  36%|███▋      | 8/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 188/300:  45%|████▌     | 10/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 188/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 188/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 188/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 188/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 188/300:  91%|█████████ | 20/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 188/300: 100%|██████████| 22/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
      " 63%|██████▎   | 188/300 [04:54<02:55,  1.57s/it]             \u001b[A\n",
      "Epoch 189/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 189/300:   9%|▉         | 2/22 [00:00<00:01, 15.21it/s]\u001b[A\n",
      "Epoch 189/300:  18%|█▊        | 4/22 [00:00<00:01, 15.90it/s]\u001b[A\n",
      "Epoch 189/300:  27%|██▋       | 6/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 189/300:  36%|███▋      | 8/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 189/300:  45%|████▌     | 10/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 189/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 189/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 189/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 189/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 189/300:  91%|█████████ | 20/22 [00:01<00:00, 16.36it/s]\u001b[A\n",
      "Epoch 189/300: 100%|██████████| 22/22 [00:01<00:00, 16.43it/s]\u001b[A\n",
      " 63%|██████▎   | 189/300 [04:56<02:53,  1.56s/it]             \u001b[A\n",
      "Epoch 190/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 190/300:   9%|▉         | 2/22 [00:00<00:01, 15.05it/s]\u001b[A\n",
      "Epoch 190/300:  18%|█▊        | 4/22 [00:00<00:01, 15.00it/s]\u001b[A\n",
      "Epoch 190/300:  27%|██▋       | 6/22 [00:00<00:01, 15.27it/s]\u001b[A\n",
      "Epoch 190/300:  36%|███▋      | 8/22 [00:00<00:00, 15.27it/s]\u001b[A\n",
      "Epoch 190/300:  45%|████▌     | 10/22 [00:00<00:00, 15.38it/s]\u001b[A\n",
      "Epoch 190/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 190/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 190/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 190/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 190/300:  91%|█████████ | 20/22 [00:01<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 190/300: 100%|██████████| 22/22 [00:01<00:00, 16.28it/s]\u001b[A\n",
      " 63%|██████▎   | 190/300 [04:57<02:52,  1.57s/it]             \u001b[A\n",
      "Epoch 191/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 191/300:   9%|▉         | 2/22 [00:00<00:01, 15.75it/s]\u001b[A\n",
      "Epoch 191/300:  18%|█▊        | 4/22 [00:00<00:01, 15.19it/s]\u001b[A\n",
      "Epoch 191/300:  27%|██▋       | 6/22 [00:00<00:01, 15.87it/s]\u001b[A\n",
      "Epoch 191/300:  36%|███▋      | 8/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 191/300:  45%|████▌     | 10/22 [00:00<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 191/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 191/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 191/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 191/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.50it/s]\u001b[A\n",
      "Epoch 191/300:  91%|█████████ | 20/22 [00:01<00:00, 15.55it/s]\u001b[A\n",
      "Epoch 191/300: 100%|██████████| 22/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
      " 64%|██████▎   | 191/300 [04:59<02:51,  1.57s/it]             \u001b[A\n",
      "Epoch 192/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 192/300:   9%|▉         | 2/22 [00:00<00:01, 16.03it/s]\u001b[A\n",
      "Epoch 192/300:  18%|█▊        | 4/22 [00:00<00:01, 15.83it/s]\u001b[A\n",
      "Epoch 192/300:  27%|██▋       | 6/22 [00:00<00:01, 15.44it/s]\u001b[A\n",
      "Epoch 192/300:  36%|███▋      | 8/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 192/300:  45%|████▌     | 10/22 [00:00<00:00, 15.36it/s]\u001b[A\n",
      "Epoch 192/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "Epoch 192/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 192/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 192/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 192/300:  91%|█████████ | 20/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 192/300: 100%|██████████| 22/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
      " 64%|██████▍   | 192/300 [05:01<02:50,  1.58s/it]             \u001b[A\n",
      "Epoch 193/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 193/300:   9%|▉         | 2/22 [00:00<00:01, 15.49it/s]\u001b[A\n",
      "Epoch 193/300:  18%|█▊        | 4/22 [00:00<00:01, 15.88it/s]\u001b[A\n",
      "Epoch 193/300:  27%|██▋       | 6/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 193/300:  36%|███▋      | 8/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 193/300:  45%|████▌     | 10/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 193/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 193/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 193/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.38it/s]\u001b[A\n",
      "Epoch 193/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 193/300:  91%|█████████ | 20/22 [00:01<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 193/300: 100%|██████████| 22/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      " 64%|██████▍   | 193/300 [05:02<02:48,  1.57s/it]             \u001b[A\n",
      "Epoch 194/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 194/300:   9%|▉         | 2/22 [00:00<00:01, 14.87it/s]\u001b[A\n",
      "Epoch 194/300:  18%|█▊        | 4/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 194/300:  27%|██▋       | 6/22 [00:00<00:01, 15.42it/s]\u001b[A\n",
      "Epoch 194/300:  36%|███▋      | 8/22 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 194/300:  45%|████▌     | 10/22 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "Epoch 194/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.24it/s]\u001b[A\n",
      "Epoch 194/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 194/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 194/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 194/300:  91%|█████████ | 20/22 [00:01<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 194/300: 100%|██████████| 22/22 [00:01<00:00, 16.20it/s]\u001b[A\n",
      " 65%|██████▍   | 194/300 [05:04<02:46,  1.57s/it]             \u001b[A\n",
      "Epoch 195/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 195/300:   9%|▉         | 2/22 [00:00<00:01, 15.23it/s]\u001b[A\n",
      "Epoch 195/300:  18%|█▊        | 4/22 [00:00<00:01, 16.15it/s]\u001b[A\n",
      "Epoch 195/300:  27%|██▋       | 6/22 [00:00<00:00, 16.75it/s]\u001b[A\n",
      "Epoch 195/300:  36%|███▋      | 8/22 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 195/300:  45%|████▌     | 10/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 195/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 195/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 195/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 195/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 195/300:  91%|█████████ | 20/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 195/300: 100%|██████████| 22/22 [00:01<00:00, 15.98it/s]\u001b[A\n",
      " 65%|██████▌   | 195/300 [05:05<02:44,  1.57s/it]             \u001b[A\n",
      "Epoch 196/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 196/300:   9%|▉         | 2/22 [00:00<00:01, 15.92it/s]\u001b[A\n",
      "Epoch 196/300:  18%|█▊        | 4/22 [00:00<00:01, 16.03it/s]\u001b[A\n",
      "Epoch 196/300:  27%|██▋       | 6/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 196/300:  36%|███▋      | 8/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 196/300:  45%|████▌     | 10/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 196/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 196/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 196/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 196/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 196/300:  91%|█████████ | 20/22 [00:01<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 196/300: 100%|██████████| 22/22 [00:01<00:00, 15.85it/s]\u001b[A\n",
      " 65%|██████▌   | 196/300 [05:07<02:43,  1.57s/it]             \u001b[A\n",
      "Epoch 197/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 197/300:   9%|▉         | 2/22 [00:00<00:01, 16.91it/s]\u001b[A\n",
      "Epoch 197/300:  18%|█▊        | 4/22 [00:00<00:01, 16.04it/s]\u001b[A\n",
      "Epoch 197/300:  27%|██▋       | 6/22 [00:00<00:01, 15.81it/s]\u001b[A\n",
      "Epoch 197/300:  36%|███▋      | 8/22 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 197/300:  45%|████▌     | 10/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 197/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Epoch 197/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 197/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 197/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 197/300:  91%|█████████ | 20/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 197/300: 100%|██████████| 22/22 [00:01<00:00, 16.20it/s]\u001b[A\n",
      " 66%|██████▌   | 197/300 [05:08<02:41,  1.57s/it]             \u001b[A\n",
      "Epoch 198/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 198/300:   9%|▉         | 2/22 [00:00<00:01, 16.16it/s]\u001b[A\n",
      "Epoch 198/300:  18%|█▊        | 4/22 [00:00<00:01, 15.58it/s]\u001b[A\n",
      "Epoch 198/300:  27%|██▋       | 6/22 [00:00<00:01, 15.56it/s]\u001b[A\n",
      "Epoch 198/300:  36%|███▋      | 8/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 198/300:  45%|████▌     | 10/22 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "Epoch 198/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 198/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 198/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 198/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 198/300:  91%|█████████ | 20/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 198/300: 100%|██████████| 22/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      " 66%|██████▌   | 198/300 [05:10<02:39,  1.56s/it]             \u001b[A\n",
      "Epoch 199/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 199/300:   9%|▉         | 2/22 [00:00<00:01, 16.44it/s]\u001b[A\n",
      "Epoch 199/300:  18%|█▊        | 4/22 [00:00<00:01, 15.96it/s]\u001b[A\n",
      "Epoch 199/300:  27%|██▋       | 6/22 [00:00<00:01, 15.83it/s]\u001b[A\n",
      "Epoch 199/300:  36%|███▋      | 8/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 199/300:  45%|████▌     | 10/22 [00:00<00:00, 15.66it/s]\u001b[A\n",
      "Epoch 199/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 199/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 199/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 199/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 199/300:  91%|█████████ | 20/22 [00:01<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 199/300: 100%|██████████| 22/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      " 66%|██████▋   | 199/300 [05:12<02:37,  1.56s/it]             \u001b[A\n",
      "Epoch 200/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 200/300:   9%|▉         | 2/22 [00:00<00:01, 15.74it/s]\u001b[A\n",
      "Epoch 200/300:  18%|█▊        | 4/22 [00:00<00:01, 15.60it/s]\u001b[A\n",
      "Epoch 200/300:  27%|██▋       | 6/22 [00:00<00:01, 15.90it/s]\u001b[A\n",
      "Epoch 200/300:  36%|███▋      | 8/22 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 200/300:  45%|████▌     | 10/22 [00:00<00:00, 15.33it/s]\u001b[A\n",
      "Epoch 200/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 200/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 200/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 200/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.60it/s]\u001b[A\n",
      "Epoch 200/300:  91%|█████████ | 20/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 200/300: 100%|██████████| 22/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      " 67%|██████▋   | 200/300 [05:13<02:36,  1.56s/it]             \u001b[A\n",
      "Epoch 201/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 201/300:   9%|▉         | 2/22 [00:00<00:01, 15.65it/s]\u001b[A\n",
      "Epoch 201/300:  18%|█▊        | 4/22 [00:00<00:01, 15.60it/s]\u001b[A\n",
      "Epoch 201/300:  27%|██▋       | 6/22 [00:00<00:01, 15.66it/s]\u001b[A\n",
      "Epoch 201/300:  36%|███▋      | 8/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 201/300:  45%|████▌     | 10/22 [00:00<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 201/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 201/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 201/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 201/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 201/300:  91%|█████████ | 20/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 201/300: 100%|██████████| 22/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      " 67%|██████▋   | 201/300 [05:15<02:34,  1.56s/it]             \u001b[A\n",
      "Epoch 202/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 202/300:   9%|▉         | 2/22 [00:00<00:01, 15.30it/s]\u001b[A\n",
      "Epoch 202/300:  18%|█▊        | 4/22 [00:00<00:01, 15.45it/s]\u001b[A\n",
      "Epoch 202/300:  27%|██▋       | 6/22 [00:00<00:01, 15.75it/s]\u001b[A\n",
      "Epoch 202/300:  36%|███▋      | 8/22 [00:00<00:00, 16.01it/s]\u001b[A\n",
      "Epoch 202/300:  45%|████▌     | 10/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 202/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.85it/s]\u001b[A\n",
      "Epoch 202/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 202/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 202/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 202/300:  91%|█████████ | 20/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 202/300: 100%|██████████| 22/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      " 67%|██████▋   | 202/300 [05:16<02:32,  1.56s/it]             \u001b[A\n",
      "Epoch 203/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 203/300:   9%|▉         | 2/22 [00:00<00:01, 16.80it/s]\u001b[A\n",
      "Epoch 203/300:  18%|█▊        | 4/22 [00:00<00:01, 16.10it/s]\u001b[A\n",
      "Epoch 203/300:  27%|██▋       | 6/22 [00:00<00:01, 15.59it/s]\u001b[AIOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      "Epoch 268/300:   9%|▉         | 2/22 [00:00<00:01, 15.56it/s]\u001b[A\n",
      "Epoch 268/300:  18%|█▊        | 4/22 [00:00<00:01, 15.06it/s]\u001b[A\n",
      "Epoch 268/300:  27%|██▋       | 6/22 [00:00<00:01, 14.78it/s]\u001b[A\n",
      "Epoch 268/300:  36%|███▋      | 8/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Epoch 268/300:  45%|████▌     | 10/22 [00:00<00:00, 15.48it/s]\u001b[A\n",
      "Epoch 268/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.51it/s]\u001b[A\n",
      "Epoch 268/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.23it/s]\u001b[A\n",
      "Epoch 268/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.25it/s]\u001b[A\n",
      "Epoch 268/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 268/300:  91%|█████████ | 20/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 268/300: 100%|██████████| 22/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      " 89%|████████▉ | 268/300 [07:00<00:51,  1.62s/it]             \u001b[A\n",
      "Epoch 269/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 269/300:   9%|▉         | 2/22 [00:00<00:01, 16.49it/s]\u001b[A\n",
      "Epoch 269/300:  18%|█▊        | 4/22 [00:00<00:01, 16.02it/s]\u001b[A\n",
      "Epoch 269/300:  27%|██▋       | 6/22 [00:00<00:01, 15.76it/s]\u001b[A\n",
      "Epoch 269/300:  36%|███▋      | 8/22 [00:00<00:00, 15.67it/s]\u001b[A\n",
      "Epoch 269/300:  45%|████▌     | 10/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 269/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 269/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.41it/s]\u001b[A\n",
      "Epoch 269/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 269/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.36it/s]\u001b[A\n",
      "Epoch 269/300:  91%|█████████ | 20/22 [00:01<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 269/300: 100%|██████████| 22/22 [00:01<00:00, 15.68it/s]\u001b[A\n",
      " 90%|████████▉ | 269/300 [07:02<00:49,  1.61s/it]             \u001b[A\n",
      "Epoch 270/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 270/300:   9%|▉         | 2/22 [00:00<00:01, 14.71it/s]\u001b[A\n",
      "Epoch 270/300:  18%|█▊        | 4/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 270/300:  27%|██▋       | 6/22 [00:00<00:01, 15.78it/s]\u001b[A\n",
      "Epoch 270/300:  36%|███▋      | 8/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 270/300:  45%|████▌     | 10/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 270/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 270/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 270/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 270/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 270/300:  91%|█████████ | 20/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 270/300: 100%|██████████| 22/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      " 90%|█████████ | 270/300 [07:03<00:47,  1.60s/it]             \u001b[A\n",
      "Epoch 271/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 271/300:   9%|▉         | 2/22 [00:00<00:01, 16.44it/s]\u001b[A\n",
      "Epoch 271/300:  18%|█▊        | 4/22 [00:00<00:01, 15.63it/s]\u001b[A\n",
      "Epoch 271/300:  27%|██▋       | 6/22 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 271/300:  36%|███▋      | 8/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 271/300:  45%|████▌     | 10/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 271/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 271/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.56it/s]\u001b[A\n",
      "Epoch 271/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 271/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 271/300:  91%|█████████ | 20/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 271/300: 100%|██████████| 22/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      " 90%|█████████ | 271/300 [07:05<00:46,  1.59s/it]             \u001b[A\n",
      "Epoch 272/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 272/300:   9%|▉         | 2/22 [00:00<00:01, 16.36it/s]\u001b[A\n",
      "Epoch 272/300:  18%|█▊        | 4/22 [00:00<00:01, 16.68it/s]\u001b[A\n",
      "Epoch 272/300:  27%|██▋       | 6/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 272/300:  36%|███▋      | 8/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 272/300:  45%|████▌     | 10/22 [00:00<00:00, 15.71it/s]\u001b[A\n",
      "Epoch 272/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "Epoch 272/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "Epoch 272/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.36it/s]\u001b[A\n",
      "Epoch 272/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 272/300:  91%|█████████ | 20/22 [00:01<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 272/300: 100%|██████████| 22/22 [00:01<00:00, 15.71it/s]\u001b[A\n",
      " 91%|█████████ | 272/300 [07:06<00:44,  1.59s/it]             \u001b[A\n",
      "Epoch 273/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 273/300:   9%|▉         | 2/22 [00:00<00:01, 16.76it/s]\u001b[A\n",
      "Epoch 273/300:  18%|█▊        | 4/22 [00:00<00:01, 16.07it/s]\u001b[A\n",
      "Epoch 273/300:  27%|██▋       | 6/22 [00:00<00:01, 15.44it/s]\u001b[A\n",
      "Epoch 273/300:  36%|███▋      | 8/22 [00:00<00:00, 15.46it/s]\u001b[A\n",
      "Epoch 273/300:  45%|████▌     | 10/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 273/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.50it/s]\u001b[A\n",
      "Epoch 273/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 273/300:  73%|███████▎  | 16/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 273/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 273/300:  91%|█████████ | 20/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 273/300: 100%|██████████| 22/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      " 91%|█████████ | 273/300 [07:08<00:42,  1.59s/it]             \u001b[A\n",
      "Epoch 274/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 274/300:   9%|▉         | 2/22 [00:00<00:01, 16.86it/s]\u001b[A\n",
      "Epoch 274/300:  18%|█▊        | 4/22 [00:00<00:01, 15.95it/s]\u001b[A\n",
      "Epoch 274/300:  27%|██▋       | 6/22 [00:00<00:01, 15.53it/s]\u001b[A\n",
      "Epoch 274/300:  36%|███▋      | 8/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 274/300:  45%|████▌     | 10/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 274/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 274/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 274/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 274/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 274/300:  91%|█████████ | 20/22 [00:01<00:00, 15.67it/s]\u001b[A\n",
      "Epoch 274/300: 100%|██████████| 22/22 [00:01<00:00, 15.79it/s]\u001b[A\n",
      " 91%|█████████▏| 274/300 [07:10<00:41,  1.59s/it]             \u001b[A\n",
      "Epoch 275/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 275/300:   9%|▉         | 2/22 [00:00<00:01, 17.11it/s]\u001b[A\n",
      "Epoch 275/300:  18%|█▊        | 4/22 [00:00<00:01, 16.64it/s]\u001b[A\n",
      "Epoch 275/300:  27%|██▋       | 6/22 [00:00<00:01, 15.90it/s]\u001b[A\n",
      "Epoch 275/300:  36%|███▋      | 8/22 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 275/300:  45%|████▌     | 10/22 [00:00<00:00, 15.54it/s]\u001b[A\n",
      "Epoch 275/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.65it/s]\u001b[A\n",
      "Epoch 275/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 275/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 275/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 275/300:  91%|█████████ | 20/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 275/300: 100%|██████████| 22/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      " 92%|█████████▏| 275/300 [07:11<00:39,  1.58s/it]             \u001b[A\n",
      "Epoch 276/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 276/300:   9%|▉         | 2/22 [00:00<00:01, 16.09it/s]\u001b[A\n",
      "Epoch 276/300:  18%|█▊        | 4/22 [00:00<00:01, 15.82it/s]\u001b[A\n",
      "Epoch 276/300:  27%|██▋       | 6/22 [00:00<00:01, 15.95it/s]\u001b[A\n",
      "Epoch 276/300:  36%|███▋      | 8/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 276/300:  45%|████▌     | 10/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 276/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 276/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 276/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 276/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 276/300:  91%|█████████ | 20/22 [00:01<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 276/300: 100%|██████████| 22/22 [00:01<00:00, 16.00it/s]\u001b[A\n",
      " 92%|█████████▏| 276/300 [07:13<00:37,  1.58s/it]             \u001b[A\n",
      "Epoch 277/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 277/300:   9%|▉         | 2/22 [00:00<00:01, 16.55it/s]\u001b[A\n",
      "Epoch 277/300:  18%|█▊        | 4/22 [00:00<00:01, 15.99it/s]\u001b[A\n",
      "Epoch 277/300:  27%|██▋       | 6/22 [00:00<00:01, 15.93it/s]\u001b[A\n",
      "Epoch 277/300:  36%|███▋      | 8/22 [00:00<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 277/300:  45%|████▌     | 10/22 [00:00<00:00, 15.47it/s]\u001b[A\n",
      "Epoch 277/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.34it/s]\u001b[A\n",
      "Epoch 277/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.63it/s]\u001b[A\n",
      "Epoch 277/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.42it/s]\u001b[A\n",
      "Epoch 277/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 277/300:  91%|█████████ | 20/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 277/300: 100%|██████████| 22/22 [00:01<00:00, 16.01it/s]\u001b[A\n",
      " 92%|█████████▏| 277/300 [07:14<00:36,  1.58s/it]             \u001b[A\n",
      "Epoch 278/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 278/300:   9%|▉         | 2/22 [00:00<00:01, 15.62it/s]\u001b[A\n",
      "Epoch 278/300:  18%|█▊        | 4/22 [00:00<00:01, 15.63it/s]\u001b[A\n",
      "Epoch 278/300:  27%|██▋       | 6/22 [00:00<00:01, 15.81it/s]\u001b[A\n",
      "Epoch 278/300:  36%|███▋      | 8/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 278/300:  45%|████▌     | 10/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 278/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 278/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 278/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.87it/s]\u001b[A\n",
      "Epoch 278/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 278/300:  91%|█████████ | 20/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 278/300: 100%|██████████| 22/22 [00:01<00:00, 15.76it/s]\u001b[A\n",
      " 93%|█████████▎| 278/300 [07:16<00:34,  1.58s/it]             \u001b[A\n",
      "Epoch 279/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 279/300:   9%|▉         | 2/22 [00:00<00:01, 15.12it/s]\u001b[A\n",
      "Epoch 279/300:  18%|█▊        | 4/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 279/300:  27%|██▋       | 6/22 [00:00<00:01, 15.77it/s]\u001b[A\n",
      "Epoch 279/300:  36%|███▋      | 8/22 [00:00<00:00, 15.92it/s]\u001b[A\n",
      "Epoch 279/300:  45%|████▌     | 10/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 279/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.78it/s]\u001b[A\n",
      "Epoch 279/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.57it/s]\u001b[A\n",
      "Epoch 279/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.70it/s]\u001b[A\n",
      "Epoch 279/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 279/300:  91%|█████████ | 20/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 279/300: 100%|██████████| 22/22 [00:01<00:00, 16.52it/s]\u001b[A\n",
      " 93%|█████████▎| 279/300 [07:18<00:33,  1.57s/it]             \u001b[A\n",
      "Epoch 280/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 280/300:   9%|▉         | 2/22 [00:00<00:01, 16.20it/s]\u001b[A\n",
      "Epoch 280/300:  18%|█▊        | 4/22 [00:00<00:01, 15.82it/s]\u001b[A\n",
      "Epoch 280/300:  27%|██▋       | 6/22 [00:00<00:01, 15.36it/s]\u001b[A\n",
      "Epoch 280/300:  36%|███▋      | 8/22 [00:00<00:00, 15.45it/s]\u001b[A\n",
      "Epoch 280/300:  45%|████▌     | 10/22 [00:00<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 280/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 280/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 280/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 280/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 280/300:  91%|█████████ | 20/22 [00:01<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 280/300: 100%|██████████| 22/22 [00:01<00:00, 16.13it/s]\u001b[A\n",
      " 93%|█████████▎| 280/300 [07:19<00:31,  1.57s/it]             \u001b[A\n",
      "Epoch 281/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 281/300:   9%|▉         | 2/22 [00:00<00:01, 16.85it/s]\u001b[A\n",
      "Epoch 281/300:  18%|█▊        | 4/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 281/300:  27%|██▋       | 6/22 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 281/300:  36%|███▋      | 8/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 281/300:  45%|████▌     | 10/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 281/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.44it/s]\u001b[A\n",
      "Epoch 281/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 281/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 281/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 281/300:  91%|█████████ | 20/22 [00:01<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 281/300: 100%|██████████| 22/22 [00:01<00:00, 15.83it/s]\u001b[A\n",
      " 94%|█████████▎| 281/300 [07:21<00:29,  1.57s/it]             \u001b[A\n",
      "Epoch 282/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 282/300:   9%|▉         | 2/22 [00:00<00:01, 15.29it/s]\u001b[A\n",
      "Epoch 282/300:  18%|█▊        | 4/22 [00:00<00:01, 15.37it/s]\u001b[A\n",
      "Epoch 282/300:  27%|██▋       | 6/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 282/300:  36%|███▋      | 8/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 282/300:  45%|████▌     | 10/22 [00:00<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 282/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.80it/s]\u001b[A\n",
      "Epoch 282/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 282/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.86it/s]\u001b[A\n",
      "Epoch 282/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 282/300:  91%|█████████ | 20/22 [00:01<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 282/300: 100%|██████████| 22/22 [00:01<00:00, 15.92it/s]\u001b[A\n",
      " 94%|█████████▍| 282/300 [07:22<00:28,  1.57s/it]             \u001b[A\n",
      "Epoch 283/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 283/300:   9%|▉         | 2/22 [00:00<00:01, 16.00it/s]\u001b[A\n",
      "Epoch 283/300:  18%|█▊        | 4/22 [00:00<00:01, 15.69it/s]\u001b[A\n",
      "Epoch 283/300:  27%|██▋       | 6/22 [00:00<00:01, 15.68it/s]\u001b[A\n",
      "Epoch 283/300:  36%|███▋      | 8/22 [00:00<00:00, 15.32it/s]\u001b[A\n",
      "Epoch 283/300:  45%|████▌     | 10/22 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "Epoch 283/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 283/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.35it/s]\u001b[A\n",
      "Epoch 283/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 283/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.62it/s]\u001b[A\n",
      "Epoch 283/300:  91%|█████████ | 20/22 [00:01<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 283/300: 100%|██████████| 22/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
      " 94%|█████████▍| 283/300 [07:24<00:26,  1.58s/it]             \u001b[A\n",
      "Epoch 284/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 284/300:   9%|▉         | 2/22 [00:00<00:01, 16.14it/s]\u001b[A\n",
      "Epoch 284/300:  18%|█▊        | 4/22 [00:00<00:01, 15.85it/s]\u001b[A\n",
      "Epoch 284/300:  27%|██▋       | 6/22 [00:00<00:01, 15.75it/s]\u001b[A\n",
      "Epoch 284/300:  36%|███▋      | 8/22 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "Epoch 284/300:  45%|████▌     | 10/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 284/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.84it/s]\u001b[A\n",
      "Epoch 284/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "Epoch 284/300:  73%|███████▎  | 16/22 [00:01<00:00, 15.88it/s]\u001b[A\n",
      "Epoch 284/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 284/300:  91%|█████████ | 20/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 284/300: 100%|██████████| 22/22 [00:01<00:00, 15.97it/s]\u001b[A\n",
      " 95%|█████████▍| 284/300 [07:25<00:25,  1.57s/it]             \u001b[A\n",
      "Epoch 285/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 285/300:   9%|▉         | 2/22 [00:00<00:01, 15.58it/s]\u001b[A\n",
      "Epoch 285/300:  18%|█▊        | 4/22 [00:00<00:01, 15.83it/s]\u001b[A\n",
      "Epoch 285/300:  27%|██▋       | 6/22 [00:00<00:01, 15.26it/s]\u001b[A\n",
      "Epoch 285/300:  36%|███▋      | 8/22 [00:00<00:00, 15.69it/s]\u001b[A\n",
      "Epoch 285/300:  45%|████▌     | 10/22 [00:00<00:00, 15.62it/s]\u001b[A\n",
      " 14%|█▎        | 41/300 [01:02<06:27,  1.50s/it]<00:50,  4.68it/s]\n",
      "Epoch 42/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42/300:   9%|▉         | 2/22 [00:00<00:01, 16.10it/s]\u001b[A\n",
      "Epoch 42/300:  18%|█▊        | 4/22 [00:00<00:01, 16.29it/s]\u001b[A\n",
      "Epoch 42/300:  27%|██▋       | 6/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 42/300:  36%|███▋      | 8/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 42/300:  45%|████▌     | 10/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 42/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 42/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 42/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 42/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.68it/s]\u001b[A\n",
      "Epoch 42/300:  91%|█████████ | 20/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 42/300: 100%|██████████| 22/22 [00:01<00:00, 16.70it/s]\u001b[A\n",
      " 14%|█▍        | 42/300 [01:03<06:28,  1.51s/it]             \u001b[A\n",
      "Epoch 43/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43/300:   9%|▉         | 2/22 [00:00<00:01, 15.37it/s]\u001b[A\n",
      "Epoch 43/300:  18%|█▊        | 4/22 [00:00<00:01, 16.00it/s]\u001b[A\n",
      "Epoch 43/300:  27%|██▋       | 6/22 [00:00<00:01, 15.33it/s]\u001b[A\n",
      "Epoch 43/300:  36%|███▋      | 8/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 43/300:  45%|████▌     | 10/22 [00:00<00:00, 16.93it/s]\u001b[A\n",
      "Epoch 43/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 43/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 43/300:  73%|███████▎  | 16/22 [00:00<00:00, 17.00it/s]\u001b[A\n",
      "Epoch 43/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 43/300:  91%|█████████ | 20/22 [00:01<00:00, 17.04it/s]\u001b[A\n",
      "Epoch 43/300: 100%|██████████| 22/22 [00:01<00:00, 16.77it/s]\u001b[A\n",
      " 14%|█▍        | 43/300 [01:05<06:25,  1.50s/it]             \u001b[A\n",
      "Epoch 44/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44/300:   9%|▉         | 2/22 [00:00<00:01, 16.93it/s]\u001b[A\n",
      "Epoch 44/300:  18%|█▊        | 4/22 [00:00<00:01, 16.58it/s]\u001b[A\n",
      "Epoch 44/300:  27%|██▋       | 6/22 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 44/300:  36%|███▋      | 8/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 44/300:  45%|████▌     | 10/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 44/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 44/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 44/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 44/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 44/300:  91%|█████████ | 20/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 44/300: 100%|██████████| 22/22 [00:01<00:00, 16.96it/s]\u001b[A\n",
      " 15%|█▍        | 44/300 [01:06<06:23,  1.50s/it]             \u001b[A\n",
      "Epoch 45/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45/300:   9%|▉         | 2/22 [00:00<00:01, 17.66it/s]\u001b[A\n",
      "Epoch 45/300:  18%|█▊        | 4/22 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "Epoch 45/300:  27%|██▋       | 6/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 45/300:  36%|███▋      | 8/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 45/300:  45%|████▌     | 10/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 45/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 45/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 45/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 45/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 45/300:  91%|█████████ | 20/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 45/300: 100%|██████████| 22/22 [00:01<00:00, 16.83it/s]\u001b[A\n",
      " 15%|█▌        | 45/300 [01:08<06:21,  1.50s/it]             \u001b[A\n",
      "Epoch 46/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46/300:   9%|▉         | 2/22 [00:00<00:01, 16.35it/s]\u001b[A\n",
      "Epoch 46/300:  18%|█▊        | 4/22 [00:00<00:01, 16.06it/s]\u001b[A\n",
      "Epoch 46/300:  27%|██▋       | 6/22 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "Epoch 46/300:  36%|███▋      | 8/22 [00:00<00:00, 15.85it/s]\u001b[A\n",
      "Epoch 46/300:  45%|████▌     | 10/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 46/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.74it/s]\u001b[A\n",
      "Epoch 46/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 46/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "Epoch 46/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 46/300:  91%|█████████ | 20/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 46/300: 100%|██████████| 22/22 [00:01<00:00, 16.71it/s]\u001b[A\n",
      " 15%|█▌        | 46/300 [01:09<06:20,  1.50s/it]             \u001b[A\n",
      "Epoch 47/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47/300:   9%|▉         | 2/22 [00:00<00:01, 16.59it/s]\u001b[A\n",
      "Epoch 47/300:  18%|█▊        | 4/22 [00:00<00:01, 17.08it/s]\u001b[A\n",
      "Epoch 47/300:  27%|██▋       | 6/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 47/300:  36%|███▋      | 8/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 47/300:  45%|████▌     | 10/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 47/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 47/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 47/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 47/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 47/300:  91%|█████████ | 20/22 [00:01<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 47/300: 100%|██████████| 22/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
      " 16%|█▌        | 47/300 [01:11<06:18,  1.50s/it]             \u001b[A\n",
      "Epoch 48/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48/300:   9%|▉         | 2/22 [00:00<00:01, 17.42it/s]\u001b[A\n",
      "Epoch 48/300:  18%|█▊        | 4/22 [00:00<00:01, 17.33it/s]\u001b[A\n",
      "Epoch 48/300:  27%|██▋       | 6/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 48/300:  36%|███▋      | 8/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 48/300:  45%|████▌     | 10/22 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 48/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 48/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 48/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 48/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 48/300:  91%|█████████ | 20/22 [00:01<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 48/300: 100%|██████████| 22/22 [00:01<00:00, 16.55it/s]\u001b[A\n",
      " 16%|█▌        | 48/300 [01:12<06:17,  1.50s/it]             \u001b[A\n",
      "Epoch 49/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49/300:   9%|▉         | 2/22 [00:00<00:01, 16.77it/s]\u001b[A\n",
      "Epoch 49/300:  18%|█▊        | 4/22 [00:00<00:01, 16.14it/s]\u001b[A\n",
      "Epoch 49/300:  27%|██▋       | 6/22 [00:00<00:00, 16.75it/s]\u001b[A\n",
      "Epoch 49/300:  36%|███▋      | 8/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 49/300:  45%|████▌     | 10/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 49/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "Epoch 49/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 49/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.88it/s]\u001b[A\n",
      "Epoch 49/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 49/300:  91%|█████████ | 20/22 [00:01<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 49/300: 100%|██████████| 22/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      " 16%|█▋        | 49/300 [01:14<06:15,  1.50s/it]             \u001b[A\n",
      "Epoch 50/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50/300:   9%|▉         | 2/22 [00:00<00:01, 15.78it/s]\u001b[A\n",
      "Epoch 50/300:  18%|█▊        | 4/22 [00:00<00:01, 16.58it/s]\u001b[A\n",
      "Epoch 50/300:  27%|██▋       | 6/22 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 50/300:  36%|███▋      | 8/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 50/300:  45%|████▌     | 10/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 50/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.89it/s]\u001b[A\n",
      "Epoch 50/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.94it/s]\u001b[A\n",
      "Epoch 50/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 50/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 50/300:  91%|█████████ | 20/22 [00:01<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 50/300: 100%|██████████| 22/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      " 17%|█▋        | 50/300 [01:15<06:14,  1.50s/it]             \u001b[A\n",
      "Epoch 51/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51/300:   9%|▉         | 2/22 [00:00<00:01, 15.87it/s]\u001b[A\n",
      "Epoch 51/300:  18%|█▊        | 4/22 [00:00<00:01, 16.62it/s]\u001b[A\n",
      "Epoch 51/300:  27%|██▋       | 6/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 51/300:  36%|███▋      | 8/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 51/300:  45%|████▌     | 10/22 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "Epoch 51/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 51/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 51/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 51/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 51/300:  91%|█████████ | 20/22 [00:01<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 51/300: 100%|██████████| 22/22 [00:01<00:00, 16.68it/s]\u001b[A\n",
      " 17%|█▋        | 51/300 [01:17<06:13,  1.50s/it]             \u001b[A\n",
      "Epoch 52/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52/300:   9%|▉         | 2/22 [00:00<00:01, 16.93it/s]\u001b[A\n",
      "Epoch 52/300:  18%|█▊        | 4/22 [00:00<00:01, 16.86it/s]\u001b[A\n",
      "Epoch 52/300:  27%|██▋       | 6/22 [00:00<00:00, 17.06it/s]\u001b[A\n",
      "Epoch 52/300:  36%|███▋      | 8/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 52/300:  45%|████▌     | 10/22 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "Epoch 52/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 52/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.77it/s]\u001b[A\n",
      "Epoch 52/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 52/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 52/300:  91%|█████████ | 20/22 [00:01<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 52/300: 100%|██████████| 22/22 [00:01<00:00, 16.52it/s]\u001b[A\n",
      " 17%|█▋        | 52/300 [01:18<06:12,  1.50s/it]             \u001b[A\n",
      "Epoch 53/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53/300:   9%|▉         | 2/22 [00:00<00:01, 15.13it/s]\u001b[A\n",
      "Epoch 53/300:  18%|█▊        | 4/22 [00:00<00:01, 16.32it/s]\u001b[A\n",
      "Epoch 53/300:  27%|██▋       | 6/22 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 53/300:  36%|███▋      | 8/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 53/300:  45%|████▌     | 10/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 53/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 53/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 53/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 53/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 53/300:  91%|█████████ | 20/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 53/300: 100%|██████████| 22/22 [00:01<00:00, 16.71it/s]\u001b[A\n",
      " 18%|█▊        | 53/300 [01:20<06:11,  1.50s/it]             \u001b[A\n",
      "Epoch 54/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54/300:   9%|▉         | 2/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 54/300:  18%|█▊        | 4/22 [00:00<00:01, 15.80it/s]\u001b[A\n",
      "Epoch 54/300:  27%|██▋       | 6/22 [00:00<00:01, 15.65it/s]\u001b[A\n",
      "Epoch 54/300:  36%|███▋      | 8/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 54/300:  45%|████▌     | 10/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 54/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.70it/s]\u001b[A\n",
      "Epoch 54/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 54/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 54/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 54/300:  91%|█████████ | 20/22 [00:01<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 54/300: 100%|██████████| 22/22 [00:01<00:00, 16.71it/s]\u001b[A\n",
      " 18%|█▊        | 54/300 [01:21<06:09,  1.50s/it]             \u001b[A\n",
      "Epoch 55/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55/300:   9%|▉         | 2/22 [00:00<00:01, 15.56it/s]\u001b[A\n",
      "Epoch 55/300:  18%|█▊        | 4/22 [00:00<00:01, 16.63it/s]\u001b[A\n",
      "Epoch 55/300:  27%|██▋       | 6/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 55/300:  36%|███▋      | 8/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 55/300:  45%|████▌     | 10/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 55/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 55/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 55/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 55/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 55/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 55/300: 100%|██████████| 22/22 [00:01<00:00, 16.92it/s]\u001b[A\n",
      " 18%|█▊        | 55/300 [01:23<06:07,  1.50s/it]             \u001b[A\n",
      "Epoch 56/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56/300:   9%|▉         | 2/22 [00:00<00:01, 16.50it/s]\u001b[A\n",
      "Epoch 56/300:  18%|█▊        | 4/22 [00:00<00:01, 16.06it/s]\u001b[A\n",
      "Epoch 56/300:  27%|██▋       | 6/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 56/300:  36%|███▋      | 8/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 56/300:  45%|████▌     | 10/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 56/300:  55%|█████▍    | 12/22 [00:00<00:00, 17.02it/s]\u001b[A\n",
      "Epoch 56/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 56/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 56/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 56/300:  91%|█████████ | 20/22 [00:01<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 56/300: 100%|██████████| 22/22 [00:01<00:00, 17.07it/s]\u001b[A\n",
      " 19%|█▊        | 56/300 [01:24<06:05,  1.50s/it]             \u001b[A\n",
      "Epoch 57/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57/300:   9%|▉         | 2/22 [00:00<00:01, 17.28it/s]\u001b[A\n",
      "Epoch 57/300:  18%|█▊        | 4/22 [00:00<00:01, 16.84it/s]\u001b[A\n",
      "Epoch 57/300:  27%|██▋       | 6/22 [00:00<00:00, 17.02it/s]\u001b[A\n",
      "Epoch 57/300:  36%|███▋      | 8/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 57/300:  45%|████▌     | 10/22 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 57/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 57/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 57/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 57/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 57/300:  91%|█████████ | 20/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 57/300: 100%|██████████| 22/22 [00:01<00:00, 16.87it/s]\u001b[A\n",
      " 19%|█▉        | 57/300 [01:26<06:03,  1.50s/it]             \u001b[A\n",
      "Epoch 58/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58/300:   9%|▉         | 2/22 [00:00<00:01, 15.64it/s]\u001b[A\n",
      "Epoch 58/300:  18%|█▊        | 4/22 [00:00<00:01, 16.46it/s]\u001b[A\n",
      "Epoch 58/300:  27%|██▋       | 6/22 [00:00<00:00, 16.87it/s]\u001b[A\n",
      "Epoch 58/300:  36%|███▋      | 8/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 58/300:  45%|████▌     | 10/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 58/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 58/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 58/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.87it/s]\u001b[A\n",
      "Epoch 58/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.70it/s]\u001b[A\n",
      "Epoch 58/300:  91%|█████████ | 20/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 58/300: 100%|██████████| 22/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      " 19%|█▉        | 58/300 [01:27<06:02,  1.50s/it]             \u001b[A\n",
      "Epoch 59/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59/300:   9%|▉         | 2/22 [00:00<00:01, 16.27it/s]\u001b[A\n",
      "Epoch 59/300:  18%|█▊        | 4/22 [00:00<00:01, 15.95it/s]\u001b[A\n",
      "Epoch 59/300:  27%|██▋       | 6/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 59/300:  36%|███▋      | 8/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 59/300:  45%|████▌     | 10/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 59/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 59/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 59/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 59/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 59/300:  91%|█████████ | 20/22 [00:01<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 59/300: 100%|██████████| 22/22 [00:01<00:00, 16.54it/s]\u001b[A\n",
      " 20%|█▉        | 59/300 [01:29<06:01,  1.50s/it]             \u001b[A\n",
      "Epoch 60/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60/300:   9%|▉         | 2/22 [00:00<00:01, 16.21it/s]\u001b[A\n",
      "Epoch 60/300:  18%|█▊        | 4/22 [00:00<00:01, 16.41it/s]\u001b[A\n",
      "Epoch 60/300:  27%|██▋       | 6/22 [00:00<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 60/300:  36%|███▋      | 8/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 60/300:  45%|████▌     | 10/22 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 60/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 60/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 60/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 60/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 60/300:  91%|█████████ | 20/22 [00:01<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 60/300: 100%|██████████| 22/22 [00:01<00:00, 16.65it/s]\u001b[A\n",
      " 20%|██        | 60/300 [01:30<06:00,  1.50s/it]             \u001b[A\n",
      "Epoch 61/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61/300:   9%|▉         | 2/22 [00:00<00:01, 17.00it/s]\u001b[A\n",
      "Epoch 61/300:  18%|█▊        | 4/22 [00:00<00:01, 16.49it/s]\u001b[A\n",
      "Epoch 61/300:  27%|██▋       | 6/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 61/300:  36%|███▋      | 8/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 61/300:  45%|████▌     | 10/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 61/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.98it/s]\u001b[A\n",
      "Epoch 61/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.72it/s]\u001b[A\n",
      "Epoch 61/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 61/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 61/300:  91%|█████████ | 20/22 [00:01<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 61/300: 100%|██████████| 22/22 [00:01<00:00, 16.75it/s]\u001b[A\n",
      " 20%|██        | 61/300 [01:32<05:59,  1.50s/it]             \u001b[A\n",
      "Epoch 62/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62/300:   9%|▉         | 2/22 [00:00<00:01, 17.23it/s]\u001b[A\n",
      "Epoch 62/300:  18%|█▊        | 4/22 [00:00<00:01, 16.93it/s]\u001b[A\n",
      "Epoch 62/300:  27%|██▋       | 6/22 [00:00<00:00, 16.93it/s]\u001b[A\n",
      "Epoch 62/300:  36%|███▋      | 8/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 62/300:  45%|████▌     | 10/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 62/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 62/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 62/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 62/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 62/300:  91%|█████████ | 20/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 62/300: 100%|██████████| 22/22 [00:01<00:00, 16.61it/s]\u001b[A\n",
      " 21%|██        | 62/300 [01:33<05:57,  1.50s/it]             \u001b[A\n",
      "Epoch 63/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63/300:   9%|▉         | 2/22 [00:00<00:01, 17.14it/s]\u001b[A\n",
      "Epoch 63/300:  18%|█▊        | 4/22 [00:00<00:01, 16.89it/s]\u001b[A\n",
      "Epoch 63/300:  27%|██▋       | 6/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 63/300:  36%|███▋      | 8/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 63/300:  45%|████▌     | 10/22 [00:00<00:00, 16.74it/s]\u001b[A\n",
      "Epoch 63/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 63/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 63/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 63/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.03it/s]\u001b[A\n",
      "Epoch 63/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 63/300: 100%|██████████| 22/22 [00:01<00:00, 16.24it/s]\u001b[A\n",
      " 21%|██        | 63/300 [01:35<05:56,  1.51s/it]             \u001b[A\n",
      "Epoch 64/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64/300:   9%|▉         | 2/22 [00:00<00:01, 16.80it/s]\u001b[A\n",
      "Epoch 64/300:  18%|█▊        | 4/22 [00:00<00:01, 17.04it/s]\u001b[A\n",
      "Epoch 64/300:  27%|██▋       | 6/22 [00:00<00:00, 16.88it/s]\u001b[A\n",
      "Epoch 64/300:  36%|███▋      | 8/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 64/300:  45%|████▌     | 10/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 64/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.70it/s]\u001b[A\n",
      "Epoch 64/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 64/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 64/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 64/300:  91%|█████████ | 20/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 64/300: 100%|██████████| 22/22 [00:01<00:00, 16.47it/s]\u001b[A\n",
      " 21%|██▏       | 64/300 [01:36<05:55,  1.51s/it]             \u001b[A\n",
      "Epoch 65/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65/300:   9%|▉         | 2/22 [00:00<00:01, 16.67it/s]\u001b[A\n",
      "Epoch 65/300:  18%|█▊        | 4/22 [00:00<00:01, 16.10it/s]\u001b[A\n",
      "Epoch 65/300:  27%|██▋       | 6/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 65/300:  36%|███▋      | 8/22 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 65/300:  45%|████▌     | 10/22 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 65/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 65/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 65/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 65/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 65/300:  91%|█████████ | 20/22 [00:01<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 65/300: 100%|██████████| 22/22 [00:01<00:00, 16.77it/s]\u001b[A\n",
      " 22%|██▏       | 65/300 [01:38<05:53,  1.50s/it]             \u001b[A\n",
      "Epoch 66/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66/300:   9%|▉         | 2/22 [00:00<00:01, 16.99it/s]\u001b[A\n",
      "Epoch 66/300:  18%|█▊        | 4/22 [00:00<00:01, 17.09it/s]\u001b[A\n",
      "Epoch 66/300:  27%|██▋       | 6/22 [00:00<00:00, 17.14it/s]\u001b[A\n",
      "Epoch 66/300:  36%|███▋      | 8/22 [00:00<00:00, 16.77it/s]\u001b[A\n",
      "Epoch 66/300:  45%|████▌     | 10/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 66/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 66/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 66/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 66/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.80it/s]\u001b[A\n",
      "Epoch 66/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 66/300: 100%|██████████| 22/22 [00:01<00:00, 16.17it/s]\u001b[A\n",
      " 22%|██▏       | 66/300 [01:39<05:51,  1.50s/it]             \u001b[A\n",
      "Epoch 67/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67/300:   9%|▉         | 2/22 [00:00<00:01, 16.94it/s]\u001b[A\n",
      "Epoch 67/300:  18%|█▊        | 4/22 [00:00<00:01, 16.95it/s]\u001b[A\n",
      "Epoch 67/300:  27%|██▋       | 6/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 67/300:  36%|███▋      | 8/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 67/300:  45%|████▌     | 10/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 67/300:  55%|█████▍    | 12/22 [00:00<00:00, 17.01it/s]\u001b[A\n",
      "Epoch 67/300:  64%|██████▎   | 14/22 [00:00<00:00, 17.23it/s]\u001b[A\n",
      "Epoch 67/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 67/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 67/300:  91%|█████████ | 20/22 [00:01<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 67/300: 100%|██████████| 22/22 [00:01<00:00, 16.54it/s]\u001b[A\n",
      " 22%|██▏       | 67/300 [01:41<05:49,  1.50s/it]             \u001b[A\n",
      "Epoch 68/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68/300:   9%|▉         | 2/22 [00:00<00:01, 16.28it/s]\u001b[A\n",
      "Epoch 68/300:  18%|█▊        | 4/22 [00:00<00:01, 16.35it/s]\u001b[A\n",
      "Epoch 68/300:  27%|██▋       | 6/22 [00:00<00:00, 16.87it/s]\u001b[A\n",
      "Epoch 68/300:  36%|███▋      | 8/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 68/300:  45%|████▌     | 10/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 68/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 68/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 68/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 68/300:  82%|████████▏ | 18/22 [00:01<00:00, 17.00it/s]\u001b[A\n",
      "Epoch 68/300:  91%|█████████ | 20/22 [00:01<00:00, 16.83it/s]\u001b[A\n",
      "Epoch 68/300: 100%|██████████| 22/22 [00:01<00:00, 16.81it/s]\u001b[A\n",
      " 23%|██▎       | 68/300 [01:42<05:46,  1.50s/it]             \u001b[A\n",
      "Epoch 69/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69/300:   9%|▉         | 2/22 [00:00<00:01, 16.87it/s]\u001b[A\n",
      "Epoch 69/300:  18%|█▊        | 4/22 [00:00<00:01, 16.13it/s]\u001b[A\n",
      "Epoch 69/300:  27%|██▋       | 6/22 [00:00<00:01, 15.94it/s]\u001b[A\n",
      "Epoch 69/300:  36%|███▋      | 8/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 69/300:  45%|████▌     | 10/22 [00:00<00:00, 17.32it/s]\u001b[A\n",
      "Epoch 69/300:  55%|█████▍    | 12/22 [00:00<00:00, 17.04it/s]\u001b[A\n",
      "Epoch 69/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.78it/s]\u001b[A\n",
      "Epoch 69/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 69/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 69/300:  91%|█████████ | 20/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 69/300: 100%|██████████| 22/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      " 23%|██▎       | 69/300 [01:44<05:46,  1.50s/it]             \u001b[A\n",
      "Epoch 70/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70/300:   9%|▉         | 2/22 [00:00<00:01, 16.73it/s]\u001b[A\n",
      "Epoch 70/300:  18%|█▊        | 4/22 [00:00<00:01, 16.57it/s]\u001b[A\n",
      "Epoch 70/300:  27%|██▋       | 6/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 70/300:  36%|███▋      | 8/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 70/300:  45%|████▌     | 10/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 70/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 70/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 70/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.71it/s]\u001b[A\n",
      "Epoch 70/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 70/300:  91%|█████████ | 20/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 70/300: 100%|██████████| 22/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      " 23%|██▎       | 70/300 [01:45<05:45,  1.50s/it]             \u001b[A\n",
      "Epoch 71/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71/300:   9%|▉         | 2/22 [00:00<00:01, 16.22it/s]\u001b[A\n",
      "Epoch 71/300:  18%|█▊        | 4/22 [00:00<00:01, 16.18it/s]\u001b[A\n",
      "Epoch 71/300:  27%|██▋       | 6/22 [00:00<00:01, 15.94it/s]\u001b[A\n",
      "Epoch 71/300:  36%|███▋      | 8/22 [00:00<00:00, 17.21it/s]\u001b[A\n",
      "Epoch 71/300:  45%|████▌     | 10/22 [00:00<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 71/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 71/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 71/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 71/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 71/300:  91%|█████████ | 20/22 [00:01<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 71/300: 100%|██████████| 22/22 [00:01<00:00, 17.11it/s]\u001b[A\n",
      " 24%|██▎       | 71/300 [01:47<05:42,  1.50s/it]             \u001b[A\n",
      "Epoch 72/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72/300:   9%|▉         | 2/22 [00:00<00:01, 16.30it/s]\u001b[A\n",
      "Epoch 72/300:  18%|█▊        | 4/22 [00:00<00:01, 16.14it/s]\u001b[A\n",
      "Epoch 72/300:  27%|██▋       | 6/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 72/300:  36%|███▋      | 8/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 72/300:  45%|████▌     | 10/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 72/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "Epoch 72/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 72/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.95it/s]\u001b[A\n",
      "Epoch 72/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 72/300:  91%|█████████ | 20/22 [00:01<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 72/300: 100%|██████████| 22/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      " 24%|██▍       | 72/300 [01:48<05:41,  1.50s/it]             \u001b[A\n",
      "Epoch 73/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73/300:   9%|▉         | 2/22 [00:00<00:01, 16.99it/s]\u001b[A\n",
      "Epoch 73/300:  18%|█▊        | 4/22 [00:00<00:01, 16.23it/s]\u001b[A\n",
      "Epoch 73/300:  27%|██▋       | 6/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 73/300:  36%|███▋      | 8/22 [00:00<00:00, 17.11it/s]\u001b[A\n",
      "Epoch 73/300:  45%|████▌     | 10/22 [00:00<00:00, 16.86it/s]\u001b[A\n",
      "Epoch 73/300:  55%|█████▍    | 12/22 [00:00<00:00, 17.11it/s]\u001b[A\n",
      "Epoch 73/300:  64%|██████▎   | 14/22 [00:00<00:00, 17.15it/s]\u001b[A\n",
      "Epoch 73/300:  73%|███████▎  | 16/22 [00:00<00:00, 17.37it/s]\u001b[A\n",
      "Epoch 73/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 73/300:  91%|█████████ | 20/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 73/300: 100%|██████████| 22/22 [00:01<00:00, 16.78it/s]\u001b[A\n",
      " 24%|██▍       | 73/300 [01:50<05:37,  1.49s/it]             \u001b[A\n",
      "Epoch 74/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74/300:   9%|▉         | 2/22 [00:00<00:01, 13.87it/s]\u001b[A\n",
      "Epoch 74/300:  18%|█▊        | 4/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 74/300:  27%|██▋       | 6/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 74/300:  36%|███▋      | 8/22 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 74/300:  45%|████▌     | 10/22 [00:00<00:00, 16.98it/s]\u001b[A\n",
      "Epoch 74/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 74/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 74/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 74/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.23it/s]\u001b[A\n",
      "Epoch 74/300:  91%|█████████ | 20/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 74/300: 100%|██████████| 22/22 [00:01<00:00, 16.65it/s]\u001b[A\n",
      " 25%|██▍       | 74/300 [01:51<05:37,  1.49s/it]             \u001b[A\n",
      "Epoch 75/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75/300:   9%|▉         | 2/22 [00:00<00:01, 15.45it/s]\u001b[A\n",
      "Epoch 75/300:  18%|█▊        | 4/22 [00:00<00:01, 15.97it/s]\u001b[A\n",
      "Epoch 75/300:  27%|██▋       | 6/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 75/300:  36%|███▋      | 8/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 75/300:  45%|████▌     | 10/22 [00:00<00:00, 16.95it/s]\u001b[A\n",
      "Epoch 75/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 75/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 75/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 75/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 75/300:  91%|█████████ | 20/22 [00:01<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 75/300: 100%|██████████| 22/22 [00:01<00:00, 16.43it/s]\u001b[A\n",
      " 25%|██▌       | 75/300 [01:53<05:37,  1.50s/it]             \u001b[A\n",
      "Epoch 76/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76/300:   9%|▉         | 2/22 [00:00<00:01, 16.89it/s]\u001b[A\n",
      "Epoch 76/300:  18%|█▊        | 4/22 [00:00<00:01, 16.43it/s]\u001b[A\n",
      "Epoch 76/300:  27%|██▋       | 6/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 76/300:  36%|███▋      | 8/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 76/300:  45%|████▌     | 10/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 76/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 76/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 76/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 76/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 76/300:  91%|█████████ | 20/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 76/300: 100%|██████████| 22/22 [00:01<00:00, 16.81it/s]\u001b[A\n",
      " 25%|██▌       | 76/300 [01:54<05:35,  1.50s/it]             \u001b[A\n",
      "Epoch 77/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77/300:   9%|▉         | 2/22 [00:00<00:01, 16.74it/s]\u001b[A\n",
      "Epoch 77/300:  18%|█▊        | 4/22 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "Epoch 77/300:  27%|██▋       | 6/22 [00:00<00:00, 16.86it/s]\u001b[A\n",
      "Epoch 77/300:  36%|███▋      | 8/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 77/300:  45%|████▌     | 10/22 [00:00<00:00, 15.73it/s]\u001b[A\n",
      "Epoch 77/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 77/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 77/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 77/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 77/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 77/300: 100%|██████████| 22/22 [00:01<00:00, 16.65it/s]\u001b[A\n",
      " 26%|██▌       | 77/300 [01:56<05:34,  1.50s/it]             \u001b[A\n",
      "Epoch 78/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78/300:   9%|▉         | 2/22 [00:00<00:01, 17.41it/s]\u001b[A\n",
      "Epoch 78/300:  18%|█▊        | 4/22 [00:00<00:01, 17.11it/s]\u001b[A\n",
      "Epoch 78/300:  27%|██▋       | 6/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 78/300:  36%|███▋      | 8/22 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 78/300:  45%|████▌     | 10/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 78/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 78/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 78/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 78/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 78/300:  91%|█████████ | 20/22 [00:01<00:00, 16.71it/s]\u001b[A\n",
      "Epoch 78/300: 100%|██████████| 22/22 [00:01<00:00, 16.66it/s]\u001b[A\n",
      " 26%|██▌       | 78/300 [01:57<05:32,  1.50s/it]             \u001b[A\n",
      "Epoch 79/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79/300:   9%|▉         | 2/22 [00:00<00:01, 17.44it/s]\u001b[A\n",
      "Epoch 79/300:  18%|█▊        | 4/22 [00:00<00:01, 17.17it/s]\u001b[A\n",
      "Epoch 79/300:  27%|██▋       | 6/22 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 79/300:  36%|███▋      | 8/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 79/300:  45%|████▌     | 10/22 [00:00<00:00, 17.06it/s]\u001b[A\n",
      "Epoch 79/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.83it/s]\u001b[A\n",
      "Epoch 79/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 79/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 79/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 79/300:  91%|█████████ | 20/22 [00:01<00:00, 16.31it/s]\u001b[A\n",
      "Epoch 79/300: 100%|██████████| 22/22 [00:01<00:00, 16.51it/s]\u001b[A\n",
      " 26%|██▋       | 79/300 [01:59<05:30,  1.50s/it]             \u001b[A\n",
      "Epoch 80/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80/300:   9%|▉         | 2/22 [00:00<00:01, 16.43it/s]\u001b[A\n",
      "Epoch 80/300:  18%|█▊        | 4/22 [00:00<00:01, 15.85it/s]\u001b[A\n",
      "Epoch 80/300:  27%|██▋       | 6/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 80/300:  36%|███▋      | 8/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 80/300:  45%|████▌     | 10/22 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "Epoch 80/300:  55%|█████▍    | 12/22 [00:00<00:00, 17.36it/s]\u001b[A\n",
      "Epoch 80/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 80/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 80/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 80/300:  91%|█████████ | 20/22 [00:01<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 80/300: 100%|██████████| 22/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      " 27%|██▋       | 80/300 [02:00<05:29,  1.50s/it]             \u001b[A\n",
      "Epoch 81/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81/300:   9%|▉         | 2/22 [00:00<00:01, 15.95it/s]\u001b[A\n",
      "Epoch 81/300:  18%|█▊        | 4/22 [00:00<00:01, 16.85it/s]\u001b[A\n",
      "Epoch 81/300:  27%|██▋       | 6/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 81/300:  36%|███▋      | 8/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 81/300:  45%|████▌     | 10/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "Epoch 81/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 81/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 81/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.97it/s]\u001b[A\n",
      "Epoch 81/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 81/300:  91%|█████████ | 20/22 [00:01<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 81/300: 100%|██████████| 22/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      " 27%|██▋       | 81/300 [02:02<05:27,  1.50s/it]             \u001b[A\n",
      "Epoch 82/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82/300:   9%|▉         | 2/22 [00:00<00:01, 17.39it/s]\u001b[A\n",
      "Epoch 82/300:  18%|█▊        | 4/22 [00:00<00:01, 16.87it/s]\u001b[A\n",
      "Epoch 82/300:  27%|██▋       | 6/22 [00:00<00:00, 16.84it/s]\u001b[A\n",
      "Epoch 82/300:  36%|███▋      | 8/22 [00:00<00:00, 16.97it/s]\u001b[A\n",
      "Epoch 82/300:  45%|████▌     | 10/22 [00:00<00:00, 16.71it/s]\u001b[A\n",
      "Epoch 82/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 82/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.81it/s]\u001b[A\n",
      "Epoch 82/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 82/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 82/300:  91%|█████████ | 20/22 [00:01<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 82/300: 100%|██████████| 22/22 [00:01<00:00, 16.75it/s]\u001b[A\n",
      " 27%|██▋       | 82/300 [02:03<05:26,  1.50s/it]             \u001b[A\n",
      "Epoch 83/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83/300:   9%|▉         | 2/22 [00:00<00:01, 15.36it/s]\u001b[A\n",
      "Epoch 83/300:  18%|█▊        | 4/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 83/300:  27%|██▋       | 6/22 [00:00<00:01, 15.80it/s]\u001b[A\n",
      "Epoch 83/300:  36%|███▋      | 8/22 [00:00<00:00, 15.75it/s]\u001b[A\n",
      "Epoch 83/300:  45%|████▌     | 10/22 [00:00<00:00, 15.61it/s]\u001b[A\n",
      "Epoch 83/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 83/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 83/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 83/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 83/300:  91%|█████████ | 20/22 [00:01<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 83/300: 100%|██████████| 22/22 [00:01<00:00, 17.04it/s]\u001b[A\n",
      " 28%|██▊       | 83/300 [02:05<05:25,  1.50s/it]             \u001b[A\n",
      "Epoch 84/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84/300:   9%|▉         | 2/22 [00:00<00:01, 15.77it/s]\u001b[A\n",
      "Epoch 84/300:  18%|█▊        | 4/22 [00:00<00:01, 16.14it/s]\u001b[A\n",
      "Epoch 84/300:  27%|██▋       | 6/22 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 84/300:  36%|███▋      | 8/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 84/300:  45%|████▌     | 10/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 84/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 84/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 84/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 84/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.35it/s]\u001b[A\n",
      "Epoch 84/300:  91%|█████████ | 20/22 [00:01<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 84/300: 100%|██████████| 22/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      " 28%|██▊       | 84/300 [02:06<05:24,  1.50s/it]             \u001b[A\n",
      "Epoch 85/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85/300:   9%|▉         | 2/22 [00:00<00:01, 17.15it/s]\u001b[A\n",
      "Epoch 85/300:  18%|█▊        | 4/22 [00:00<00:01, 16.49it/s]\u001b[A\n",
      "Epoch 85/300:  27%|██▋       | 6/22 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 85/300:  36%|███▋      | 8/22 [00:00<00:00, 16.84it/s]\u001b[A\n",
      "Epoch 85/300:  45%|████▌     | 10/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 85/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 85/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 85/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 85/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 85/300:  91%|█████████ | 20/22 [00:01<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 85/300: 100%|██████████| 22/22 [00:01<00:00, 16.55it/s]\u001b[A\n",
      " 28%|██▊       | 85/300 [02:08<05:23,  1.50s/it]             \u001b[A\n",
      "Epoch 86/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 86/300:   9%|▉         | 2/22 [00:00<00:01, 15.91it/s]\u001b[A\n",
      "Epoch 86/300:  18%|█▊        | 4/22 [00:00<00:01, 16.80it/s]\u001b[A\n",
      "Epoch 86/300:  27%|██▋       | 6/22 [00:00<00:00, 16.63it/s]\u001b[A\n",
      "Epoch 86/300:  36%|███▋      | 8/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "Epoch 86/300:  45%|████▌     | 10/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 86/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 86/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 86/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 86/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.75it/s]\u001b[A\n",
      "Epoch 86/300:  91%|█████████ | 20/22 [00:01<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 86/300: 100%|██████████| 22/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      " 29%|██▊       | 86/300 [02:09<05:22,  1.51s/it]             \u001b[A\n",
      "Epoch 87/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87/300:   9%|▉         | 2/22 [00:00<00:01, 17.26it/s]\u001b[A\n",
      "Epoch 87/300:  18%|█▊        | 4/22 [00:00<00:01, 17.23it/s]\u001b[A\n",
      "Epoch 87/300:  27%|██▋       | 6/22 [00:00<00:00, 17.17it/s]\u001b[A\n",
      "Epoch 87/300:  36%|███▋      | 8/22 [00:00<00:00, 17.31it/s]\u001b[A\n",
      "Epoch 87/300:  45%|████▌     | 10/22 [00:00<00:00, 16.93it/s]\u001b[A\n",
      "Epoch 87/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 87/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 87/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 87/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 87/300:  91%|█████████ | 20/22 [00:01<00:00, 16.05it/s]\u001b[A\n",
      "Epoch 87/300: 100%|██████████| 22/22 [00:01<00:00, 16.53it/s]\u001b[A\n",
      " 29%|██▉       | 87/300 [02:11<05:19,  1.50s/it]             \u001b[A\n",
      "Epoch 88/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88/300:   9%|▉         | 2/22 [00:00<00:01, 16.54it/s]\u001b[A\n",
      "Epoch 88/300:  18%|█▊        | 4/22 [00:00<00:01, 15.81it/s]\u001b[A\n",
      "Epoch 88/300:  27%|██▋       | 6/22 [00:00<00:00, 16.70it/s]\u001b[A\n",
      "Epoch 88/300:  36%|███▋      | 8/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 88/300:  45%|████▌     | 10/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 88/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 88/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.77it/s]\u001b[A\n",
      "Epoch 88/300:  73%|███████▎  | 16/22 [00:00<00:00, 17.07it/s]\u001b[A\n",
      "Epoch 88/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.77it/s]\u001b[A\n",
      "Epoch 88/300:  91%|█████████ | 20/22 [00:01<00:00, 16.90it/s]\u001b[A\n",
      "Epoch 88/300: 100%|██████████| 22/22 [00:01<00:00, 17.03it/s]\u001b[A\n",
      " 29%|██▉       | 88/300 [02:12<05:17,  1.50s/it]             \u001b[A\n",
      "Epoch 89/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89/300:   9%|▉         | 2/22 [00:00<00:01, 15.88it/s]\u001b[A\n",
      "Epoch 89/300:  18%|█▊        | 4/22 [00:00<00:01, 16.25it/s]\u001b[A\n",
      "Epoch 89/300:  27%|██▋       | 6/22 [00:00<00:00, 16.38it/s]\u001b[A\n",
      "Epoch 89/300:  36%|███▋      | 8/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 89/300:  45%|████▌     | 10/22 [00:00<00:00, 16.08it/s]\u001b[A\n",
      "Epoch 89/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 89/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 89/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 89/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 89/300:  91%|█████████ | 20/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 89/300: 100%|██████████| 22/22 [00:01<00:00, 16.51it/s]\u001b[A\n",
      " 30%|██▉       | 89/300 [02:14<05:16,  1.50s/it]             \u001b[A\n",
      "Epoch 90/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90/300:   9%|▉         | 2/22 [00:00<00:01, 18.74it/s]\u001b[A\n",
      "Epoch 90/300:  18%|█▊        | 4/22 [00:00<00:01, 16.99it/s]\u001b[A\n",
      "Epoch 90/300:  27%|██▋       | 6/22 [00:00<00:00, 17.05it/s]\u001b[A\n",
      "Epoch 90/300:  36%|███▋      | 8/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 90/300:  45%|████▌     | 10/22 [00:00<00:00, 16.70it/s]\u001b[A\n",
      "Epoch 90/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 90/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 90/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 90/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      "Epoch 90/300:  91%|█████████ | 20/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 90/300: 100%|██████████| 22/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      " 30%|███       | 90/300 [02:15<05:14,  1.50s/it]             \u001b[A\n",
      "Epoch 91/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91/300:   9%|▉         | 2/22 [00:00<00:01, 15.52it/s]\u001b[A\n",
      "Epoch 91/300:  18%|█▊        | 4/22 [00:00<00:01, 15.27it/s]\u001b[A\n",
      "Epoch 91/300:  27%|██▋       | 6/22 [00:00<00:01, 15.71it/s]\u001b[A\n",
      "Epoch 91/300:  36%|███▋      | 8/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 91/300:  45%|████▌     | 10/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 91/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 91/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.51it/s]\u001b[A\n",
      "Epoch 91/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.90it/s]\u001b[A\n",
      "Epoch 91/300:  82%|████████▏ | 18/22 [00:01<00:00, 17.02it/s]\u001b[A\n",
      "Epoch 91/300:  91%|█████████ | 20/22 [00:01<00:00, 16.87it/s]\u001b[A\n",
      "Epoch 91/300: 100%|██████████| 22/22 [00:01<00:00, 16.56it/s]\u001b[A\n",
      " 30%|███       | 91/300 [02:17<05:13,  1.50s/it]             \u001b[A\n",
      "Epoch 92/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92/300:   9%|▉         | 2/22 [00:00<00:01, 17.24it/s]\u001b[A\n",
      "Epoch 92/300:  18%|█▊        | 4/22 [00:00<00:01, 16.69it/s]\u001b[A\n",
      "Epoch 92/300:  27%|██▋       | 6/22 [00:00<00:00, 17.42it/s]\u001b[A\n",
      "Epoch 92/300:  36%|███▋      | 8/22 [00:00<00:00, 17.67it/s]\u001b[A\n",
      "Epoch 92/300:  45%|████▌     | 10/22 [00:00<00:00, 17.32it/s]\u001b[A\n",
      "Epoch 92/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 92/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "Epoch 92/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.64it/s]\u001b[A\n",
      "Epoch 92/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 92/300:  91%|█████████ | 20/22 [00:01<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 92/300: 100%|██████████| 22/22 [00:01<00:00, 16.63it/s]\u001b[A\n",
      " 31%|███       | 92/300 [02:18<05:11,  1.50s/it]             \u001b[A\n",
      "Epoch 93/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93/300:   9%|▉         | 2/22 [00:00<00:01, 16.16it/s]\u001b[A\n",
      "Epoch 93/300:  18%|█▊        | 4/22 [00:00<00:01, 15.90it/s]\u001b[A\n",
      "Epoch 93/300:  27%|██▋       | 6/22 [00:00<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 93/300:  36%|███▋      | 8/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 93/300:  45%|████▌     | 10/22 [00:00<00:00, 16.54it/s]\u001b[A\n",
      "Epoch 93/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 93/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 93/300:  73%|███████▎  | 16/22 [00:00<00:00, 17.01it/s]\u001b[A\n",
      "Epoch 93/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 93/300:  91%|█████████ | 20/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 93/300: 100%|██████████| 22/22 [00:01<00:00, 16.31it/s]\u001b[A\n",
      " 31%|███       | 93/300 [02:20<05:10,  1.50s/it]             \u001b[A\n",
      "Epoch 94/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94/300:   9%|▉         | 2/22 [00:00<00:01, 16.12it/s]\u001b[A\n",
      "Epoch 94/300:  18%|█▊        | 4/22 [00:00<00:01, 16.62it/s]\u001b[A\n",
      "Epoch 94/300:  27%|██▋       | 6/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 94/300:  36%|███▋      | 8/22 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 94/300:  45%|████▌     | 10/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "Epoch 94/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.86it/s]\u001b[A\n",
      "Epoch 94/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 94/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.97it/s]\u001b[A\n",
      "Epoch 94/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.79it/s]\u001b[A\n",
      "Epoch 94/300:  91%|█████████ | 20/22 [00:01<00:00, 16.68it/s]\u001b[A\n",
      "Epoch 94/300: 100%|██████████| 22/22 [00:01<00:00, 17.04it/s]\u001b[A\n",
      " 31%|███▏      | 94/300 [02:21<05:07,  1.49s/it]             \u001b[A\n",
      "Epoch 95/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95/300:   9%|▉         | 2/22 [00:00<00:01, 16.54it/s]\u001b[A\n",
      "Epoch 95/300:  18%|█▊        | 4/22 [00:00<00:01, 16.46it/s]\u001b[A\n",
      "Epoch 95/300:  27%|██▋       | 6/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 95/300:  36%|███▋      | 8/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 95/300:  45%|████▌     | 10/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 95/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.94it/s]\u001b[A\n",
      "Epoch 95/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.56it/s]\u001b[A\n",
      "Epoch 95/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.40it/s]\u001b[A\n",
      "Epoch 95/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.96it/s]\u001b[A\n",
      "Epoch 95/300:  91%|█████████ | 20/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 95/300: 100%|██████████| 22/22 [00:01<00:00, 16.96it/s]\u001b[A\n",
      " 32%|███▏      | 95/300 [02:23<05:06,  1.49s/it]             \u001b[A\n",
      "Epoch 96/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96/300:   9%|▉         | 2/22 [00:00<00:01, 17.14it/s]\u001b[A\n",
      "Epoch 96/300:  18%|█▊        | 4/22 [00:00<00:01, 16.84it/s]\u001b[A\n",
      "Epoch 96/300:  27%|██▋       | 6/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 96/300:  36%|███▋      | 8/22 [00:00<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 96/300:  45%|████▌     | 10/22 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "Epoch 96/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.71it/s]\u001b[A\n",
      "Epoch 96/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.14it/s]\u001b[A\n",
      "Epoch 96/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "Epoch 96/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.36it/s]\u001b[A\n",
      "Epoch 96/300:  91%|█████████ | 20/22 [00:01<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 96/300: 100%|██████████| 22/22 [00:01<00:00, 16.84it/s]\u001b[A\n",
      " 32%|███▏      | 96/300 [02:24<05:04,  1.49s/it]             \u001b[A\n",
      "Epoch 97/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97/300:   9%|▉         | 2/22 [00:00<00:01, 17.17it/s]\u001b[A\n",
      "Epoch 97/300:  18%|█▊        | 4/22 [00:00<00:01, 17.46it/s]\u001b[A\n",
      "Epoch 97/300:  27%|██▋       | 6/22 [00:00<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 97/300:  36%|███▋      | 8/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 97/300:  45%|████▌     | 10/22 [00:00<00:00, 16.17it/s]\u001b[A\n",
      "Epoch 97/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 97/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 97/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 97/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.93it/s]\u001b[A\n",
      "Epoch 97/300:  91%|█████████ | 20/22 [00:01<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 97/300: 100%|██████████| 22/22 [00:01<00:00, 16.42it/s]\u001b[A\n",
      " 32%|███▏      | 97/300 [02:26<05:03,  1.50s/it]             \u001b[A\n",
      "Epoch 98/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98/300:   9%|▉         | 2/22 [00:00<00:01, 16.21it/s]\u001b[A\n",
      "Epoch 98/300:  18%|█▊        | 4/22 [00:00<00:01, 16.97it/s]\u001b[A\n",
      "Epoch 98/300:  27%|██▋       | 6/22 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "Epoch 98/300:  36%|███▋      | 8/22 [00:00<00:00, 16.85it/s]\u001b[A\n",
      "Epoch 98/300:  45%|████▌     | 10/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 98/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 98/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 98/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 98/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 98/300:  91%|█████████ | 20/22 [00:01<00:00, 16.02it/s]\u001b[A\n",
      "Epoch 98/300: 100%|██████████| 22/22 [00:01<00:00, 16.69it/s]\u001b[A\n",
      " 33%|███▎      | 98/300 [02:27<05:03,  1.50s/it]             \u001b[A\n",
      "Epoch 99/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99/300:   9%|▉         | 2/22 [00:00<00:01, 17.32it/s]\u001b[A\n",
      "Epoch 99/300:  18%|█▊        | 4/22 [00:00<00:01, 17.34it/s]\u001b[A\n",
      "Epoch 99/300:  27%|██▋       | 6/22 [00:00<00:00, 17.60it/s]\u001b[A\n",
      "Epoch 99/300:  36%|███▋      | 8/22 [00:00<00:00, 17.37it/s]\u001b[A\n",
      "Epoch 99/300:  45%|████▌     | 10/22 [00:00<00:00, 17.06it/s]\u001b[A\n",
      "Epoch 99/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.85it/s]\u001b[A\n",
      "Epoch 99/300:  64%|██████▎   | 14/22 [00:00<00:00, 17.17it/s]\u001b[A\n",
      "Epoch 99/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.97it/s]\u001b[A\n",
      "Epoch 99/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 99/300:  91%|█████████ | 20/22 [00:01<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 99/300: 100%|██████████| 22/22 [00:01<00:00, 15.98it/s]\u001b[A\n",
      " 33%|███▎      | 99/300 [02:29<05:01,  1.50s/it]             \u001b[A\n",
      "Epoch 100/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100/300:   9%|▉         | 2/22 [00:00<00:01, 14.66it/s]\u001b[A\n",
      "Epoch 100/300:  18%|█▊        | 4/22 [00:00<00:01, 15.40it/s]\u001b[A\n",
      "Epoch 100/300:  27%|██▋       | 6/22 [00:00<00:00, 16.20it/s]\u001b[A\n",
      "Epoch 100/300:  36%|███▋      | 8/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 100/300:  45%|████▌     | 10/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 100/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 100/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 100/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.80it/s]\u001b[A\n",
      "Epoch 100/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.72it/s]\u001b[A\n",
      "Epoch 100/300:  91%|█████████ | 20/22 [00:01<00:00, 16.92it/s]\u001b[A\n",
      "Epoch 100/300: 100%|██████████| 22/22 [00:01<00:00, 16.67it/s]\u001b[A\n",
      " 33%|███▎      | 100/300 [02:30<05:00,  1.50s/it]             \u001b[A\n",
      "Epoch 101/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101/300:   9%|▉         | 2/22 [00:00<00:01, 17.55it/s]\u001b[A\n",
      "Epoch 101/300:  18%|█▊        | 4/22 [00:00<00:01, 17.53it/s]\u001b[A\n",
      "Epoch 101/300:  27%|██▋       | 6/22 [00:00<00:00, 17.42it/s]\u001b[A\n",
      "Epoch 101/300:  36%|███▋      | 8/22 [00:00<00:00, 17.07it/s]\u001b[A\n",
      "Epoch 101/300:  45%|████▌     | 10/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 101/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 101/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 101/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 101/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 101/300:  91%|█████████ | 20/22 [00:01<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 101/300: 100%|██████████| 22/22 [00:01<00:00, 16.41it/s]\u001b[A\n",
      " 34%|███▎      | 101/300 [02:32<04:58,  1.50s/it]             \u001b[A\n",
      "Epoch 102/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102/300:   9%|▉         | 2/22 [00:00<00:01, 16.68it/s]\u001b[A\n",
      "Epoch 102/300:  18%|█▊        | 4/22 [00:00<00:01, 17.11it/s]\u001b[A\n",
      "Epoch 102/300:  27%|██▋       | 6/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 102/300:  36%|███▋      | 8/22 [00:00<00:00, 15.82it/s]\u001b[A\n",
      "Epoch 102/300:  45%|████▌     | 10/22 [00:00<00:00, 16.10it/s]\u001b[A\n",
      "Epoch 102/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.63it/s]\u001b[A\n",
      "Epoch 102/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 102/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 102/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 102/300:  91%|█████████ | 20/22 [00:01<00:00, 16.23it/s]\u001b[A\n",
      "Epoch 102/300: 100%|██████████| 22/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      " 34%|███▍      | 102/300 [02:33<04:57,  1.50s/it]             \u001b[A\n",
      "Epoch 103/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103/300:   9%|▉         | 2/22 [00:00<00:01, 16.30it/s]\u001b[A\n",
      "Epoch 103/300:  18%|█▊        | 4/22 [00:00<00:01, 16.38it/s]\u001b[A\n",
      "Epoch 103/300:  27%|██▋       | 6/22 [00:00<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 103/300:  36%|███▋      | 8/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 103/300:  45%|████▌     | 10/22 [00:00<00:00, 15.94it/s]\u001b[A\n",
      "Epoch 103/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 103/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 103/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 103/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 103/300:  91%|█████████ | 20/22 [00:01<00:00, 16.25it/s]\u001b[A\n",
      "Epoch 103/300: 100%|██████████| 22/22 [00:01<00:00, 16.73it/s]\u001b[A\n",
      " 34%|███▍      | 103/300 [02:35<04:56,  1.51s/it]             \u001b[A\n",
      "Epoch 104/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 104/300:   9%|▉         | 2/22 [00:00<00:01, 15.57it/s]\u001b[A\n",
      "Epoch 104/300:  18%|█▊        | 4/22 [00:00<00:01, 15.51it/s]\u001b[A\n",
      "Epoch 104/300:  27%|██▋       | 6/22 [00:00<00:01, 15.98it/s]\u001b[A\n",
      "Epoch 104/300:  36%|███▋      | 8/22 [00:00<00:00, 16.04it/s]\u001b[A\n",
      "Epoch 104/300:  45%|████▌     | 10/22 [00:00<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 104/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 104/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 104/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.71it/s]\u001b[A\n",
      "Epoch 104/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.88it/s]\u001b[A\n",
      "Epoch 104/300:  91%|█████████ | 20/22 [00:01<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 104/300: 100%|██████████| 22/22 [00:01<00:00, 16.77it/s]\u001b[A\n",
      " 35%|███▍      | 104/300 [02:36<04:54,  1.50s/it]             \u001b[A\n",
      "Epoch 105/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 105/300:   9%|▉         | 2/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 105/300:  18%|█▊        | 4/22 [00:00<00:01, 16.58it/s]\u001b[A\n",
      "Epoch 105/300:  27%|██▋       | 6/22 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "Epoch 105/300:  36%|███▋      | 8/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 105/300:  45%|████▌     | 10/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 105/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 105/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.77it/s]\u001b[A\n",
      "Epoch 105/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.82it/s]\u001b[A\n",
      "Epoch 105/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.80it/s]\u001b[A\n",
      "Epoch 105/300:  91%|█████████ | 20/22 [00:01<00:00, 16.47it/s]\u001b[A\n",
      "Epoch 105/300: 100%|██████████| 22/22 [00:01<00:00, 16.39it/s]\u001b[A\n",
      " 35%|███▌      | 105/300 [02:38<04:53,  1.50s/it]             \u001b[A\n",
      "Epoch 106/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 106/300:   9%|▉         | 2/22 [00:00<00:01, 16.72it/s]\u001b[A\n",
      "Epoch 106/300:  18%|█▊        | 4/22 [00:00<00:01, 17.09it/s]\u001b[A\n",
      "Epoch 106/300:  27%|██▋       | 6/22 [00:00<00:00, 16.33it/s]\u001b[A\n",
      "Epoch 106/300:  36%|███▋      | 8/22 [00:00<00:00, 16.19it/s]\u001b[A\n",
      "Epoch 106/300:  45%|████▌     | 10/22 [00:00<00:00, 16.75it/s]\u001b[A\n",
      "Epoch 106/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 106/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.29it/s]\u001b[A\n",
      "Epoch 106/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.16it/s]\u001b[A\n",
      "Epoch 106/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.62it/s]\u001b[A\n",
      "Epoch 106/300:  91%|█████████ | 20/22 [00:01<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 106/300: 100%|██████████| 22/22 [00:01<00:00, 16.83it/s]\u001b[A\n",
      " 35%|███▌      | 106/300 [02:39<04:51,  1.50s/it]             \u001b[A\n",
      "Epoch 107/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 107/300:   9%|▉         | 2/22 [00:00<00:01, 15.82it/s]\u001b[A\n",
      "Epoch 107/300:  18%|█▊        | 4/22 [00:00<00:01, 15.86it/s]\u001b[A\n",
      "Epoch 107/300:  27%|██▋       | 6/22 [00:00<00:01, 15.76it/s]\u001b[A\n",
      "Epoch 107/300:  36%|███▋      | 8/22 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "Epoch 107/300:  45%|████▌     | 10/22 [00:00<00:00, 16.68it/s]\u001b[A\n",
      "Epoch 107/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 107/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.41it/s]\u001b[A\n",
      "Epoch 107/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 107/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.63it/s]\u001b[A\n",
      "Epoch 107/300:  91%|█████████ | 20/22 [00:01<00:00, 16.63it/s]\u001b[A\n",
      "Epoch 107/300: 100%|██████████| 22/22 [00:01<00:00, 16.92it/s]\u001b[A\n",
      " 36%|███▌      | 107/300 [02:41<04:49,  1.50s/it]             \u001b[A\n",
      "Epoch 108/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 108/300:   9%|▉         | 2/22 [00:00<00:01, 17.27it/s]\u001b[A\n",
      "Epoch 108/300:  18%|█▊        | 4/22 [00:00<00:01, 16.33it/s]\u001b[A\n",
      "Epoch 108/300:  27%|██▋       | 6/22 [00:00<00:01, 15.66it/s]\u001b[A\n",
      "Epoch 108/300:  36%|███▋      | 8/22 [00:00<00:00, 15.79it/s]\u001b[A\n",
      "Epoch 108/300:  45%|████▌     | 10/22 [00:00<00:00, 15.76it/s]\u001b[A\n",
      "Epoch 108/300:  55%|█████▍    | 12/22 [00:00<00:00, 15.95it/s]\u001b[A\n",
      "Epoch 108/300:  64%|██████▎   | 14/22 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "Epoch 108/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "Epoch 108/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.43it/s]\u001b[A\n",
      "Epoch 108/300:  91%|█████████ | 20/22 [00:01<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 108/300: 100%|██████████| 22/22 [00:01<00:00, 16.73it/s]\u001b[A\n",
      " 36%|███▌      | 108/300 [02:42<04:49,  1.51s/it]             \u001b[A\n",
      "Epoch 109/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109/300:   9%|▉         | 2/22 [00:00<00:01, 15.84it/s]\u001b[A\n",
      "Epoch 109/300:  18%|█▊        | 4/22 [00:00<00:01, 16.34it/s]\u001b[A\n",
      "Epoch 109/300:  27%|██▋       | 6/22 [00:00<00:00, 16.76it/s]\u001b[A\n",
      "Epoch 109/300:  36%|███▋      | 8/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 109/300:  45%|████▌     | 10/22 [00:00<00:00, 16.53it/s]\u001b[A\n",
      "Epoch 109/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.21it/s]\u001b[A\n",
      "Epoch 109/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "Epoch 109/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 109/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 109/300:  91%|█████████ | 20/22 [00:01<00:00, 16.44it/s]\u001b[A\n",
      "Epoch 109/300: 100%|██████████| 22/22 [00:01<00:00, 16.66it/s]\u001b[A\n",
      " 36%|███▋      | 109/300 [02:44<04:47,  1.50s/it]             \u001b[A\n",
      "Epoch 110/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 110/300:   9%|▉         | 2/22 [00:00<00:01, 15.46it/s]\u001b[A\n",
      "Epoch 110/300:  18%|█▊        | 4/22 [00:00<00:01, 15.59it/s]\u001b[A\n",
      "Epoch 110/300:  27%|██▋       | 6/22 [00:00<00:01, 15.99it/s]\u001b[A\n",
      "Epoch 110/300:  36%|███▋      | 8/22 [00:00<00:00, 15.93it/s]\u001b[A\n",
      "Epoch 110/300:  45%|████▌     | 10/22 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "Epoch 110/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.39it/s]\u001b[A\n",
      "Epoch 110/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.46it/s]\u001b[A\n",
      "Epoch 110/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 110/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 110/300:  91%|█████████ | 20/22 [00:01<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 110/300: 100%|██████████| 22/22 [00:01<00:00, 16.90it/s]\u001b[A\n",
      " 37%|███▋      | 110/300 [02:45<04:45,  1.50s/it]             \u001b[A\n",
      "Epoch 111/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 111/300:   9%|▉         | 2/22 [00:00<00:01, 16.71it/s]\u001b[A\n",
      "Epoch 111/300:  18%|█▊        | 4/22 [00:00<00:01, 17.14it/s]\u001b[A\n",
      "Epoch 111/300:  27%|██▋       | 6/22 [00:00<00:00, 17.03it/s]\u001b[A\n",
      "Epoch 111/300:  36%|███▋      | 8/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 111/300:  45%|████▌     | 10/22 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "Epoch 111/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.42it/s]\u001b[A\n",
      "Epoch 111/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.77it/s]\u001b[A\n",
      "Epoch 111/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.86it/s]\u001b[A\n",
      "Epoch 111/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.53it/s]\u001b[A\n",
      "Epoch 111/300:  91%|█████████ | 20/22 [00:01<00:00, 16.37it/s]\u001b[A\n",
      "Epoch 111/300: 100%|██████████| 22/22 [00:01<00:00, 16.65it/s]\u001b[A\n",
      " 37%|███▋      | 111/300 [02:47<04:43,  1.50s/it]             \u001b[A\n",
      "Epoch 112/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 112/300:   9%|▉         | 2/22 [00:00<00:01, 16.52it/s]\u001b[A\n",
      "Epoch 112/300:  18%|█▊        | 4/22 [00:00<00:01, 15.92it/s]\u001b[A\n",
      "Epoch 112/300:  27%|██▋       | 6/22 [00:00<00:00, 16.48it/s]\u001b[A\n",
      "Epoch 112/300:  36%|███▋      | 8/22 [00:00<00:00, 16.30it/s]\u001b[A\n",
      "Epoch 112/300:  45%|████▌     | 10/22 [00:00<00:00, 16.07it/s]\u001b[A\n",
      "Epoch 112/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 112/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "Epoch 112/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 112/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 112/300:  91%|█████████ | 20/22 [00:01<00:00, 16.45it/s]\u001b[A\n",
      "Epoch 112/300: 100%|██████████| 22/22 [00:01<00:00, 16.58it/s]\u001b[A\n",
      " 37%|███▋      | 112/300 [02:48<04:42,  1.50s/it]             \u001b[A\n",
      "Epoch 113/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 113/300:   9%|▉         | 2/22 [00:00<00:01, 17.14it/s]\u001b[A\n",
      "Epoch 113/300:  18%|█▊        | 4/22 [00:00<00:01, 16.51it/s]\u001b[A\n",
      "Epoch 113/300:  27%|██▋       | 6/22 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 113/300:  36%|███▋      | 8/22 [00:00<00:00, 16.65it/s]\u001b[A\n",
      "Epoch 113/300:  45%|████▌     | 10/22 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "Epoch 113/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 113/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.66it/s]\u001b[A\n",
      "Epoch 113/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.91it/s]\u001b[A\n",
      "Epoch 113/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.22it/s]\u001b[A\n",
      "Epoch 113/300:  91%|█████████ | 20/22 [00:01<00:00, 16.27it/s]\u001b[A\n",
      "Epoch 113/300: 100%|██████████| 22/22 [00:01<00:00, 16.40it/s]\u001b[A\n",
      " 38%|███▊      | 113/300 [02:50<04:40,  1.50s/it]             \u001b[A\n",
      "Epoch 114/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 114/300:   9%|▉         | 2/22 [00:00<00:01, 15.60it/s]\u001b[A\n",
      "Epoch 114/300:  18%|█▊        | 4/22 [00:00<00:01, 16.32it/s]\u001b[A\n",
      "Epoch 114/300:  27%|██▋       | 6/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 114/300:  36%|███▋      | 8/22 [00:00<00:00, 16.55it/s]\u001b[A\n",
      "Epoch 114/300:  45%|████▌     | 10/22 [00:00<00:00, 16.81it/s]\u001b[A\n",
      "Epoch 114/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.57it/s]\u001b[A\n",
      "Epoch 114/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.73it/s]\u001b[A\n",
      "Epoch 114/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.36it/s]\u001b[A\n",
      "Epoch 114/300:  82%|████████▏ | 18/22 [00:01<00:00, 15.99it/s]\u001b[A\n",
      "Epoch 114/300:  91%|█████████ | 20/22 [00:01<00:00, 16.06it/s]\u001b[A\n",
      "Epoch 114/300: 100%|██████████| 22/22 [00:01<00:00, 16.59it/s]\u001b[A\n",
      " 38%|███▊      | 114/300 [02:51<04:39,  1.50s/it]             \u001b[A\n",
      "Epoch 115/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 115/300:   9%|▉         | 2/22 [00:00<00:01, 16.53it/s]\u001b[A\n",
      "Epoch 115/300:  18%|█▊        | 4/22 [00:00<00:01, 16.83it/s]\u001b[A\n",
      "Epoch 115/300:  27%|██▋       | 6/22 [00:00<00:00, 17.10it/s]\u001b[A\n",
      "Epoch 115/300:  36%|███▋      | 8/22 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 115/300:  45%|████▌     | 10/22 [00:00<00:00, 16.50it/s]\u001b[A\n",
      "Epoch 115/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "Epoch 115/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.11it/s]\u001b[A\n",
      "Epoch 115/300:  73%|███████▎  | 16/22 [00:00<00:00, 15.89it/s]\u001b[A\n",
      "Epoch 115/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.32it/s]\u001b[A\n",
      "Epoch 115/300:  91%|█████████ | 20/22 [00:01<00:00, 16.26it/s]\u001b[A\n",
      "Epoch 115/300: 100%|██████████| 22/22 [00:01<00:00, 16.48it/s]\u001b[A\n",
      " 38%|███▊      | 115/300 [02:53<04:38,  1.50s/it]             \u001b[A\n",
      "Epoch 116/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 116/300:   9%|▉         | 2/22 [00:00<00:01, 16.84it/s]\u001b[A\n",
      "Epoch 116/300:  18%|█▊        | 4/22 [00:00<00:01, 17.21it/s]\u001b[A\n",
      "Epoch 116/300:  27%|██▋       | 6/22 [00:00<00:00, 16.28it/s]\u001b[A\n",
      "Epoch 116/300:  36%|███▋      | 8/22 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "Epoch 116/300:  45%|████▌     | 10/22 [00:00<00:00, 16.61it/s]\u001b[A\n",
      "Epoch 116/300:  55%|█████▍    | 12/22 [00:00<00:00, 16.67it/s]\u001b[A\n",
      "Epoch 116/300:  64%|██████▎   | 14/22 [00:00<00:00, 16.59it/s]\u001b[A\n",
      "Epoch 116/300:  73%|███████▎  | 16/22 [00:00<00:00, 16.64it/s]\u001b[A\n",
      "Epoch 116/300:  82%|████████▏ | 18/22 [00:01<00:00, 16.74it/s]\u001b[A\n",
      "Epoch 116/300:  91%|█████████ | 20/22 [00:01<00:00, 16.34it/s]\u001b[A\n",
      "Epoch 116/300: 100%|██████████| 22/22 [00:01<00:00, 16.50it/s]\u001b[A\n",
      "                                                              \u001b[AIOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "\n",
      " 32%|███▏      | 95/300 [01:55<04:10,  1.22s/it]             \u001b[A\n",
      "Epoch 96/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96/300:  14%|█▎        | 3/22 [00:00<00:00, 20.82it/s]\u001b[A\n",
      "Epoch 96/300:  27%|██▋       | 6/22 [00:00<00:00, 19.86it/s]\u001b[A\n",
      "Epoch 96/300:  36%|███▋      | 8/22 [00:00<00:00, 19.52it/s]\u001b[A\n",
      "Epoch 96/300:  50%|█████     | 11/22 [00:00<00:00, 20.17it/s]\u001b[A\n",
      "Epoch 96/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.22it/s]\u001b[A\n",
      "Epoch 96/300:  77%|███████▋  | 17/22 [00:00<00:00, 20.81it/s]\u001b[A\n",
      "Epoch 96/300:  91%|█████████ | 20/22 [00:00<00:00, 20.65it/s]\u001b[A\n",
      " 32%|███▏      | 96/300 [01:57<04:09,  1.23s/it]             \u001b[A\n",
      "Epoch 97/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97/300:  14%|█▎        | 3/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 97/300:  27%|██▋       | 6/22 [00:00<00:00, 20.62it/s]\u001b[A\n",
      "Epoch 97/300:  41%|████      | 9/22 [00:00<00:00, 20.22it/s]\u001b[A\n",
      "Epoch 97/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.04it/s]\u001b[A\n",
      "Epoch 97/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.14it/s]\u001b[A\n",
      "Epoch 97/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.38it/s]\u001b[A\n",
      "Epoch 97/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.20it/s]\u001b[A\n",
      " 32%|███▏      | 97/300 [01:58<04:08,  1.23s/it]             \u001b[A\n",
      "Epoch 98/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98/300:   9%|▉         | 2/22 [00:00<00:01, 19.62it/s]\u001b[A\n",
      "Epoch 98/300:  23%|██▎       | 5/22 [00:00<00:00, 20.70it/s]\u001b[A\n",
      "Epoch 98/300:  36%|███▋      | 8/22 [00:00<00:00, 20.97it/s]\u001b[A\n",
      "Epoch 98/300:  50%|█████     | 11/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 98/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.76it/s]\u001b[A\n",
      "Epoch 98/300:  77%|███████▋  | 17/22 [00:00<00:00, 20.19it/s]\u001b[A\n",
      "Epoch 98/300:  91%|█████████ | 20/22 [00:00<00:00, 20.48it/s]\u001b[A\n",
      " 33%|███▎      | 98/300 [01:59<04:07,  1.22s/it]             \u001b[A\n",
      "Epoch 99/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99/300:  14%|█▎        | 3/22 [00:00<00:00, 20.65it/s]\u001b[A\n",
      "Epoch 99/300:  27%|██▋       | 6/22 [00:00<00:00, 20.55it/s]\u001b[A\n",
      "Epoch 99/300:  41%|████      | 9/22 [00:00<00:00, 20.68it/s]\u001b[A\n",
      "Epoch 99/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.40it/s]\u001b[A\n",
      "Epoch 99/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.29it/s]\u001b[A\n",
      "Epoch 99/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.16it/s]\u001b[A\n",
      "Epoch 99/300:  95%|█████████▌| 21/22 [00:01<00:00, 21.02it/s]\u001b[A\n",
      " 33%|███▎      | 99/300 [02:00<04:05,  1.22s/it]             \u001b[A\n",
      "Epoch 100/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100/300:   9%|▉         | 2/22 [00:00<00:01, 19.57it/s]\u001b[A\n",
      "Epoch 100/300:  23%|██▎       | 5/22 [00:00<00:00, 20.52it/s]\u001b[A\n",
      "Epoch 100/300:  36%|███▋      | 8/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      "Epoch 100/300:  50%|█████     | 11/22 [00:00<00:00, 20.43it/s]\u001b[A\n",
      "Epoch 100/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.87it/s]\u001b[A\n",
      "Epoch 100/300:  77%|███████▋  | 17/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 100/300:  91%|█████████ | 20/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      " 33%|███▎      | 100/300 [02:02<04:03,  1.22s/it]             \u001b[A\n",
      "Epoch 101/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101/300:  14%|█▎        | 3/22 [00:00<00:00, 20.80it/s]\u001b[A\n",
      "Epoch 101/300:  27%|██▋       | 6/22 [00:00<00:00, 20.46it/s]\u001b[A\n",
      "Epoch 101/300:  41%|████      | 9/22 [00:00<00:00, 20.38it/s]\u001b[A\n",
      "Epoch 101/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.22it/s]\u001b[A\n",
      "Epoch 101/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.26it/s]\u001b[A\n",
      "Epoch 101/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.56it/s]\u001b[A\n",
      "Epoch 101/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.52it/s]\u001b[A\n",
      " 34%|███▎      | 101/300 [02:03<04:02,  1.22s/it]             \u001b[A\n",
      "Epoch 102/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102/300:  14%|█▎        | 3/22 [00:00<00:00, 20.11it/s]\u001b[A\n",
      "Epoch 102/300:  27%|██▋       | 6/22 [00:00<00:00, 20.74it/s]\u001b[A\n",
      "Epoch 102/300:  41%|████      | 9/22 [00:00<00:00, 19.90it/s]\u001b[A\n",
      "Epoch 102/300:  50%|█████     | 11/22 [00:00<00:00, 19.75it/s]\u001b[A\n",
      "Epoch 102/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.64it/s]\u001b[A\n",
      "Epoch 102/300:  77%|███████▋  | 17/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 102/300:  91%|█████████ | 20/22 [00:00<00:00, 20.58it/s]\u001b[A\n",
      " 34%|███▍      | 102/300 [02:04<04:01,  1.22s/it]             \u001b[A\n",
      "Epoch 103/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103/300:  14%|█▎        | 3/22 [00:00<00:00, 20.45it/s]\u001b[A\n",
      "Epoch 103/300:  27%|██▋       | 6/22 [00:00<00:00, 20.83it/s]\u001b[A\n",
      "Epoch 103/300:  41%|████      | 9/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 103/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.28it/s]\u001b[A\n",
      "Epoch 103/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.80it/s]\u001b[A\n",
      "Epoch 103/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.80it/s]\u001b[A\n",
      "Epoch 103/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.73it/s]\u001b[A\n",
      " 34%|███▍      | 103/300 [02:05<03:59,  1.21s/it]             \u001b[A\n",
      "Epoch 104/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 104/300:  14%|█▎        | 3/22 [00:00<00:00, 20.88it/s]\u001b[A\n",
      "Epoch 104/300:  27%|██▋       | 6/22 [00:00<00:00, 20.32it/s]\u001b[A\n",
      "Epoch 104/300:  41%|████      | 9/22 [00:00<00:00, 20.22it/s]\u001b[A\n",
      "Epoch 104/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.38it/s]\u001b[A\n",
      "Epoch 104/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.46it/s]\u001b[A\n",
      "Epoch 104/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.42it/s]\u001b[A\n",
      "Epoch 104/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.68it/s]\u001b[A\n",
      " 35%|███▍      | 104/300 [02:06<03:57,  1.21s/it]             \u001b[A\n",
      "Epoch 105/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 105/300:   9%|▉         | 2/22 [00:00<00:01, 18.80it/s]\u001b[A\n",
      "Epoch 105/300:  23%|██▎       | 5/22 [00:00<00:00, 20.23it/s]\u001b[A\n",
      "Epoch 105/300:  36%|███▋      | 8/22 [00:00<00:00, 20.77it/s]\u001b[A\n",
      "Epoch 105/300:  50%|█████     | 11/22 [00:00<00:00, 20.56it/s]\u001b[A\n",
      "Epoch 105/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.27it/s]\u001b[A\n",
      "Epoch 105/300:  77%|███████▋  | 17/22 [00:00<00:00, 20.49it/s]\u001b[A\n",
      "Epoch 105/300:  91%|█████████ | 20/22 [00:00<00:00, 20.27it/s]\u001b[A\n",
      " 35%|███▌      | 105/300 [02:08<03:57,  1.22s/it]             \u001b[A\n",
      "Epoch 106/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 106/300:  14%|█▎        | 3/22 [00:00<00:00, 20.58it/s]\u001b[A\n",
      "Epoch 106/300:  27%|██▋       | 6/22 [00:00<00:00, 20.62it/s]\u001b[A\n",
      "Epoch 106/300:  41%|████      | 9/22 [00:00<00:00, 20.23it/s]\u001b[A\n",
      "Epoch 106/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.05it/s]\u001b[A\n",
      "Epoch 106/300:  68%|██████▊   | 15/22 [00:00<00:00, 19.82it/s]\u001b[A\n",
      "Epoch 106/300:  82%|████████▏ | 18/22 [00:00<00:00, 19.96it/s]\u001b[A\n",
      "Epoch 106/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.48it/s]\u001b[A\n",
      " 35%|███▌      | 106/300 [02:09<03:57,  1.22s/it]             \u001b[A\n",
      "Epoch 107/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 107/300:  14%|█▎        | 3/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 107/300:  27%|██▋       | 6/22 [00:00<00:00, 20.09it/s]\u001b[A\n",
      "Epoch 107/300:  41%|████      | 9/22 [00:00<00:00, 20.70it/s]\u001b[A\n",
      "Epoch 107/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 107/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.97it/s]\u001b[A\n",
      "Epoch 107/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.43it/s]\u001b[A\n",
      "Epoch 107/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.88it/s]\u001b[A\n",
      " 36%|███▌      | 107/300 [02:10<03:55,  1.22s/it]             \u001b[A\n",
      "Epoch 108/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 108/300:   9%|▉         | 2/22 [00:00<00:01, 19.04it/s]\u001b[A\n",
      "Epoch 108/300:  23%|██▎       | 5/22 [00:00<00:00, 19.91it/s]\u001b[A\n",
      "Epoch 108/300:  36%|███▋      | 8/22 [00:00<00:00, 20.51it/s]\u001b[A\n",
      "Epoch 108/300:  50%|█████     | 11/22 [00:00<00:00, 20.58it/s]\u001b[A\n",
      "Epoch 108/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.72it/s]\u001b[A\n",
      "Epoch 108/300:  77%|███████▋  | 17/22 [00:00<00:00, 20.64it/s]\u001b[A\n",
      "Epoch 108/300:  91%|█████████ | 20/22 [00:00<00:00, 20.38it/s]\u001b[A\n",
      " 36%|███▌      | 108/300 [02:11<03:54,  1.22s/it]             \u001b[A\n",
      "Epoch 109/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109/300:  14%|█▎        | 3/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 109/300:  27%|██▋       | 6/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 109/300:  41%|████      | 9/22 [00:00<00:00, 20.79it/s]\u001b[A\n",
      "Epoch 109/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.79it/s]\u001b[A\n",
      "Epoch 109/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
      "Epoch 109/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.44it/s]\u001b[A\n",
      "Epoch 109/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.41it/s]\u001b[A\n",
      " 36%|███▋      | 109/300 [02:13<03:52,  1.22s/it]             \u001b[A\n",
      "Epoch 110/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 110/300:  14%|█▎        | 3/22 [00:00<00:00, 20.51it/s]\u001b[A\n",
      "Epoch 110/300:  27%|██▋       | 6/22 [00:00<00:00, 20.69it/s]\u001b[A\n",
      "Epoch 110/300:  41%|████      | 9/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
      "Epoch 110/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.68it/s]\u001b[A\n",
      "Epoch 110/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      "Epoch 110/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.68it/s]\u001b[A\n",
      "Epoch 110/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.48it/s]\u001b[A\n",
      " 37%|███▋      | 110/300 [02:14<03:51,  1.22s/it]             \u001b[A\n",
      "Epoch 111/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 111/300:   9%|▉         | 2/22 [00:00<00:01, 19.63it/s]\u001b[A\n",
      "Epoch 111/300:  18%|█▊        | 4/22 [00:00<00:00, 19.82it/s]\u001b[A\n",
      "Epoch 111/300:  27%|██▋       | 6/22 [00:00<00:00, 19.43it/s]\u001b[A\n",
      "Epoch 111/300:  41%|████      | 9/22 [00:00<00:00, 19.88it/s]\u001b[A\n",
      "Epoch 111/300:  50%|█████     | 11/22 [00:00<00:00, 19.73it/s]\u001b[A\n",
      "Epoch 111/300:  64%|██████▎   | 14/22 [00:00<00:00, 20.31it/s]\u001b[A\n",
      "Epoch 111/300:  77%|███████▋  | 17/22 [00:00<00:00, 20.01it/s]\u001b[A\n",
      "Epoch 111/300:  91%|█████████ | 20/22 [00:00<00:00, 20.48it/s]\u001b[A\n",
      " 37%|███▋      | 111/300 [02:15<03:51,  1.22s/it]             \u001b[A\n",
      "Epoch 112/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 112/300:  14%|█▎        | 3/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 112/300:  27%|██▋       | 6/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 112/300:  41%|████      | 9/22 [00:00<00:00, 20.82it/s]\u001b[A\n",
      "Epoch 112/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.76it/s]\u001b[A\n",
      "Epoch 112/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.49it/s]\u001b[A\n",
      "Epoch 112/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.10it/s]\u001b[A\n",
      "Epoch 112/300:  95%|█████████▌| 21/22 [00:01<00:00, 19.96it/s]\u001b[A\n",
      " 37%|███▋      | 112/300 [02:16<03:50,  1.23s/it]             \u001b[A\n",
      "Epoch 113/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 113/300:  14%|█▎        | 3/22 [00:00<00:00, 19.91it/s]\u001b[A\n",
      "Epoch 113/300:  27%|██▋       | 6/22 [00:00<00:00, 20.70it/s]\u001b[A\n",
      "Epoch 113/300:  41%|████      | 9/22 [00:00<00:00, 20.27it/s]\u001b[A\n",
      "Epoch 113/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.06it/s]\u001b[A\n",
      "Epoch 113/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.18it/s]\u001b[A\n",
      "Epoch 113/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.26it/s]\u001b[A\n",
      "Epoch 113/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.46it/s]\u001b[A\n",
      " 38%|███▊      | 113/300 [02:17<03:49,  1.23s/it]             \u001b[A\n",
      "Epoch 114/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 114/300:  14%|█▎        | 3/22 [00:00<00:00, 21.06it/s]\u001b[A\n",
      "Epoch 114/300:  27%|██▋       | 6/22 [00:00<00:00, 19.85it/s]\u001b[A\n",
      "Epoch 114/300:  41%|████      | 9/22 [00:00<00:00, 19.91it/s]\u001b[A\n",
      "Epoch 114/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.18it/s]\u001b[A\n",
      "Epoch 114/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.08it/s]\u001b[A\n",
      "Epoch 114/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.29it/s]\u001b[A\n",
      "Epoch 114/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.39it/s]\u001b[A\n",
      " 38%|███▊      | 114/300 [02:19<03:48,  1.23s/it]             \u001b[A\n",
      "Epoch 115/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 115/300:   9%|▉         | 2/22 [00:00<00:01, 18.60it/s]\u001b[A\n",
      "Epoch 115/300:  18%|█▊        | 4/22 [00:00<00:00, 19.21it/s]\u001b[A\n",
      "Epoch 115/300:  32%|███▏      | 7/22 [00:00<00:00, 19.93it/s]\u001b[A\n",
      "Epoch 115/300:  41%|████      | 9/22 [00:00<00:00, 19.81it/s]\u001b[A\n",
      "Epoch 115/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.39it/s]\u001b[A\n",
      "Epoch 115/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.38it/s]\u001b[A\n",
      "Epoch 115/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.56it/s]\u001b[A\n",
      "Epoch 115/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.34it/s]\u001b[A\n",
      " 38%|███▊      | 115/300 [02:20<03:47,  1.23s/it]             \u001b[A\n",
      "Epoch 116/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 116/300:  14%|█▎        | 3/22 [00:00<00:00, 20.43it/s]\u001b[A\n",
      "Epoch 116/300:  27%|██▋       | 6/22 [00:00<00:00, 20.20it/s]\u001b[A\n",
      "Epoch 116/300:  41%|████      | 9/22 [00:00<00:00, 20.33it/s]\u001b[A\n",
      "Epoch 116/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.21it/s]\u001b[A\n",
      "Epoch 116/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.39it/s]\u001b[A\n",
      "Epoch 116/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.51it/s]\u001b[A\n",
      "Epoch 116/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.36it/s]\u001b[A\n",
      " 39%|███▊      | 116/300 [02:21<03:46,  1.23s/it]             \u001b[A\n",
      "Epoch 117/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 117/300:  14%|█▎        | 3/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 117/300:  27%|██▋       | 6/22 [00:00<00:00, 20.69it/s]\u001b[A\n",
      "Epoch 117/300:  41%|████      | 9/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
      "Epoch 117/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.36it/s]\u001b[A\n",
      "Epoch 117/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.36it/s]\u001b[A\n",
      "Epoch 117/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.41it/s]\u001b[A\n",
      "Epoch 117/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.53it/s]\u001b[A\n",
      " 39%|███▉      | 117/300 [02:22<03:44,  1.23s/it]             \u001b[A\n",
      "Loading graphs: 100%|██████████| 1000/1000 [02:24<00:00,  6.93it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]\n",
      "Epoch 1/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1/300:  14%|█▎        | 3/22 [00:00<00:00, 22.48it/s]\u001b[A\n",
      "Epoch 1/300:  27%|██▋       | 6/22 [00:00<00:00, 22.23it/s]\u001b[A\n",
      "Epoch 1/300:  41%|████      | 9/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 1/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 1/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 1/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 1/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "  0%|          | 1/300 [00:01<05:53,  1.18s/it]             \u001b[A\n",
      "Epoch 2/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2/300:  14%|█▎        | 3/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 2/300:  27%|██▋       | 6/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 2/300:  41%|████      | 9/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 2/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 2/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 2/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 2/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "  1%|          | 2/300 [00:02<05:49,  1.17s/it]             \u001b[A\n",
      "Epoch 3/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3/300:  14%|█▎        | 3/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 3/300:  27%|██▋       | 6/22 [00:00<00:00, 21.13it/s]\u001b[A\n",
      "Epoch 3/300:  41%|████      | 9/22 [00:00<00:00, 20.85it/s]\u001b[A\n",
      "Epoch 3/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 3/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 3/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 3/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "  1%|          | 3/300 [00:03<05:47,  1.17s/it]             \u001b[A\n",
      "Epoch 4/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4/300:  14%|█▎        | 3/22 [00:00<00:00, 22.50it/s]\u001b[A\n",
      "Epoch 4/300:  27%|██▋       | 6/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 4/300:  41%|████      | 9/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 4/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 4/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 4/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 4/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "  1%|▏         | 4/300 [00:04<05:45,  1.17s/it]             \u001b[A\n",
      "Epoch 5/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5/300:  14%|█▎        | 3/22 [00:00<00:00, 22.55it/s]\u001b[A\n",
      "Epoch 5/300:  27%|██▋       | 6/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 5/300:  41%|████      | 9/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 5/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 5/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.31it/s]\u001b[A\n",
      "Epoch 5/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 5/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "  2%|▏         | 5/300 [00:05<05:43,  1.16s/it]             \u001b[A\n",
      "Epoch 6/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6/300:  14%|█▎        | 3/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 6/300:  27%|██▋       | 6/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 6/300:  41%|████      | 9/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 6/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 6/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 6/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 6/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "  2%|▏         | 6/300 [00:07<05:43,  1.17s/it]             \u001b[A\n",
      "Epoch 7/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7/300:  14%|█▎        | 3/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 7/300:  27%|██▋       | 6/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 7/300:  41%|████      | 9/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 7/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 7/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 7/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 7/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "  2%|▏         | 7/300 [00:08<05:43,  1.17s/it]             \u001b[A\n",
      "Epoch 8/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8/300:  14%|█▎        | 3/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 8/300:  27%|██▋       | 6/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 8/300:  41%|████      | 9/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 8/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.23it/s]\u001b[A\n",
      "Epoch 8/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.20it/s]\u001b[A\n",
      "Epoch 8/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 8/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "  3%|▎         | 8/300 [00:09<05:39,  1.16s/it]             \u001b[A\n",
      "Epoch 9/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9/300:  14%|█▎        | 3/22 [00:00<00:00, 22.01it/s]\u001b[A\n",
      "Epoch 9/300:  27%|██▋       | 6/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 9/300:  41%|████      | 9/22 [00:00<00:00, 22.02it/s]\u001b[A\n",
      "Epoch 9/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 9/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.15it/s]\u001b[A\n",
      "Epoch 9/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.01it/s]\u001b[A\n",
      "Epoch 9/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      "  3%|▎         | 9/300 [00:10<05:36,  1.16s/it]             \u001b[A\n",
      "Epoch 10/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10/300:  14%|█▎        | 3/22 [00:00<00:00, 23.67it/s]\u001b[A\n",
      "Epoch 10/300:  27%|██▋       | 6/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 10/300:  41%|████      | 9/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 10/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 10/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 10/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 10/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "  3%|▎         | 10/300 [00:11<05:35,  1.16s/it]             \u001b[A\n",
      "Epoch 11/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11/300:  14%|█▎        | 3/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 11/300:  27%|██▋       | 6/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 11/300:  41%|████      | 9/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 11/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.83it/s]\u001b[A\n",
      "Epoch 11/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.87it/s]\u001b[A\n",
      "Epoch 11/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
      "Epoch 11/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      "  4%|▎         | 11/300 [00:12<05:36,  1.16s/it]             \u001b[A\n",
      "Epoch 12/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12/300:  14%|█▎        | 3/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 12/300:  27%|██▋       | 6/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 12/300:  41%|████      | 9/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 12/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 12/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 12/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 12/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "  4%|▍         | 12/300 [00:13<05:36,  1.17s/it]             \u001b[A\n",
      "Epoch 13/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13/300:  14%|█▎        | 3/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 13/300:  27%|██▋       | 6/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 13/300:  41%|████      | 9/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 13/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 13/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 13/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.06it/s]\u001b[A\n",
      "Epoch 13/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "  4%|▍         | 13/300 [00:15<05:35,  1.17s/it]             \u001b[A\n",
      "Epoch 14/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14/300:  14%|█▎        | 3/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 14/300:  27%|██▋       | 6/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 14/300:  41%|████      | 9/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
      "Epoch 14/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 14/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 14/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 14/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "  5%|▍         | 14/300 [00:16<05:34,  1.17s/it]             \u001b[A\n",
      "Epoch 15/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15/300:  14%|█▎        | 3/22 [00:00<00:00, 22.48it/s]\u001b[A\n",
      "Epoch 15/300:  27%|██▋       | 6/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 15/300:  41%|████      | 9/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 15/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 15/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 15/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 15/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "  5%|▌         | 15/300 [00:17<05:34,  1.17s/it]             \u001b[A\n",
      "Epoch 16/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16/300:  14%|█▎        | 3/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 16/300:  27%|██▋       | 6/22 [00:00<00:00, 20.82it/s]\u001b[A\n",
      "Epoch 16/300:  41%|████      | 9/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 16/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 16/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 16/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 16/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "  5%|▌         | 16/300 [00:18<05:31,  1.17s/it]             \u001b[A\n",
      "Epoch 17/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17/300:  14%|█▎        | 3/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 17/300:  27%|██▋       | 6/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 17/300:  41%|████      | 9/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 17/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.21it/s]\u001b[A\n",
      "Epoch 17/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 17/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 17/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "  6%|▌         | 17/300 [00:19<05:29,  1.17s/it]             \u001b[A\n",
      "Epoch 18/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18/300:  14%|█▎        | 3/22 [00:00<00:00, 22.59it/s]\u001b[A\n",
      "Epoch 18/300:  27%|██▋       | 6/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 18/300:  41%|████      | 9/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 18/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 18/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 18/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 18/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "  6%|▌         | 18/300 [00:20<05:27,  1.16s/it]             \u001b[A\n",
      "Epoch 19/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19/300:  14%|█▎        | 3/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 19/300:  27%|██▋       | 6/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 19/300:  41%|████      | 9/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 19/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 19/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 19/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 19/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "  6%|▋         | 19/300 [00:22<05:26,  1.16s/it]             \u001b[A\n",
      "Epoch 20/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20/300:  14%|█▎        | 3/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 20/300:  27%|██▋       | 6/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 20/300:  41%|████      | 9/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 20/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 20/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.78it/s]\u001b[A\n",
      "Epoch 20/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 20/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.97it/s]\u001b[A\n",
      "  7%|▋         | 20/300 [00:23<05:24,  1.16s/it]             \u001b[A\n",
      "Epoch 21/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21/300:  14%|█▎        | 3/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 21/300:  27%|██▋       | 6/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 21/300:  41%|████      | 9/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
      "Epoch 21/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 21/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 21/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 21/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "  7%|▋         | 21/300 [00:24<05:24,  1.16s/it]             \u001b[A\n",
      "Epoch 22/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22/300:  14%|█▎        | 3/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 22/300:  27%|██▋       | 6/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 22/300:  41%|████      | 9/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 22/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 22/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 22/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 22/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "  7%|▋         | 22/300 [00:25<05:23,  1.16s/it]             \u001b[A\n",
      "Epoch 23/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23/300:  14%|█▎        | 3/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 23/300:  27%|██▋       | 6/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 23/300:  41%|████      | 9/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
      "Epoch 23/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 23/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 23/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 23/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "  8%|▊         | 23/300 [00:26<05:24,  1.17s/it]             \u001b[A\n",
      "Epoch 24/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24/300:  14%|█▎        | 3/22 [00:00<00:00, 22.89it/s]\u001b[A\n",
      "Epoch 24/300:  27%|██▋       | 6/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 24/300:  41%|████      | 9/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 24/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 24/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 24/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 24/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "  8%|▊         | 24/300 [00:27<05:22,  1.17s/it]             \u001b[A\n",
      "Epoch 25/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25/300:  14%|█▎        | 3/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 25/300:  27%|██▋       | 6/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
      "Epoch 25/300:  41%|████      | 9/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 25/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 25/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 25/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 25/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "  8%|▊         | 25/300 [00:29<05:21,  1.17s/it]             \u001b[A\n",
      "Epoch 26/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26/300:  14%|█▎        | 3/22 [00:00<00:00, 20.53it/s]\u001b[A\n",
      "Epoch 26/300:  27%|██▋       | 6/22 [00:00<00:00, 20.93it/s]\u001b[A\n",
      "Epoch 26/300:  41%|████      | 9/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 26/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 26/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 26/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 26/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "  9%|▊         | 26/300 [00:30<05:19,  1.17s/it]             \u001b[A\n",
      "Epoch 27/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27/300:  14%|█▎        | 3/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "Epoch 27/300:  27%|██▋       | 6/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 27/300:  41%|████      | 9/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 27/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.02it/s]\u001b[A\n",
      "Epoch 27/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 27/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "Epoch 27/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "  9%|▉         | 27/300 [00:31<05:17,  1.16s/it]             \u001b[A\n",
      "Epoch 28/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28/300:  14%|█▎        | 3/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 28/300:  27%|██▋       | 6/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 28/300:  41%|████      | 9/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 28/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.11it/s]\u001b[A\n",
      "Epoch 28/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 28/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 28/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "  9%|▉         | 28/300 [00:32<05:16,  1.16s/it]             \u001b[A\n",
      "Epoch 29/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29/300:  14%|█▎        | 3/22 [00:00<00:00, 22.02it/s]\u001b[A\n",
      "Epoch 29/300:  27%|██▋       | 6/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 29/300:  41%|████      | 9/22 [00:00<00:00, 20.89it/s]\u001b[A\n",
      "Epoch 29/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 29/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 29/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 29/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      " 10%|▉         | 29/300 [00:33<05:15,  1.17s/it]             \u001b[A\n",
      "Epoch 30/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 30/300:  14%|█▎        | 3/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 30/300:  27%|██▋       | 6/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 30/300:  41%|████      | 9/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 30/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 30/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 30/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      "Epoch 30/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      " 10%|█         | 30/300 [00:34<05:15,  1.17s/it]             \u001b[A\n",
      "Epoch 31/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 31/300:  14%|█▎        | 3/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 31/300:  27%|██▋       | 6/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
      "Epoch 31/300:  41%|████      | 9/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 31/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 31/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 31/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 31/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      " 10%|█         | 31/300 [00:36<05:14,  1.17s/it]             \u001b[A\n",
      "Epoch 32/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 32/300:  14%|█▎        | 3/22 [00:00<00:00, 22.63it/s]\u001b[A\n",
      "Epoch 32/300:  27%|██▋       | 6/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 32/300:  41%|████      | 9/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 32/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 32/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 32/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 32/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      " 11%|█         | 32/300 [00:37<05:12,  1.17s/it]             \u001b[A\n",
      "Epoch 33/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 33/300:  14%|█▎        | 3/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 33/300:  27%|██▋       | 6/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 33/300:  41%|████      | 9/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 33/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 33/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 33/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 33/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      " 11%|█         | 33/300 [00:38<05:10,  1.16s/it]             \u001b[A\n",
      "Epoch 34/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 34/300:  14%|█▎        | 3/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 34/300:  27%|██▋       | 6/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 34/300:  41%|████      | 9/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 34/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.02it/s]\u001b[A\n",
      "Epoch 34/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 34/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 34/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      " 11%|█▏        | 34/300 [00:39<05:10,  1.17s/it]             \u001b[A\n",
      "Epoch 35/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 35/300:  14%|█▎        | 3/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 35/300:  27%|██▋       | 6/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 35/300:  41%|████      | 9/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 35/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.98it/s]\u001b[A\n",
      "Epoch 35/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 35/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 35/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      " 12%|█▏        | 35/300 [00:40<05:09,  1.17s/it]             \u001b[A\n",
      "Epoch 36/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 36/300:  14%|█▎        | 3/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 36/300:  27%|██▋       | 6/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 36/300:  41%|████      | 9/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 36/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 36/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 36/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 36/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      " 12%|█▏        | 36/300 [00:41<05:07,  1.17s/it]             \u001b[A\n",
      "Epoch 37/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 37/300:  14%|█▎        | 3/22 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "Epoch 37/300:  27%|██▋       | 6/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 37/300:  41%|████      | 9/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 37/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 37/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 37/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 37/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      " 12%|█▏        | 37/300 [00:43<05:05,  1.16s/it]             \u001b[A\n",
      "Epoch 38/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 38/300:  14%|█▎        | 3/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 38/300:  27%|██▋       | 6/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 38/300:  41%|████      | 9/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 38/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 38/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 38/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.72it/s]\u001b[A\n",
      "Epoch 38/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      " 13%|█▎        | 38/300 [00:44<05:05,  1.17s/it]             \u001b[A\n",
      "Epoch 39/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39/300:  14%|█▎        | 3/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 39/300:  27%|██▋       | 6/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
      "Epoch 39/300:  41%|████      | 9/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
      "Epoch 39/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 39/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 39/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 39/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      " 13%|█▎        | 39/300 [00:45<05:04,  1.17s/it]             \u001b[A\n",
      "Epoch 40/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 40/300:  14%|█▎        | 3/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 40/300:  27%|██▋       | 6/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 40/300:  41%|████      | 9/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 40/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.40it/s]\u001b[A\n",
      "Epoch 40/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 40/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 40/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      " 13%|█▎        | 40/300 [00:46<05:02,  1.16s/it]             \u001b[A\n",
      "Epoch 41/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 41/300:  14%|█▎        | 3/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 41/300:  27%|██▋       | 6/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 41/300:  41%|████      | 9/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 41/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 41/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 41/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 41/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      " 14%|█▎        | 41/300 [00:47<05:02,  1.17s/it]             \u001b[A\n",
      "Epoch 42/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 42/300:  14%|█▎        | 3/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 42/300:  27%|██▋       | 6/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 42/300:  41%|████      | 9/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 42/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 42/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 42/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 42/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      " 14%|█▍        | 42/300 [00:48<05:01,  1.17s/it]             \u001b[A\n",
      "Epoch 43/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 43/300:  14%|█▎        | 3/22 [00:00<00:00, 20.79it/s]\u001b[A\n",
      "Epoch 43/300:  27%|██▋       | 6/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
      "Epoch 43/300:  41%|████      | 9/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 43/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 43/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
      "Epoch 43/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 43/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      " 14%|█▍        | 43/300 [00:50<05:00,  1.17s/it]             \u001b[A\n",
      "Epoch 44/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 44/300:  14%|█▎        | 3/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 44/300:  27%|██▋       | 6/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 44/300:  41%|████      | 9/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 44/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 44/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 44/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 44/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      " 15%|█▍        | 44/300 [00:51<04:58,  1.17s/it]             \u001b[A\n",
      "Epoch 45/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 45/300:  14%|█▎        | 3/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      "Epoch 45/300:  27%|██▋       | 6/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 45/300:  41%|████      | 9/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 45/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 45/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 45/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 45/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      " 15%|█▌        | 45/300 [00:52<04:57,  1.17s/it]             \u001b[A\n",
      "Epoch 46/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 46/300:  14%|█▎        | 3/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 46/300:  27%|██▋       | 6/22 [00:00<00:00, 21.03it/s]\u001b[A\n",
      "Epoch 46/300:  41%|████      | 9/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 46/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 46/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 46/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 46/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      " 15%|█▌        | 46/300 [00:53<04:56,  1.17s/it]             \u001b[A\n",
      "Epoch 47/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 47/300:  14%|█▎        | 3/22 [00:00<00:00, 22.40it/s]\u001b[A\n",
      "Epoch 47/300:  27%|██▋       | 6/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 47/300:  41%|████      | 9/22 [00:00<00:00, 22.47it/s]\u001b[A\n",
      "Epoch 47/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.18it/s]\u001b[A\n",
      "Epoch 47/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 47/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 47/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      " 16%|█▌        | 47/300 [00:54<04:54,  1.16s/it]             \u001b[A\n",
      "Epoch 48/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 48/300:  14%|█▎        | 3/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 48/300:  27%|██▋       | 6/22 [00:00<00:00, 22.74it/s]\u001b[A\n",
      "Epoch 48/300:  41%|████      | 9/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 48/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 48/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 48/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 48/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      " 16%|█▌        | 48/300 [00:55<04:52,  1.16s/it]             \u001b[A\n",
      "Epoch 49/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49/300:  14%|█▎        | 3/22 [00:00<00:00, 22.51it/s]\u001b[A\n",
      "Epoch 49/300:  27%|██▋       | 6/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 49/300:  41%|████      | 9/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 49/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.93it/s]\u001b[A\n",
      "Epoch 49/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 49/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      "Epoch 49/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      " 16%|█▋        | 49/300 [00:57<04:52,  1.16s/it]             \u001b[A\n",
      "Epoch 50/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 50/300:  14%|█▎        | 3/22 [00:00<00:00, 22.03it/s]\u001b[A\n",
      "Epoch 50/300:  27%|██▋       | 6/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 50/300:  41%|████      | 9/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 50/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 50/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 50/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 50/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      " 17%|█▋        | 50/300 [00:58<04:50,  1.16s/it]             \u001b[A\n",
      "Epoch 51/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 51/300:  14%|█▎        | 3/22 [00:00<00:00, 22.29it/s]\u001b[A\n",
      "Epoch 51/300:  27%|██▋       | 6/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 51/300:  41%|████      | 9/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 51/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 51/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.03it/s]\u001b[A\n",
      "Epoch 51/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 51/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      " 17%|█▋        | 51/300 [00:59<04:50,  1.17s/it]             \u001b[A\n",
      "Epoch 52/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 52/300:  14%|█▎        | 3/22 [00:00<00:00, 22.01it/s]\u001b[A\n",
      "Epoch 52/300:  27%|██▋       | 6/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 52/300:  41%|████      | 9/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 52/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 52/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 52/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 52/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      " 17%|█▋        | 52/300 [01:00<04:49,  1.17s/it]             \u001b[A\n",
      "Epoch 53/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 53/300:  14%|█▎        | 3/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 53/300:  27%|██▋       | 6/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 53/300:  41%|████      | 9/22 [00:00<00:00, 22.22it/s]\u001b[A\n",
      "Epoch 53/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 53/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 53/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 53/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      " 18%|█▊        | 53/300 [01:01<04:48,  1.17s/it]             \u001b[A\n",
      "Epoch 54/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 54/300:  14%|█▎        | 3/22 [00:00<00:00, 22.32it/s]\u001b[A\n",
      "Epoch 54/300:  27%|██▋       | 6/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 54/300:  41%|████      | 9/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 54/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 54/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 54/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 54/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      " 18%|█▊        | 54/300 [01:02<04:46,  1.16s/it]             \u001b[A\n",
      "Epoch 55/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 55/300:  14%|█▎        | 3/22 [00:00<00:00, 22.35it/s]\u001b[A\n",
      "Epoch 55/300:  27%|██▋       | 6/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 55/300:  41%|████      | 9/22 [00:00<00:00, 21.98it/s]\u001b[A\n",
      "Epoch 55/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 55/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      "Epoch 55/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 55/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      " 18%|█▊        | 55/300 [01:04<04:45,  1.17s/it]             \u001b[A\n",
      "Epoch 56/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 56/300:  14%|█▎        | 3/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 56/300:  27%|██▋       | 6/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 56/300:  41%|████      | 9/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 56/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 56/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 56/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 56/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      " 19%|█▊        | 56/300 [01:05<04:44,  1.17s/it]             \u001b[A\n",
      "Epoch 57/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 57/300:  14%|█▎        | 3/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 57/300:  27%|██▋       | 6/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 57/300:  41%|████      | 9/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 57/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.06it/s]\u001b[A\n",
      "Epoch 57/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.80it/s]\u001b[A\n",
      "Epoch 57/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.02it/s]\u001b[A\n",
      "Epoch 57/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.02it/s]\u001b[A\n",
      " 19%|█▉        | 57/300 [01:06<04:44,  1.17s/it]             \u001b[A\n",
      "Epoch 58/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 58/300:  14%|█▎        | 3/22 [00:00<00:00, 22.23it/s]\u001b[A\n",
      "Epoch 58/300:  27%|██▋       | 6/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 58/300:  41%|████      | 9/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 58/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 58/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 58/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 58/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      " 19%|█▉        | 58/300 [01:07<04:42,  1.17s/it]             \u001b[A\n",
      "Epoch 59/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59/300:  14%|█▎        | 3/22 [00:00<00:00, 22.77it/s]\u001b[A\n",
      "Epoch 59/300:  27%|██▋       | 6/22 [00:00<00:00, 22.39it/s]\u001b[A\n",
      "Epoch 59/300:  41%|████      | 9/22 [00:00<00:00, 22.29it/s]\u001b[A\n",
      "Epoch 59/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 59/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 59/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 59/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      " 20%|█▉        | 59/300 [01:08<04:40,  1.17s/it]             \u001b[A\n",
      "Epoch 60/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 60/300:  14%|█▎        | 3/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 60/300:  27%|██▋       | 6/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 60/300:  41%|████      | 9/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
      "Epoch 60/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 60/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 60/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 60/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      " 20%|██        | 60/300 [01:09<04:39,  1.16s/it]             \u001b[A\n",
      "Epoch 61/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 61/300:  14%|█▎        | 3/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 61/300:  27%|██▋       | 6/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 61/300:  41%|████      | 9/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 61/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 61/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 61/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 61/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      " 20%|██        | 61/300 [01:11<04:38,  1.16s/it]             \u001b[A\n",
      "Epoch 62/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 62/300:  14%|█▎        | 3/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 62/300:  27%|██▋       | 6/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 62/300:  41%|████      | 9/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 62/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 62/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 62/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 62/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      " 21%|██        | 62/300 [01:12<04:35,  1.16s/it]             \u001b[A\n",
      "Epoch 63/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 63/300:  14%|█▎        | 3/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 63/300:  27%|██▋       | 6/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "Epoch 63/300:  41%|████      | 9/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 63/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      "Epoch 63/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 63/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 63/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      " 21%|██        | 63/300 [01:13<04:35,  1.16s/it]             \u001b[A\n",
      "Epoch 64/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 64/300:  14%|█▎        | 3/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 64/300:  27%|██▋       | 6/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 64/300:  41%|████      | 9/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 64/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 64/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 64/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 64/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      " 21%|██▏       | 64/300 [01:14<04:34,  1.16s/it]             \u001b[A\n",
      "Epoch 65/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 65/300:  14%|█▎        | 3/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 65/300:  27%|██▋       | 6/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 65/300:  41%|████      | 9/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 65/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 65/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 65/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 65/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      " 22%|██▏       | 65/300 [01:15<04:33,  1.16s/it]             \u001b[A\n",
      "Epoch 66/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 66/300:  14%|█▎        | 3/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 66/300:  27%|██▋       | 6/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 66/300:  41%|████      | 9/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
      "Epoch 66/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 66/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 66/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 66/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      " 22%|██▏       | 66/300 [01:16<04:32,  1.17s/it]             \u001b[A\n",
      "Epoch 67/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 67/300:  14%|█▎        | 3/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 67/300:  27%|██▋       | 6/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 67/300:  41%|████      | 9/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 67/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 67/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 67/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.03it/s]\u001b[A\n",
      "Epoch 67/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      " 22%|██▏       | 67/300 [01:18<04:32,  1.17s/it]             \u001b[A\n",
      "Epoch 68/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 68/300:  14%|█▎        | 3/22 [00:00<00:00, 20.76it/s]\u001b[A\n",
      "Epoch 68/300:  27%|██▋       | 6/22 [00:00<00:00, 20.82it/s]\u001b[A\n",
      "Epoch 68/300:  41%|████      | 9/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
      "Epoch 68/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 68/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 68/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 68/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      " 23%|██▎       | 68/300 [01:19<04:31,  1.17s/it]             \u001b[A\n",
      "Epoch 69/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69/300:  14%|█▎        | 3/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      "Epoch 69/300:  27%|██▋       | 6/22 [00:00<00:00, 20.97it/s]\u001b[A\n",
      "Epoch 69/300:  41%|████      | 9/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 69/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 69/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.97it/s]\u001b[A\n",
      "Epoch 69/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "Epoch 69/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.07it/s]\u001b[A\n",
      " 23%|██▎       | 69/300 [01:20<04:29,  1.17s/it]             \u001b[A\n",
      "Epoch 70/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 70/300:  14%|█▎        | 3/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 70/300:  27%|██▋       | 6/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 70/300:  41%|████      | 9/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 70/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 70/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 70/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 70/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      " 23%|██▎       | 70/300 [01:21<04:27,  1.16s/it]             \u001b[A\n",
      "Epoch 71/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 71/300:  14%|█▎        | 3/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "Epoch 71/300:  27%|██▋       | 6/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 71/300:  41%|████      | 9/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 71/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 71/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 71/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 71/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      " 24%|██▎       | 71/300 [01:22<04:25,  1.16s/it]             \u001b[A\n",
      "Epoch 72/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 72/300:  14%|█▎        | 3/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 72/300:  27%|██▋       | 6/22 [00:00<00:00, 20.53it/s]\u001b[A\n",
      "Epoch 72/300:  41%|████      | 9/22 [00:00<00:00, 20.86it/s]\u001b[A\n",
      "Epoch 72/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 72/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 72/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 72/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      " 24%|██▍       | 72/300 [01:23<04:24,  1.16s/it]             \u001b[A\n",
      "Epoch 73/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 73/300:  14%|█▎        | 3/22 [00:00<00:00, 20.64it/s]\u001b[A\n",
      "Epoch 73/300:  27%|██▋       | 6/22 [00:00<00:00, 20.65it/s]\u001b[A\n",
      "Epoch 73/300:  41%|████      | 9/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 73/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "Epoch 73/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 73/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 73/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      " 24%|██▍       | 73/300 [01:25<04:23,  1.16s/it]             \u001b[A\n",
      "Epoch 74/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 74/300:  14%|█▎        | 3/22 [00:00<00:00, 22.79it/s]\u001b[A\n",
      "Epoch 74/300:  27%|██▋       | 6/22 [00:00<00:00, 22.49it/s]\u001b[A\n",
      "Epoch 74/300:  41%|████      | 9/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 74/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 74/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 74/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 74/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      " 25%|██▍       | 74/300 [01:26<04:22,  1.16s/it]             \u001b[A\n",
      "Epoch 75/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 75/300:  14%|█▎        | 3/22 [00:00<00:00, 20.40it/s]\u001b[A\n",
      "Epoch 75/300:  27%|██▋       | 6/22 [00:00<00:00, 20.71it/s]\u001b[A\n",
      "Epoch 75/300:  41%|████      | 9/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 75/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 75/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 75/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 75/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      " 25%|██▌       | 75/300 [01:27<04:22,  1.17s/it]             \u001b[A\n",
      "Epoch 76/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 76/300:  14%|█▎        | 3/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 76/300:  27%|██▋       | 6/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 76/300:  41%|████      | 9/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 76/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 76/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 76/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 76/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      " 25%|██▌       | 76/300 [01:28<04:21,  1.17s/it]             \u001b[A\n",
      "Epoch 77/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 77/300:  14%|█▎        | 3/22 [00:00<00:00, 23.45it/s]\u001b[A\n",
      "Epoch 77/300:  27%|██▋       | 6/22 [00:00<00:00, 22.58it/s]\u001b[A\n",
      "Epoch 77/300:  41%|████      | 9/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 77/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 77/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 77/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 77/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      " 26%|██▌       | 77/300 [01:29<04:20,  1.17s/it]             \u001b[A\n",
      "Epoch 78/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 78/300:  14%|█▎        | 3/22 [00:00<00:00, 22.53it/s]\u001b[A\n",
      "Epoch 78/300:  27%|██▋       | 6/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 78/300:  41%|████      | 9/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 78/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 78/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 78/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 78/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      " 26%|██▌       | 78/300 [01:30<04:18,  1.16s/it]             \u001b[A\n",
      "Epoch 79/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79/300:  14%|█▎        | 3/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 79/300:  27%|██▋       | 6/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 79/300:  41%|████      | 9/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 79/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 79/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Epoch 79/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      "Epoch 79/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      " 26%|██▋       | 79/300 [01:32<04:18,  1.17s/it]             \u001b[A\n",
      "Epoch 80/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 80/300:  14%|█▎        | 3/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 80/300:  27%|██▋       | 6/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 80/300:  41%|████      | 9/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 80/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 80/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 80/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 80/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      " 27%|██▋       | 80/300 [01:33<04:17,  1.17s/it]             \u001b[A\n",
      "Epoch 81/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 81/300:  14%|█▎        | 3/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 81/300:  27%|██▋       | 6/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 81/300:  41%|████      | 9/22 [00:00<00:00, 20.82it/s]\u001b[A\n",
      "Epoch 81/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
      "Epoch 81/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.83it/s]\u001b[A\n",
      "Epoch 81/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 81/300:  95%|█████████▌| 21/22 [00:01<00:00, 20.88it/s]\u001b[A\n",
      " 27%|██▋       | 81/300 [01:34<04:17,  1.17s/it]             \u001b[A\n",
      "Epoch 82/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 82/300:  14%|█▎        | 3/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 82/300:  27%|██▋       | 6/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 82/300:  41%|████      | 9/22 [00:00<00:00, 22.27it/s]\u001b[A\n",
      "Epoch 82/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 82/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 82/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 82/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      " 27%|██▋       | 82/300 [01:35<04:14,  1.17s/it]             \u001b[A\n",
      "Epoch 83/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 83/300:  14%|█▎        | 3/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 83/300:  27%|██▋       | 6/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 83/300:  41%|████      | 9/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 83/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 83/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 83/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 83/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      " 28%|██▊       | 83/300 [01:36<04:13,  1.17s/it]             \u001b[A\n",
      "Epoch 84/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 84/300:  14%|█▎        | 3/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 84/300:  27%|██▋       | 6/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 84/300:  41%|████      | 9/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 84/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 84/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 84/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 84/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      " 28%|██▊       | 84/300 [01:37<04:12,  1.17s/it]             \u001b[A\n",
      "Epoch 85/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 85/300:  14%|█▎        | 3/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 85/300:  27%|██▋       | 6/22 [00:00<00:00, 20.95it/s]\u001b[A\n",
      "Epoch 85/300:  41%|████      | 9/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 85/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.03it/s]\u001b[A\n",
      "Epoch 85/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.17it/s]\u001b[A\n",
      "Epoch 85/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 85/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      " 28%|██▊       | 85/300 [01:39<04:11,  1.17s/it]             \u001b[A\n",
      "Epoch 86/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 86/300:  14%|█▎        | 3/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 86/300:  27%|██▋       | 6/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 86/300:  41%|████      | 9/22 [00:00<00:00, 22.18it/s]\u001b[A\n",
      "Epoch 86/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 86/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 86/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 86/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      " 29%|██▊       | 86/300 [01:40<04:09,  1.17s/it]             \u001b[A\n",
      "Epoch 87/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 87/300:  14%|█▎        | 3/22 [00:00<00:00, 22.27it/s]\u001b[A\n",
      "Epoch 87/300:  27%|██▋       | 6/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 87/300:  41%|████      | 9/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 87/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 87/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 87/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 87/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      " 29%|██▉       | 87/300 [01:41<04:07,  1.16s/it]             \u001b[A\n",
      "Epoch 88/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 88/300:  14%|█▎        | 3/22 [00:00<00:00, 23.49it/s]\u001b[A\n",
      "Epoch 88/300:  27%|██▋       | 6/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 88/300:  41%|████      | 9/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 88/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 88/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 88/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 88/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      " 29%|██▉       | 88/300 [01:42<04:07,  1.17s/it]             \u001b[A\n",
      "Epoch 89/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89/300:  14%|█▎        | 3/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 89/300:  27%|██▋       | 6/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 89/300:  41%|████      | 9/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 89/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 89/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 89/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 89/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      " 30%|██▉       | 89/300 [01:43<04:06,  1.17s/it]             \u001b[A\n",
      "Epoch 90/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 90/300:  14%|█▎        | 3/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 90/300:  27%|██▋       | 6/22 [00:00<00:00, 22.16it/s]\u001b[A\n",
      "Epoch 90/300:  41%|████      | 9/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 90/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 90/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 90/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 90/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      " 30%|███       | 90/300 [01:44<04:03,  1.16s/it]             \u001b[A\n",
      "Epoch 91/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 91/300:  14%|█▎        | 3/22 [00:00<00:00, 22.98it/s]\u001b[A\n",
      "Epoch 91/300:  27%|██▋       | 6/22 [00:00<00:00, 22.26it/s]\u001b[A\n",
      "Epoch 91/300:  41%|████      | 9/22 [00:00<00:00, 22.41it/s]\u001b[A\n",
      "Epoch 91/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 91/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.17it/s]\u001b[A\n",
      "Epoch 91/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 91/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      " 30%|███       | 91/300 [01:46<04:01,  1.16s/it]             \u001b[A\n",
      "Epoch 92/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 92/300:  14%|█▎        | 3/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 92/300:  27%|██▋       | 6/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 92/300:  41%|████      | 9/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 92/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 92/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 92/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 92/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      " 31%|███       | 92/300 [01:47<04:00,  1.16s/it]             \u001b[A\n",
      "Epoch 93/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 93/300:  14%|█▎        | 3/22 [00:00<00:00, 20.51it/s]\u001b[A\n",
      "Epoch 93/300:  27%|██▋       | 6/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Epoch 93/300:  41%|████      | 9/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 93/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 93/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 93/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 93/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      " 31%|███       | 93/300 [01:48<04:00,  1.16s/it]             \u001b[A\n",
      "Epoch 94/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 94/300:  14%|█▎        | 3/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 94/300:  27%|██▋       | 6/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 94/300:  41%|████      | 9/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 94/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 94/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 94/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 94/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      " 31%|███▏      | 94/300 [01:49<03:59,  1.16s/it]             \u001b[A\n",
      "Epoch 95/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 95/300:  14%|█▎        | 3/22 [00:00<00:00, 22.21it/s]\u001b[A\n",
      "Epoch 95/300:  27%|██▋       | 6/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 95/300:  41%|████      | 9/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 95/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 95/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 95/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 95/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      " 32%|███▏      | 95/300 [01:50<03:58,  1.16s/it]             \u001b[A\n",
      "Epoch 96/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 96/300:  14%|█▎        | 3/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 96/300:  27%|██▋       | 6/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 96/300:  41%|████      | 9/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 96/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 96/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 96/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 96/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      " 32%|███▏      | 96/300 [01:51<03:57,  1.17s/it]             \u001b[A\n",
      "Epoch 97/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 97/300:  14%|█▎        | 3/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 97/300:  27%|██▋       | 6/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 97/300:  41%|████      | 9/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 97/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 97/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 97/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 97/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      " 32%|███▏      | 97/300 [01:53<03:56,  1.16s/it]             \u001b[A\n",
      "Epoch 98/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 98/300:  14%|█▎        | 3/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 98/300:  27%|██▋       | 6/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 98/300:  41%|████      | 9/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 98/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 98/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 98/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 98/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      " 33%|███▎      | 98/300 [01:54<03:55,  1.17s/it]             \u001b[A\n",
      "Epoch 99/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99/300:  14%|█▎        | 3/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 99/300:  27%|██▋       | 6/22 [00:00<00:00, 21.09it/s]\u001b[A\n",
      "Epoch 99/300:  41%|████      | 9/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 99/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 99/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 99/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 99/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      " 33%|███▎      | 99/300 [01:55<03:54,  1.17s/it]             \u001b[A\n",
      "Epoch 100/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 100/300:  14%|█▎        | 3/22 [00:00<00:00, 22.84it/s]\u001b[A\n",
      "Epoch 100/300:  27%|██▋       | 6/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 100/300:  41%|████      | 9/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 100/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 100/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 100/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 100/300:  95%|█████████▌| 21/22 [00:00<00:00, 20.79it/s]\u001b[A\n",
      " 33%|███▎      | 100/300 [01:56<03:54,  1.17s/it]             \u001b[A\n",
      "Epoch 101/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 101/300:  14%|█▎        | 3/22 [00:00<00:00, 22.79it/s]\u001b[A\n",
      "Epoch 101/300:  27%|██▋       | 6/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 101/300:  41%|████      | 9/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 101/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 101/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 101/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 101/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      " 34%|███▎      | 101/300 [01:57<03:52,  1.17s/it]             \u001b[A\n",
      "Epoch 102/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 102/300:  14%|█▎        | 3/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 102/300:  27%|██▋       | 6/22 [00:00<00:00, 22.18it/s]\u001b[A\n",
      "Epoch 102/300:  41%|████      | 9/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 102/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 102/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 102/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 102/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      " 34%|███▍      | 102/300 [01:58<03:50,  1.16s/it]             \u001b[A\n",
      "Epoch 103/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 103/300:  14%|█▎        | 3/22 [00:00<00:00, 20.83it/s]\u001b[A\n",
      "Epoch 103/300:  27%|██▋       | 6/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 103/300:  41%|████      | 9/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 103/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 103/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 103/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 103/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      " 34%|███▍      | 103/300 [02:00<03:49,  1.16s/it]             \u001b[A\n",
      "Epoch 104/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 104/300:  14%|█▎        | 3/22 [00:00<00:00, 23.17it/s]\u001b[A\n",
      "Epoch 104/300:  27%|██▋       | 6/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 104/300:  41%|████      | 9/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 104/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 104/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 104/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 104/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      " 35%|███▍      | 104/300 [02:01<03:47,  1.16s/it]             \u001b[A\n",
      "Epoch 105/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 105/300:  14%|█▎        | 3/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 105/300:  27%|██▋       | 6/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 105/300:  41%|████      | 9/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 105/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 105/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 105/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 105/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      " 35%|███▌      | 105/300 [02:02<03:47,  1.17s/it]             \u001b[A\n",
      "Epoch 106/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 106/300:  14%|█▎        | 3/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 106/300:  27%|██▋       | 6/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 106/300:  41%|████      | 9/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      "Epoch 106/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 106/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 106/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 106/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      " 35%|███▌      | 106/300 [02:03<03:46,  1.17s/it]             \u001b[A\n",
      "Epoch 107/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 107/300:  14%|█▎        | 3/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 107/300:  27%|██▋       | 6/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 107/300:  41%|████      | 9/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 107/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 107/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 107/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 107/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      " 36%|███▌      | 107/300 [02:04<03:45,  1.17s/it]             \u001b[A\n",
      "Epoch 108/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 108/300:  14%|█▎        | 3/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 108/300:  27%|██▋       | 6/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 108/300:  41%|████      | 9/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 108/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 108/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 108/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 108/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      " 36%|███▌      | 108/300 [02:05<03:43,  1.16s/it]             \u001b[A\n",
      "Epoch 109/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 109/300:  14%|█▎        | 3/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 109/300:  27%|██▋       | 6/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 109/300:  41%|████      | 9/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 109/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 109/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 109/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 109/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      " 36%|███▋      | 109/300 [02:07<03:42,  1.16s/it]             \u001b[A\n",
      "Epoch 110/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 110/300:  14%|█▎        | 3/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 110/300:  27%|██▋       | 6/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 110/300:  41%|████      | 9/22 [00:00<00:00, 22.07it/s]\u001b[A\n",
      "Epoch 110/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.44it/s]\u001b[A\n",
      "Epoch 110/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.99it/s]\u001b[A\n",
      "Epoch 110/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.99it/s]\u001b[A\n",
      "Epoch 110/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      " 37%|███▋      | 110/300 [02:08<03:40,  1.16s/it]             \u001b[A\n",
      "Epoch 111/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 111/300:  14%|█▎        | 3/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 111/300:  27%|██▋       | 6/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 111/300:  41%|████      | 9/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 111/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 111/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 111/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 111/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      " 37%|███▋      | 111/300 [02:09<03:39,  1.16s/it]             \u001b[A\n",
      "Epoch 112/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 112/300:  14%|█▎        | 3/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 112/300:  27%|██▋       | 6/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 112/300:  41%|████      | 9/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 112/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 112/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 112/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "Epoch 112/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      " 37%|███▋      | 112/300 [02:10<03:38,  1.16s/it]             \u001b[A\n",
      "Epoch 113/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 113/300:  14%|█▎        | 3/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 113/300:  27%|██▋       | 6/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 113/300:  41%|████      | 9/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 113/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 113/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 113/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 113/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      " 38%|███▊      | 113/300 [02:11<03:37,  1.16s/it]             \u001b[A\n",
      "Epoch 114/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 114/300:  14%|█▎        | 3/22 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "Epoch 114/300:  27%|██▋       | 6/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 114/300:  41%|████      | 9/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 114/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 114/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 114/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.09it/s]\u001b[A\n",
      "Epoch 114/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      " 38%|███▊      | 114/300 [02:12<03:35,  1.16s/it]             \u001b[A\n",
      "Epoch 115/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 115/300:  14%|█▎        | 3/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 115/300:  27%|██▋       | 6/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 115/300:  41%|████      | 9/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 115/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 115/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 115/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 115/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      " 38%|███▊      | 115/300 [02:13<03:34,  1.16s/it]             \u001b[A\n",
      "Epoch 116/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 116/300:  14%|█▎        | 3/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 116/300:  27%|██▋       | 6/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 116/300:  41%|████      | 9/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 116/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 116/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 116/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.32it/s]\u001b[A\n",
      "Epoch 116/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      " 39%|███▊      | 116/300 [02:15<03:34,  1.17s/it]             \u001b[A\n",
      "Epoch 117/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 117/300:  14%|█▎        | 3/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 117/300:  27%|██▋       | 6/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 117/300:  41%|████      | 9/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 117/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 117/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 117/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 117/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      " 39%|███▉      | 117/300 [02:16<03:33,  1.17s/it]             \u001b[A\n",
      "Epoch 118/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 118/300:  14%|█▎        | 3/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 118/300:  27%|██▋       | 6/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      "Epoch 118/300:  41%|████      | 9/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 118/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.13it/s]\u001b[A\n",
      "Epoch 118/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 118/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 118/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      " 39%|███▉      | 118/300 [02:17<03:32,  1.17s/it]             \u001b[A\n",
      "Epoch 119/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 119/300:  14%|█▎        | 3/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 119/300:  27%|██▋       | 6/22 [00:00<00:00, 22.34it/s]\u001b[A\n",
      "Epoch 119/300:  41%|████      | 9/22 [00:00<00:00, 22.37it/s]\u001b[A\n",
      "Epoch 119/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 119/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 119/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 119/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      " 40%|███▉      | 119/300 [02:18<03:30,  1.16s/it]             \u001b[A\n",
      "Epoch 120/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 120/300:  14%|█▎        | 3/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 120/300:  27%|██▋       | 6/22 [00:00<00:00, 20.78it/s]\u001b[A\n",
      "Epoch 120/300:  41%|████      | 9/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      "Epoch 120/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 120/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 120/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 120/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      " 40%|████      | 120/300 [02:19<03:29,  1.16s/it]             \u001b[A\n",
      "Epoch 121/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 121/300:  14%|█▎        | 3/22 [00:00<00:00, 22.64it/s]\u001b[A\n",
      "Epoch 121/300:  27%|██▋       | 6/22 [00:00<00:00, 22.11it/s]\u001b[A\n",
      "Epoch 121/300:  41%|████      | 9/22 [00:00<00:00, 22.11it/s]\u001b[A\n",
      "Epoch 121/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 121/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "Epoch 121/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 121/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      " 40%|████      | 121/300 [02:20<03:28,  1.16s/it]             \u001b[A\n",
      "Epoch 122/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 122/300:  14%|█▎        | 3/22 [00:00<00:00, 22.41it/s]\u001b[A\n",
      "Epoch 122/300:  27%|██▋       | 6/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 122/300:  41%|████      | 9/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 122/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 122/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 122/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      "Epoch 122/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      " 41%|████      | 122/300 [02:22<03:26,  1.16s/it]             \u001b[A\n",
      "Epoch 123/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 123/300:  14%|█▎        | 3/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 123/300:  27%|██▋       | 6/22 [00:00<00:00, 20.86it/s]\u001b[A\n",
      "Epoch 123/300:  41%|████      | 9/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 123/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 123/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 123/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 123/300:  95%|█████████▌| 21/22 [00:00<00:00, 20.90it/s]\u001b[A\n",
      " 41%|████      | 123/300 [02:23<03:26,  1.17s/it]             \u001b[A\n",
      "Epoch 124/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 124/300:  14%|█▎        | 3/22 [00:00<00:00, 22.14it/s]\u001b[A\n",
      "Epoch 124/300:  27%|██▋       | 6/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 124/300:  41%|████      | 9/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 124/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 124/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 124/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 124/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      " 41%|████▏     | 124/300 [02:24<03:24,  1.16s/it]             \u001b[A\n",
      "Epoch 125/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 125/300:  14%|█▎        | 3/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 125/300:  27%|██▋       | 6/22 [00:00<00:00, 20.78it/s]\u001b[A\n",
      "Epoch 125/300:  41%|████      | 9/22 [00:00<00:00, 20.91it/s]\u001b[A\n",
      "Epoch 125/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 125/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 125/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.93it/s]\u001b[A\n",
      "Epoch 125/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      " 42%|████▏     | 125/300 [02:25<03:24,  1.17s/it]             \u001b[A\n",
      "Epoch 126/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 126/300:  14%|█▎        | 3/22 [00:00<00:00, 22.13it/s]\u001b[A\n",
      "Epoch 126/300:  27%|██▋       | 6/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 126/300:  41%|████      | 9/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 126/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 126/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 126/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 126/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      " 42%|████▏     | 126/300 [02:26<03:23,  1.17s/it]             \u001b[A\n",
      "Epoch 127/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 127/300:  14%|█▎        | 3/22 [00:00<00:00, 20.89it/s]\u001b[A\n",
      "Epoch 127/300:  27%|██▋       | 6/22 [00:00<00:00, 21.06it/s]\u001b[A\n",
      "Epoch 127/300:  41%|████      | 9/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 127/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 127/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 127/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 127/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      " 42%|████▏     | 127/300 [02:28<03:22,  1.17s/it]             \u001b[A\n",
      "Epoch 128/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 128/300:  14%|█▎        | 3/22 [00:00<00:00, 20.55it/s]\u001b[A\n",
      "Epoch 128/300:  27%|██▋       | 6/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 128/300:  41%|████      | 9/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 128/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 128/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.17it/s]\u001b[A\n",
      "Epoch 128/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 128/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      " 43%|████▎     | 128/300 [02:29<03:20,  1.16s/it]             \u001b[A\n",
      "Epoch 129/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 129/300:  14%|█▎        | 3/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 129/300:  27%|██▋       | 6/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 129/300:  41%|████      | 9/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 129/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 129/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 129/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 129/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      " 43%|████▎     | 129/300 [02:30<03:18,  1.16s/it]             \u001b[A\n",
      "Epoch 130/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 130/300:  14%|█▎        | 3/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      "Epoch 130/300:  27%|██▋       | 6/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 130/300:  41%|████      | 9/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 130/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 130/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 130/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 130/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      " 43%|████▎     | 130/300 [02:31<03:17,  1.16s/it]             \u001b[A\n",
      "Epoch 131/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 131/300:  14%|█▎        | 3/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 131/300:  27%|██▋       | 6/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 131/300:  41%|████      | 9/22 [00:00<00:00, 22.35it/s]\u001b[A\n",
      "Epoch 131/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 131/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 131/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 131/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      " 44%|████▎     | 131/300 [02:32<03:16,  1.16s/it]             \u001b[A\n",
      "Epoch 132/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 132/300:  14%|█▎        | 3/22 [00:00<00:00, 22.43it/s]\u001b[A\n",
      "Epoch 132/300:  27%|██▋       | 6/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 132/300:  41%|████      | 9/22 [00:00<00:00, 22.07it/s]\u001b[A\n",
      "Epoch 132/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "Epoch 132/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 132/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 132/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      " 44%|████▍     | 132/300 [02:33<03:15,  1.16s/it]             \u001b[A\n",
      "Epoch 133/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 133/300:  14%|█▎        | 3/22 [00:00<00:00, 21.02it/s]\u001b[A\n",
      "Epoch 133/300:  27%|██▋       | 6/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 133/300:  41%|████      | 9/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 133/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.88it/s]\u001b[A\n",
      "Epoch 133/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 133/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 133/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      " 44%|████▍     | 133/300 [02:34<03:14,  1.17s/it]             \u001b[A\n",
      "Epoch 134/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 134/300:  14%|█▎        | 3/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 134/300:  27%|██▋       | 6/22 [00:00<00:00, 22.66it/s]\u001b[A\n",
      "Epoch 134/300:  41%|████      | 9/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 134/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 134/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 134/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "Epoch 134/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      " 45%|████▍     | 134/300 [02:36<03:12,  1.16s/it]             \u001b[A\n",
      "Epoch 135/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 135/300:  14%|█▎        | 3/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 135/300:  27%|██▋       | 6/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 135/300:  41%|████      | 9/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 135/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      "Epoch 135/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 135/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "Epoch 135/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      " 45%|████▌     | 135/300 [02:37<03:12,  1.16s/it]             \u001b[A\n",
      "Epoch 136/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 136/300:  14%|█▎        | 3/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 136/300:  27%|██▋       | 6/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 136/300:  41%|████      | 9/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 136/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 136/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 136/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 136/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      " 45%|████▌     | 136/300 [02:38<03:11,  1.17s/it]             \u001b[A\n",
      "Epoch 137/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 137/300:  14%|█▎        | 3/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 137/300:  27%|██▋       | 6/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 137/300:  41%|████      | 9/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 137/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 137/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 137/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 137/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      " 46%|████▌     | 137/300 [02:39<03:09,  1.17s/it]             \u001b[A\n",
      "Epoch 138/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 138/300:  14%|█▎        | 3/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 138/300:  27%|██▋       | 6/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Epoch 138/300:  41%|████      | 9/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Epoch 138/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 138/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 138/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 138/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.24it/s]\u001b[A\n",
      " 46%|████▌     | 138/300 [02:40<03:08,  1.17s/it]             \u001b[A\n",
      "Epoch 139/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 139/300:  14%|█▎        | 3/22 [00:00<00:00, 22.30it/s]\u001b[A\n",
      "Epoch 139/300:  27%|██▋       | 6/22 [00:00<00:00, 22.21it/s]\u001b[A\n",
      "Epoch 139/300:  41%|████      | 9/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 139/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 139/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 139/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 139/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      " 46%|████▋     | 139/300 [02:41<03:07,  1.16s/it]             \u001b[A\n",
      "Epoch 140/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 140/300:  14%|█▎        | 3/22 [00:00<00:00, 20.79it/s]\u001b[A\n",
      "Epoch 140/300:  27%|██▋       | 6/22 [00:00<00:00, 22.00it/s]\u001b[A\n",
      "Epoch 140/300:  41%|████      | 9/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 140/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 140/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 140/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 140/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      " 47%|████▋     | 140/300 [02:43<03:06,  1.16s/it]             \u001b[A\n",
      "Epoch 141/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 141/300:  14%|█▎        | 3/22 [00:00<00:00, 22.24it/s]\u001b[A\n",
      "Epoch 141/300:  27%|██▋       | 6/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 141/300:  41%|████      | 9/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 141/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.77it/s]\u001b[A\n",
      "Epoch 141/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.59it/s]\u001b[A\n",
      "Epoch 141/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 141/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      " 47%|████▋     | 141/300 [02:44<03:05,  1.17s/it]             \u001b[A\n",
      "Epoch 142/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 142/300:  14%|█▎        | 3/22 [00:00<00:00, 21.98it/s]\u001b[A\n",
      "Epoch 142/300:  27%|██▋       | 6/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 142/300:  41%|████      | 9/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 142/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 142/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 142/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 142/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      " 47%|████▋     | 142/300 [02:45<03:03,  1.16s/it]             \u001b[A\n",
      "Epoch 143/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 143/300:  14%|█▎        | 3/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 143/300:  27%|██▋       | 6/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 143/300:  41%|████      | 9/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 143/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.98it/s]\u001b[A\n",
      "Epoch 143/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.42it/s]\u001b[A\n",
      "Epoch 143/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 143/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.01it/s]\u001b[A\n",
      " 48%|████▊     | 143/300 [02:46<03:01,  1.16s/it]             \u001b[A\n",
      "Epoch 144/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 144/300:  14%|█▎        | 3/22 [00:00<00:00, 23.10it/s]\u001b[A\n",
      "Epoch 144/300:  27%|██▋       | 6/22 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "Epoch 144/300:  41%|████      | 9/22 [00:00<00:00, 22.59it/s]\u001b[A\n",
      "Epoch 144/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.15it/s]\u001b[A\n",
      "Epoch 144/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 144/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 144/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      " 48%|████▊     | 144/300 [02:47<03:00,  1.15s/it]             \u001b[A\n",
      "Epoch 145/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 145/300:  14%|█▎        | 3/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 145/300:  27%|██▋       | 6/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 145/300:  41%|████      | 9/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 145/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 145/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 145/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "Epoch 145/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      " 48%|████▊     | 145/300 [02:48<02:59,  1.16s/it]             \u001b[A\n",
      "Epoch 146/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 146/300:  14%|█▎        | 3/22 [00:00<00:00, 20.97it/s]\u001b[A\n",
      "Epoch 146/300:  27%|██▋       | 6/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 146/300:  41%|████      | 9/22 [00:00<00:00, 21.28it/s]\u001b[A\n",
      "Epoch 146/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 146/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 146/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 146/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      " 49%|████▊     | 146/300 [02:50<02:58,  1.16s/it]             \u001b[A\n",
      "Epoch 147/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 147/300:  14%|█▎        | 3/22 [00:00<00:00, 22.07it/s]\u001b[A\n",
      "Epoch 147/300:  27%|██▋       | 6/22 [00:00<00:00, 22.59it/s]\u001b[A\n",
      "Epoch 147/300:  41%|████      | 9/22 [00:00<00:00, 22.10it/s]\u001b[A\n",
      "Epoch 147/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.98it/s]\u001b[A\n",
      "Epoch 147/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 147/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "Epoch 147/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      " 49%|████▉     | 147/300 [02:51<02:56,  1.16s/it]             \u001b[A\n",
      "Epoch 148/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 148/300:  14%|█▎        | 3/22 [00:00<00:00, 22.27it/s]\u001b[A\n",
      "Epoch 148/300:  27%|██▋       | 6/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 148/300:  41%|████      | 9/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 148/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 148/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 148/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.49it/s]\u001b[A\n",
      "Epoch 148/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      " 49%|████▉     | 148/300 [02:52<02:56,  1.16s/it]             \u001b[A\n",
      "Epoch 149/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 149/300:  14%|█▎        | 3/22 [00:00<00:00, 22.73it/s]\u001b[A\n",
      "Epoch 149/300:  27%|██▋       | 6/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 149/300:  41%|████      | 9/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 149/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 149/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 149/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 149/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      " 50%|████▉     | 149/300 [02:53<02:54,  1.16s/it]             \u001b[A\n",
      "Epoch 150/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 150/300:  14%|█▎        | 3/22 [00:00<00:00, 22.10it/s]\u001b[A\n",
      "Epoch 232/300:  41%|████      | 9/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 232/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.03it/s]\u001b[A\n",
      "Epoch 232/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 232/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 232/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      " 77%|███████▋  | 232/300 [04:30<01:19,  1.17s/it]             \u001b[A\n",
      "Epoch 233/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 233/300:  14%|█▎        | 3/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 233/300:  27%|██▋       | 6/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 233/300:  41%|████      | 9/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 233/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.30it/s]\u001b[A\n",
      "Epoch 233/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 233/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.51it/s]\u001b[A\n",
      "Epoch 233/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      " 78%|███████▊  | 233/300 [04:31<01:17,  1.16s/it]             \u001b[A\n",
      "Epoch 234/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 234/300:  14%|█▎        | 3/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 234/300:  27%|██▋       | 6/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      "Epoch 234/300:  41%|████      | 9/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 234/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 234/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 234/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 234/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      " 78%|███████▊  | 234/300 [04:32<01:16,  1.16s/it]             \u001b[A\n",
      "Epoch 235/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 235/300:  14%|█▎        | 3/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 235/300:  27%|██▋       | 6/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 235/300:  41%|████      | 9/22 [00:00<00:00, 22.32it/s]\u001b[A\n",
      "Epoch 235/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 235/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.67it/s]\u001b[A\n",
      "Epoch 235/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 235/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      " 78%|███████▊  | 235/300 [04:34<01:15,  1.17s/it]             \u001b[A\n",
      "Epoch 236/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 236/300:  14%|█▎        | 3/22 [00:00<00:00, 22.41it/s]\u001b[A\n",
      "Epoch 236/300:  27%|██▋       | 6/22 [00:00<00:00, 22.19it/s]\u001b[A\n",
      "Epoch 236/300:  41%|████      | 9/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 236/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.96it/s]\u001b[A\n",
      "Epoch 236/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.85it/s]\u001b[A\n",
      "Epoch 236/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 236/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      " 79%|███████▊  | 236/300 [04:35<01:14,  1.16s/it]             \u001b[A\n",
      "Epoch 237/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 237/300:  14%|█▎        | 3/22 [00:00<00:00, 20.81it/s]\u001b[A\n",
      "Epoch 237/300:  27%|██▋       | 6/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 237/300:  41%|████      | 9/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 237/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 237/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 237/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 237/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      " 79%|███████▉  | 237/300 [04:36<01:13,  1.16s/it]             \u001b[A\n",
      "Epoch 238/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 238/300:  14%|█▎        | 3/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 238/300:  27%|██▋       | 6/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 238/300:  41%|████      | 9/22 [00:00<00:00, 20.66it/s]\u001b[A\n",
      "Epoch 238/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 238/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 238/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 238/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      " 79%|███████▉  | 238/300 [04:37<01:12,  1.17s/it]             \u001b[A\n",
      "Epoch 239/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 239/300:  14%|█▎        | 3/22 [00:00<00:00, 22.17it/s]\u001b[A\n",
      "Epoch 239/300:  27%|██▋       | 6/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 239/300:  41%|████      | 9/22 [00:00<00:00, 21.12it/s]\u001b[A\n",
      "Epoch 239/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 239/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 239/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 239/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      " 80%|███████▉  | 239/300 [04:38<01:11,  1.17s/it]             \u001b[A\n",
      "Epoch 240/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 240/300:  14%|█▎        | 3/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 240/300:  27%|██▋       | 6/22 [00:00<00:00, 21.78it/s]\u001b[A\n",
      "Epoch 240/300:  41%|████      | 9/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 240/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.01it/s]\u001b[A\n",
      "Epoch 240/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 240/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 240/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      " 80%|████████  | 240/300 [04:39<01:10,  1.17s/it]             \u001b[A\n",
      "Epoch 241/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 241/300:  14%|█▎        | 3/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 241/300:  27%|██▋       | 6/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 241/300:  41%|████      | 9/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 241/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.43it/s]\u001b[A\n",
      "Epoch 241/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.35it/s]\u001b[A\n",
      "Epoch 241/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.38it/s]\u001b[A\n",
      "Epoch 241/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      " 80%|████████  | 241/300 [04:41<01:08,  1.16s/it]             \u001b[A\n",
      "Epoch 242/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 242/300:  14%|█▎        | 3/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 242/300:  27%|██▋       | 6/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 242/300:  41%|████      | 9/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 242/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 242/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 242/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 242/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      " 81%|████████  | 242/300 [04:42<01:07,  1.16s/it]             \u001b[A\n",
      "Epoch 243/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 243/300:  14%|█▎        | 3/22 [00:00<00:00, 22.12it/s]\u001b[A\n",
      "Epoch 243/300:  27%|██▋       | 6/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 243/300:  41%|████      | 9/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 243/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 243/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 243/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 243/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      " 81%|████████  | 243/300 [04:43<01:06,  1.16s/it]             \u001b[A\n",
      "Epoch 244/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 244/300:  14%|█▎        | 3/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 244/300:  27%|██▋       | 6/22 [00:00<00:00, 21.52it/s]\u001b[A\n",
      "Epoch 244/300:  41%|████      | 9/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      "Epoch 244/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 244/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 244/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 244/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      " 81%|████████▏ | 244/300 [04:44<01:05,  1.17s/it]             \u001b[A\n",
      "Epoch 245/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 245/300:  14%|█▎        | 3/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 245/300:  27%|██▋       | 6/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 245/300:  41%|████      | 9/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 245/300:  55%|█████▍    | 12/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 245/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 245/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 245/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      " 82%|████████▏ | 245/300 [04:45<01:04,  1.17s/it]             \u001b[A\n",
      "Epoch 246/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 246/300:  14%|█▎        | 3/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 246/300:  27%|██▋       | 6/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 246/300:  41%|████      | 9/22 [00:00<00:00, 21.34it/s]\u001b[A\n",
      "Epoch 246/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 246/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 246/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.42it/s]\u001b[A\n",
      "Epoch 246/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      " 82%|████████▏ | 246/300 [04:46<01:03,  1.17s/it]             \u001b[A\n",
      "Epoch 247/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 247/300:  14%|█▎        | 3/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 247/300:  27%|██▋       | 6/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 247/300:  41%|████      | 9/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 247/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 247/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 247/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "Epoch 247/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.17it/s]\u001b[A\n",
      " 82%|████████▏ | 247/300 [04:48<01:02,  1.17s/it]             \u001b[A\n",
      "Epoch 248/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 248/300:  14%|█▎        | 3/22 [00:00<00:00, 22.20it/s]\u001b[A\n",
      "Epoch 248/300:  27%|██▋       | 6/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 248/300:  41%|████      | 9/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 248/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 248/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 248/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 248/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      " 83%|████████▎ | 248/300 [04:49<01:01,  1.17s/it]             \u001b[A\n",
      "Epoch 249/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 249/300:  14%|█▎        | 3/22 [00:00<00:00, 21.99it/s]\u001b[A\n",
      "Epoch 249/300:  27%|██▋       | 6/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 249/300:  41%|████      | 9/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 249/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 249/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "Epoch 249/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 249/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      " 83%|████████▎ | 249/300 [04:50<00:59,  1.17s/it]             \u001b[A\n",
      "Epoch 250/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 250/300:  14%|█▎        | 3/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 250/300:  27%|██▋       | 6/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 250/300:  41%|████      | 9/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 250/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 250/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 250/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 250/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      " 83%|████████▎ | 250/300 [04:51<00:58,  1.17s/it]             \u001b[A\n",
      "Epoch 251/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 251/300:  14%|█▎        | 3/22 [00:00<00:00, 22.29it/s]\u001b[A\n",
      "Epoch 251/300:  27%|██▋       | 6/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 251/300:  41%|████      | 9/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 251/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.99it/s]\u001b[A\n",
      "Epoch 251/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 251/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 251/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.08it/s]\u001b[A\n",
      " 84%|████████▎ | 251/300 [04:52<00:57,  1.17s/it]             \u001b[A\n",
      "Epoch 252/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 252/300:  14%|█▎        | 3/22 [00:00<00:00, 20.61it/s]\u001b[A\n",
      "Epoch 252/300:  27%|██▋       | 6/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "Epoch 252/300:  41%|████      | 9/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      "Epoch 252/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 252/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 252/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.84it/s]\u001b[A\n",
      "Epoch 252/300:  95%|█████████▌| 21/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      " 84%|████████▍ | 252/300 [04:53<00:56,  1.18s/it]             \u001b[A\n",
      "Epoch 253/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 253/300:  14%|█▎        | 3/22 [00:00<00:00, 21.27it/s]\u001b[A\n",
      "Epoch 253/300:  27%|██▋       | 6/22 [00:00<00:00, 21.22it/s]\u001b[A\n",
      "Epoch 253/300:  41%|████      | 9/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 253/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 253/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.92it/s]\u001b[A\n",
      "Epoch 253/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.97it/s]\u001b[A\n",
      "Epoch 253/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      " 84%|████████▍ | 253/300 [04:55<00:55,  1.18s/it]             \u001b[A\n",
      "Epoch 254/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 254/300:  14%|█▎        | 3/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 254/300:  27%|██▋       | 6/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 254/300:  41%|████      | 9/22 [00:00<00:00, 21.86it/s]\u001b[A\n",
      "Epoch 254/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 254/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 254/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 254/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      " 85%|████████▍ | 254/300 [04:56<00:54,  1.17s/it]             \u001b[A\n",
      "Epoch 255/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 255/300:   9%|▉         | 2/22 [00:00<00:01, 18.60it/s]\u001b[A\n",
      "Epoch 255/300:  23%|██▎       | 5/22 [00:00<00:00, 20.86it/s]\u001b[A\n",
      "Epoch 255/300:  36%|███▋      | 8/22 [00:00<00:00, 21.01it/s]\u001b[A\n",
      "Epoch 255/300:  50%|█████     | 11/22 [00:00<00:00, 21.16it/s]\u001b[A\n",
      "Epoch 255/300:  64%|██████▎   | 14/22 [00:00<00:00, 21.97it/s]\u001b[A\n",
      "Epoch 255/300:  77%|███████▋  | 17/22 [00:00<00:00, 22.02it/s]\u001b[A\n",
      "Epoch 255/300:  91%|█████████ | 20/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      " 85%|████████▌ | 255/300 [04:57<00:52,  1.18s/it]             \u001b[A\n",
      "Epoch 256/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 256/300:  14%|█▎        | 3/22 [00:00<00:00, 22.49it/s]\u001b[A\n",
      "Epoch 256/300:  27%|██▋       | 6/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 256/300:  41%|████      | 9/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 256/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 256/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 256/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 256/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      " 85%|████████▌ | 256/300 [04:58<00:51,  1.17s/it]             \u001b[A\n",
      "Epoch 257/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 257/300:  14%|█▎        | 3/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
      "Epoch 257/300:  27%|██▋       | 6/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      "Epoch 257/300:  41%|████      | 9/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 257/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 257/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.82it/s]\u001b[A\n",
      "Epoch 257/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 257/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.17it/s]\u001b[A\n",
      " 86%|████████▌ | 257/300 [04:59<00:50,  1.17s/it]             \u001b[A\n",
      "Epoch 258/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 258/300:  14%|█▎        | 3/22 [00:00<00:00, 22.69it/s]\u001b[A\n",
      "Epoch 258/300:  27%|██▋       | 6/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 258/300:  41%|████      | 9/22 [00:00<00:00, 21.25it/s]\u001b[A\n",
      "Epoch 258/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.67it/s]\n",
      "Epoch 274/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 274/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      " 91%|█████████▏| 274/300 [05:19<00:30,  1.17s/it]             \u001b[A\n",
      "Epoch 275/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 275/300:  14%|█▎        | 3/22 [00:00<00:00, 22.74it/s]\u001b[A\n",
      "Epoch 275/300:  27%|██▋       | 6/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 275/300:  41%|████      | 9/22 [00:00<00:00, 22.21it/s]\u001b[A\n",
      "Epoch 275/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.22it/s]\u001b[A\n",
      "Epoch 275/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 275/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 275/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.03it/s]\u001b[A\n",
      " 92%|█████████▏| 275/300 [05:20<00:29,  1.16s/it]             \u001b[A\n",
      "Epoch 276/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 276/300:  14%|█▎        | 3/22 [00:00<00:00, 20.77it/s]\u001b[A\n",
      "Epoch 276/300:  27%|██▋       | 6/22 [00:00<00:00, 21.61it/s]\u001b[A\n",
      "Epoch 276/300:  41%|████      | 9/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 276/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.63it/s]\u001b[A\n",
      "Epoch 276/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 276/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 276/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.29it/s]\u001b[A\n",
      " 92%|█████████▏| 276/300 [05:21<00:27,  1.17s/it]             \u001b[A\n",
      "Epoch 277/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 277/300:  14%|█▎        | 3/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 277/300:  27%|██▋       | 6/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 277/300:  41%|████      | 9/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 277/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 277/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.31it/s]\u001b[A\n",
      "Epoch 277/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 277/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      " 92%|█████████▏| 277/300 [05:23<00:26,  1.17s/it]             \u001b[A\n",
      "Epoch 278/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 278/300:  14%|█▎        | 3/22 [00:00<00:00, 20.94it/s]\u001b[A\n",
      "Epoch 278/300:  27%|██▋       | 6/22 [00:00<00:00, 21.35it/s]\u001b[A\n",
      "Epoch 278/300:  41%|████      | 9/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 278/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 278/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 278/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 278/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      " 93%|█████████▎| 278/300 [05:24<00:25,  1.17s/it]             \u001b[A\n",
      "Epoch 279/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 279/300:  14%|█▎        | 3/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 279/300:  27%|██▋       | 6/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      "Epoch 279/300:  41%|████      | 9/22 [00:00<00:00, 22.08it/s]\u001b[A\n",
      "Epoch 279/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 279/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.91it/s]\u001b[A\n",
      "Epoch 279/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "Epoch 279/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.89it/s]\u001b[A\n",
      " 93%|█████████▎| 279/300 [05:25<00:24,  1.16s/it]             \u001b[A\n",
      "Epoch 280/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 280/300:  14%|█▎        | 3/22 [00:00<00:00, 20.40it/s]\u001b[A\n",
      "Epoch 280/300:  27%|██▋       | 6/22 [00:00<00:00, 20.88it/s]\u001b[A\n",
      "Epoch 280/300:  41%|████      | 9/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      "Epoch 280/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 280/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 280/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 280/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      " 93%|█████████▎| 280/300 [05:26<00:23,  1.16s/it]             \u001b[A\n",
      "Epoch 281/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 281/300:  14%|█▎        | 3/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 281/300:  27%|██▋       | 6/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      "Epoch 281/300:  41%|████      | 9/22 [00:00<00:00, 21.00it/s]\u001b[A\n",
      "Epoch 281/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 281/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.88it/s]\u001b[A\n",
      "Epoch 281/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 281/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.37it/s]\u001b[A\n",
      " 94%|█████████▎| 281/300 [05:27<00:22,  1.17s/it]             \u001b[A\n",
      "Epoch 282/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 282/300:  14%|█▎        | 3/22 [00:00<00:00, 21.87it/s]\u001b[A\n",
      "Epoch 282/300:  27%|██▋       | 6/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 282/300:  41%|████      | 9/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 282/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      "Epoch 282/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "Epoch 282/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "Epoch 282/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.47it/s]\u001b[A\n",
      " 94%|█████████▍| 282/300 [05:28<00:21,  1.17s/it]             \u001b[A\n",
      "Epoch 283/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 283/300:  14%|█▎        | 3/22 [00:00<00:00, 21.18it/s]\u001b[A\n",
      "Epoch 283/300:  27%|██▋       | 6/22 [00:00<00:00, 20.87it/s]\u001b[A\n",
      "Epoch 283/300:  41%|████      | 9/22 [00:00<00:00, 21.70it/s]\u001b[A\n",
      "Epoch 283/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      "Epoch 283/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 283/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.56it/s]\u001b[A\n",
      "Epoch 283/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.72it/s]\u001b[A\n",
      " 94%|█████████▍| 283/300 [05:30<00:19,  1.17s/it]             \u001b[A\n",
      "Epoch 284/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 284/300:  14%|█▎        | 3/22 [00:00<00:00, 22.27it/s]\u001b[A\n",
      "Epoch 284/300:  27%|██▋       | 6/22 [00:00<00:00, 22.24it/s]\u001b[A\n",
      "Epoch 284/300:  41%|████      | 9/22 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "Epoch 284/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 284/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.40it/s]\u001b[A\n",
      "Epoch 284/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 284/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.13it/s]\u001b[A\n",
      " 95%|█████████▍| 284/300 [05:31<00:18,  1.17s/it]             \u001b[A\n",
      "Epoch 285/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 285/300:  14%|█▎        | 3/22 [00:00<00:00, 22.18it/s]\u001b[A\n",
      "Epoch 285/300:  27%|██▋       | 6/22 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "Epoch 285/300:  41%|████      | 9/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 285/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "Epoch 285/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.19it/s]\u001b[A\n",
      "Epoch 285/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      "Epoch 285/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      " 95%|█████████▌| 285/300 [05:32<00:17,  1.17s/it]             \u001b[A\n",
      "Epoch 286/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 286/300:  14%|█▎        | 3/22 [00:00<00:00, 22.21it/s]\u001b[A\n",
      "Epoch 286/300:  27%|██▋       | 6/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      "Epoch 286/300:  41%|████      | 9/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 286/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.74it/s]\u001b[A\n",
      "Epoch 286/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "Epoch 286/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.92it/s]\u001b[A\n",
      "Epoch 286/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      " 95%|█████████▌| 286/300 [05:33<00:16,  1.17s/it]             \u001b[A\n",
      "Epoch 287/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 287/300:  14%|█▎        | 3/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 287/300:  27%|██▋       | 6/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      "Epoch 287/300:  41%|████      | 9/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 287/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.14it/s]\u001b[A\n",
      "Epoch 287/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.23it/s]\u001b[A\n",
      "Epoch 287/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "Epoch 287/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      " 96%|█████████▌| 287/300 [05:34<00:15,  1.17s/it]             \u001b[A\n",
      "Epoch 288/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 288/300:  14%|█▎        | 3/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 288/300:  27%|██▋       | 6/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      "Epoch 288/300:  41%|████      | 9/22 [00:00<00:00, 20.96it/s]\u001b[A\n",
      "Epoch 288/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.11it/s]\u001b[A\n",
      "Epoch 288/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "Epoch 288/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.05it/s]\u001b[A\n",
      "Epoch 288/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.45it/s]\u001b[A\n",
      " 96%|█████████▌| 288/300 [05:35<00:13,  1.16s/it]             \u001b[A\n",
      "Epoch 289/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 289/300:  14%|█▎        | 3/22 [00:00<00:00, 22.36it/s]\u001b[A\n",
      "Epoch 289/300:  27%|██▋       | 6/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 289/300:  41%|████      | 9/22 [00:00<00:00, 21.57it/s]\u001b[A\n",
      "Epoch 289/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 289/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 289/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 289/300:  95%|█████████▌| 21/22 [00:00<00:00, 22.11it/s]\u001b[A\n",
      " 96%|█████████▋| 289/300 [05:37<00:12,  1.16s/it]             \u001b[A\n",
      "Epoch 290/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 290/300:  14%|█▎        | 3/22 [00:00<00:00, 23.15it/s]\u001b[A\n",
      "Epoch 290/300:  27%|██▋       | 6/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 290/300:  41%|████      | 9/22 [00:00<00:00, 22.22it/s]\u001b[A\n",
      "Epoch 290/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.09it/s]\u001b[A\n",
      "Epoch 290/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.04it/s]\u001b[A\n",
      "Epoch 290/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.90it/s]\u001b[A\n",
      "Epoch 290/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.60it/s]\u001b[A\n",
      " 97%|█████████▋| 290/300 [05:38<00:11,  1.16s/it]             \u001b[A\n",
      "Epoch 291/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 291/300:  14%|█▎        | 3/22 [00:00<00:00, 21.97it/s]\u001b[A\n",
      "Epoch 291/300:  27%|██▋       | 6/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 291/300:  41%|████      | 9/22 [00:00<00:00, 21.73it/s]\u001b[A\n",
      "Epoch 291/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 291/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 291/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.53it/s]\u001b[A\n",
      "Epoch 291/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.04it/s]\u001b[A\n",
      " 97%|█████████▋| 291/300 [05:39<00:10,  1.16s/it]             \u001b[A\n",
      "Epoch 292/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 292/300:  14%|█▎        | 3/22 [00:00<00:00, 22.53it/s]\u001b[A\n",
      "Epoch 292/300:  27%|██▋       | 6/22 [00:00<00:00, 21.38it/s]\u001b[A\n",
      "Epoch 292/300:  41%|████      | 9/22 [00:00<00:00, 21.59it/s]\u001b[A\n",
      "Epoch 292/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 292/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "Epoch 292/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.45it/s]\u001b[A\n",
      "Epoch 292/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      " 97%|█████████▋| 292/300 [05:40<00:09,  1.17s/it]             \u001b[A\n",
      "Epoch 293/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 293/300:  14%|█▎        | 3/22 [00:00<00:00, 22.51it/s]\u001b[A\n",
      "Epoch 293/300:  27%|██▋       | 6/22 [00:00<00:00, 22.31it/s]\u001b[A\n",
      "Epoch 293/300:  41%|████      | 9/22 [00:00<00:00, 21.71it/s]\u001b[A\n",
      "Epoch 293/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.05it/s]\u001b[A\n",
      "Epoch 293/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.91it/s]\u001b[A\n",
      "Epoch 293/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.30it/s]\u001b[A\n",
      "Epoch 293/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.39it/s]\u001b[A\n",
      " 98%|█████████▊| 293/300 [05:41<00:08,  1.17s/it]             \u001b[A\n",
      "Epoch 294/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 294/300:  14%|█▎        | 3/22 [00:00<00:00, 22.69it/s]\u001b[A\n",
      "Epoch 294/300:  27%|██▋       | 6/22 [00:00<00:00, 22.16it/s]\u001b[A\n",
      "Epoch 294/300:  41%|████      | 9/22 [00:00<00:00, 22.54it/s]\u001b[A\n",
      "Epoch 294/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.19it/s]\u001b[A\n",
      "Epoch 294/300:  68%|██████▊   | 15/22 [00:00<00:00, 22.31it/s]\u001b[A\n",
      "Epoch 294/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "Epoch 294/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.95it/s]\u001b[A\n",
      " 98%|█████████▊| 294/300 [05:42<00:06,  1.16s/it]             \u001b[A\n",
      "Epoch 295/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 295/300:  14%|█▎        | 3/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 295/300:  27%|██▋       | 6/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 295/300:  41%|████      | 9/22 [00:00<00:00, 21.58it/s]\u001b[A\n",
      "Epoch 295/300:  55%|█████▍    | 12/22 [00:00<00:00, 22.06it/s]\u001b[A\n",
      "Epoch 295/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "Epoch 295/300:  82%|████████▏ | 18/22 [00:00<00:00, 22.16it/s]\u001b[A\n",
      "Epoch 295/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.75it/s]\u001b[A\n",
      " 98%|█████████▊| 295/300 [05:44<00:05,  1.16s/it]             \u001b[A\n",
      "Epoch 296/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 296/300:  14%|█▎        | 3/22 [00:00<00:00, 22.45it/s]\u001b[A\n",
      "Epoch 296/300:  27%|██▋       | 6/22 [00:00<00:00, 22.28it/s]\u001b[A\n",
      "Epoch 296/300:  41%|████      | 9/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      "Epoch 296/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 296/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "Epoch 296/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.41it/s]\u001b[A\n",
      "Epoch 296/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.54it/s]\u001b[A\n",
      " 99%|█████████▊| 296/300 [05:45<00:04,  1.16s/it]             \u001b[A\n",
      "Epoch 297/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 297/300:  14%|█▎        | 3/22 [00:00<00:00, 20.98it/s]\u001b[A\n",
      "Epoch 297/300:  27%|██▋       | 6/22 [00:00<00:00, 21.02it/s]\u001b[A\n",
      "Epoch 297/300:  41%|████      | 9/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 297/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.21it/s]\u001b[A\n",
      "Epoch 297/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 297/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 297/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.93it/s]\u001b[A\n",
      " 99%|█████████▉| 297/300 [05:46<00:03,  1.16s/it]             \u001b[A\n",
      "Epoch 298/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 298/300:  14%|█▎        | 3/22 [00:00<00:00, 20.78it/s]\u001b[A\n",
      "Epoch 298/300:  27%|██▋       | 6/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 298/300:  41%|████      | 9/22 [00:00<00:00, 21.44it/s]\u001b[A\n",
      "Epoch 298/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.68it/s]\u001b[A\n",
      "Epoch 298/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.36it/s]\u001b[A\n",
      "Epoch 298/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "Epoch 298/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      " 99%|█████████▉| 298/300 [05:47<00:02,  1.17s/it]             \u001b[A\n",
      "Epoch 299/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 299/300:  14%|█▎        | 3/22 [00:00<00:00, 22.31it/s]\u001b[A\n",
      "Epoch 299/300:  27%|██▋       | 6/22 [00:00<00:00, 22.14it/s]\u001b[A\n",
      "Epoch 299/300:  41%|████      | 9/22 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "Epoch 299/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.99it/s]\u001b[A\n",
      "Epoch 299/300:  68%|██████▊   | 15/22 [00:00<00:00, 21.50it/s]\u001b[A\n",
      "Epoch 299/300:  82%|████████▏ | 18/22 [00:00<00:00, 21.43it/s]\u001b[A\n",
      "Epoch 299/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.69it/s]\u001b[A\n",
      "100%|█████████▉| 299/300 [05:48<00:01,  1.16s/it]             \u001b[A\n",
      "Epoch 300/300:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 300/300:  14%|█▎        | 3/22 [00:00<00:00, 21.07it/s]\u001b[A\n",
      "Epoch 300/300:  27%|██▋       | 6/22 [00:00<00:00, 21.83it/s]\u001b[A\n",
      "Epoch 300/300:  41%|████      | 9/22 [00:00<00:00, 21.33it/s]\u001b[A\n",
      "Epoch 300/300:  55%|█████▍    | 12/22 [00:00<00:00, 21.15it/s]\u001b[A\n",
      "Epoch 300/300:  68%|██████▊   | 15/22 [00:00<00:00, 20.99it/s]\u001b[A\n",
      "Epoch 300/300:  82%|████████▏ | 18/22 [00:00<00:00, 20.75it/s]\u001b[A\n",
      "Epoch 300/300:  95%|█████████▌| 21/22 [00:00<00:00, 21.26it/s]\u001b[A\n",
      "100%|██████████| 300/300 [05:49<00:00,  1.17s/it]             \u001b[A\n",
      "[I 2024-08-21 23:31:12,773] Trial 20 finished with value: -10.179813385009766 and parameters: {'hidden_channels': 64, 'num_layers': 3, 'dropout_rate': 0.06298115943763027, 'lr': 0.0015566246671648023, 'weight_decay': 8.457710325001924e-08, 'optimizer': 'Adam', 'activation': 'ReLU', 'skip_connection': True, 'grad_clip': 1.0060411194447851, 'self_loops': True, 'k_val': 8, 'use_augmentation': False}. Best is trial 11 with value: -11.003248405456542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 20 Results ---\n",
      "Hyperparameters:\n",
      "  hidden_channels: 64\n",
      "  num_layers: 3\n",
      "  dropout_rate: 0.06298115943763027\n",
      "  lr: 0.0015566246671648023\n",
      "  weight_decay: 8.457710325001924e-08\n",
      "  optimizer: Adam\n",
      "  activation: ReLU\n",
      "  skip_connection: True\n",
      "  grad_clip: 1.0060411194447851\n",
      "  self_loops: True\n",
      "  k_val: 8\n",
      "  use_augmentation: False\n",
      "\n",
      "Current Model Architecture:\n",
      "HeteroMetaLayerGNN(\n",
      "  (activation): ReLU()\n",
      "  (node_encoders): ModuleDict(\n",
      "    (A): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (B): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (C): Linear(in_features=6, out_features=64, bias=True)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x MetaLayer(\n",
      "      edge_model=EdgeModel(\n",
      "      (edge_mlp): Sequential(\n",
      "        (0): Linear(in_features=129, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    ),\n",
      "      node_model=NodeModel(\n",
      "      (node_mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    ),\n",
      "      global_model=GlobalModel(\n",
      "      (global_mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.06298115943763027, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Best Validation Loss for this trial: -1.01798e+01\n",
      "Best Overall Validation Loss: -1.10032e+01 (Trial 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading graphs:  12%|█▏        | 123/1000 [00:21<01:34,  9.27it/s]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set up the study\n",
    "study_name = 'heterotrain'\n",
    "n_trials = 100  # Total number of trials to run (including previous ones if resuming)\n",
    "n_jobs = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the directory for saving files\n",
    "save_dir = '/scratch/gpfs/hk4638/astrid_optimization/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Set up the storage\n",
    "storage_name = os.path.join(save_dir, 'new_astrid_study.db')\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    url=f\"sqlite:///{storage_name}\",\n",
    "    engine_kwargs={\"connect_args\": {\"timeout\": 100}}\n",
    ")\n",
    "\n",
    "# Check if the study already exists\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "    print(f\"Resuming optimization from existing study '{study_name}'\")\n",
    "    print(f\"Number of completed trials: {len(study.trials)}\")\n",
    "except KeyError:\n",
    "    # If the study doesn't exist, create a new one\n",
    "    study = optuna.create_study(study_name=study_name, storage=storage, \n",
    "                                sampler=optuna.samplers.TPESampler(n_startup_trials=10),\n",
    "                                direction='minimize')\n",
    "    print(f\"Created new study '{study_name}'\")\n",
    "\n",
    "# Assuming 'normalized_data_list' and necessary datasets are already loaded and prepared\n",
    "in_channels_dict = {\n",
    "    'A': normalized_data_list[0]['A'].num_features,\n",
    "    'B': normalized_data_list[0]['B'].num_features,\n",
    "    'C': normalized_data_list[0]['C'].num_features\n",
    "}\n",
    "\n",
    "# Create the objective\n",
    "objective = Objective(in_channels_dict=in_channels_dict, epochs=300, device=device, directory='/scratch/gpfs/hk4638/FinalData/NewData')\n",
    "\n",
    "# Calculate the number of trials to run\n",
    "n_trials_to_run = max(0, n_trials - len(study.trials))\n",
    "\n",
    "if n_trials_to_run > 0:\n",
    "    print(f\"Running {n_trials_to_run} additional trials...\")\n",
    "    study.optimize(objective, n_trials=n_trials_to_run, n_jobs=n_jobs)\n",
    "else:\n",
    "    print(\"No additional trials to run. The study has already completed the specified number of trials.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target trial number 8 is greater than or equal to the highest trial number (-1). No reset needed.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "def reset_study_to_trial(study_name, storage_path, target_trial_number):\n",
    "    if not os.path.exists(storage_path):\n",
    "        raise FileNotFoundError(f\"Database file not found: {storage_path}\")\n",
    "\n",
    "    storage_url = f\"sqlite:///{storage_path}\"\n",
    "    \n",
    "    # Load the existing study\n",
    "    old_study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "    \n",
    "    # Get the total number of trials\n",
    "    total_trials = len(old_study.trials)\n",
    "    \n",
    "    if target_trial_number >= total_trials - 1:\n",
    "        print(f\"Target trial number {target_trial_number} is greater than or equal to the highest trial number ({total_trials - 1}). No reset needed.\")\n",
    "        return old_study\n",
    "    \n",
    "    # Confirm with the user\n",
    "    print(f\"This will reset the study to include only trials up to and including trial {target_trial_number}.\")\n",
    "    print(f\"Current number of trials: {total_trials}\")\n",
    "    print(f\"Number of trials to keep: {target_trial_number + 1}\")\n",
    "    print(f\"Number of trials to remove: {total_trials - (target_trial_number + 1)}\")\n",
    "    confirm = input(\"Are you sure you want to proceed? (yes/no): \")\n",
    "    \n",
    "    if confirm.lower() != 'yes':\n",
    "        print(\"Operation cancelled.\")\n",
    "        return old_study\n",
    "    \n",
    "    # Create a new study with the same name (this will overwrite the old one)\n",
    "    new_study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_url,\n",
    "        direction=old_study.direction,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # Clear all trials from the new study\n",
    "    new_study._storage.delete_study(new_study._study_id)\n",
    "    \n",
    "    # Recreate the study\n",
    "    new_study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_url,\n",
    "        direction=old_study.direction,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # Copy trials up to and including the target trial number\n",
    "    for trial in old_study.trials[:target_trial_number + 1]:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            new_study.add_trial(\n",
    "                optuna.trial.create_trial(\n",
    "                    params=trial.params,\n",
    "                    distributions=trial.distributions,\n",
    "                    value=trial.value,\n",
    "                    state=trial.state\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    print(f\"Study reset to include {len(new_study.trials)} trials\")\n",
    "    return new_study\n",
    "\n",
    "# Usage\n",
    "storage_path = \"/scratch/gpfs/hk4638/astrid_optimization/astrid_study.db\"\n",
    "study_name = \"astrid_gnn_optimization\"\n",
    "target_trial_number = 8\n",
    "\n",
    "try:\n",
    "    study = reset_study_to_trial(study_name, storage_path, target_trial_number)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please check the path to your database file and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise  # This will print the full traceback\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "def reset_optuna_database(study_name, storage_path):\n",
    "    \"\"\"\n",
    "    Reset the Optuna database by deleting the existing file and creating a new study.\n",
    "    \n",
    "    Args:\n",
    "    study_name (str): The name of the study.\n",
    "    storage_path (str): The path to the SQLite database file.\n",
    "    \n",
    "    Returns:\n",
    "    optuna.Study: A new Optuna study object.\n",
    "    \"\"\"\n",
    "    # Check if the database file exists\n",
    "    if os.path.exists(storage_path):\n",
    "        # Confirm with the user before deleting\n",
    "        confirm = input(f\"Are you sure you want to delete the existing study '{study_name}'? (y/n): \")\n",
    "        if confirm.lower() == 'y':\n",
    "            # Delete the existing database file\n",
    "            os.remove(storage_path)\n",
    "            print(f\"Deleted existing database: {storage_path}\")\n",
    "        else:\n",
    "            print(\"Database reset cancelled.\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Created new study '{study_name}' at {storage_path}\")\n",
    "    return study\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you sure you want to delete the existing study 'astrid_gnn_optimization'? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing database: /scratch/gpfs/hk4638/astrid_optimization/astrid_study.db\n",
      "Created new study 'astrid_gnn_optimization' at /scratch/gpfs/hk4638/astrid_optimization/astrid_study.db\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Set up the study\n",
    "study_name = 'astrid_gnn_optimization'\n",
    "storage_path = '/scratch/gpfs/hk4638/astrid_optimization/astrid_study.db'\n",
    "\n",
    "# Reset the database if desired\n",
    "study = reset_optuna_database(study_name, storage_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0jimGzhBHUd"
   },
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set the hyperparameters\n",
    "hidden_channels = 64  # From the JSON object\n",
    "num_layers = 2  # From the JSON object\n",
    "dropout_rate = 0.14086170125786326  # From the JSON object\n",
    "lr = 0.0030537368445218895  # From the JSON object\n",
    "weight_decay = 0.0037291676576673635  # From the JSON object\n",
    "batch_size = 32  # From the JSON object\n",
    "optimizer_name = \"AdamW\"  # From the JSON object\n",
    "activation = \"ReLU\"  # From the JSON object\n",
    "skip_connection = True  # From the JSON object\n",
    "grad_clip = 3.009306227986649  # From the JSON object\n",
    "self_loops = False  # From the JSON object\n",
    "k_val = 10  # From the JSON object\n",
    "use_augmentation = False  # From the JSON object\n",
    "\n",
    "# Initialize model with the new hyperparameters\n",
    "in_channels_dict = {\n",
    "    'A': normalized_data_list[0]['A'].num_features,\n",
    "    'B': normalized_data_list[0]['B'].num_features,\n",
    "    'C': normalized_data_list[0]['C'].num_features\n",
    "}\n",
    "out_channels = 2  # Assuming single output for omega_m\n",
    "\n",
    "model = HeteroMetaLayerGNN(\n",
    "    in_channels_dict=in_channels_dict,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    "    dropout_rate=dropout_rate,\n",
    "    activation=activation,\n",
    "    skip_connection=skip_connection,\n",
    "    self_loops=self_loops\n",
    ")\n",
    "\n",
    "# Load model state\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = '/scratch/gpfs/hk4638/astrid_optimization/model_88.pth'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Move model to the appropriate device\n",
    "trained_model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    test_means = []\n",
    "    test_stds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            mean_out, std_out = out[:, 0], out[:, 1]  # Split mean and std\n",
    "            test_means.append(mean_out)\n",
    "            test_stds.append(F.softplus(std_out))  # Apply softplus to ensure std is positive\n",
    "            test_true.append(data.y.to(device))\n",
    "    \n",
    "    test_means = torch.cat(test_means, dim=0)\n",
    "    test_stds = torch.cat(test_stds, dim=0)\n",
    "    test_true = torch.cat(test_true, dim=0)\n",
    "    \n",
    "    # Move to CPU for metric computation\n",
    "    test_means_cpu = test_means.cpu().numpy()\n",
    "    test_stds_cpu = test_stds.cpu().numpy()\n",
    "    test_true_cpu = test_true.cpu().numpy()\n",
    "    \n",
    "    # Calculate R² using sklearn (as a double-check)\n",
    "    r2_sklearn = r2_score(test_true_cpu, test_means_cpu)\n",
    "    \n",
    "    # Calculate R² using the provided formula\n",
    "    y_true_mean = np.mean(test_true_cpu)\n",
    "    numerator = np.sum((test_true_cpu - test_means_cpu)**2)\n",
    "    denominator = np.sum((test_true_cpu - y_true_mean)**2)\n",
    "    r2 = 1 - (numerator / denominator)\n",
    "    test_mse = mean_squared_error(test_true_cpu, test_means_cpu)\n",
    "    \n",
    "    # Calculate mean relative error (ε)\n",
    "    epsilon_small = 1e-8\n",
    "    epsilon = np.mean(np.abs(test_true_cpu - test_means_cpu) / (test_true_cpu + epsilon_small))\n",
    "    \n",
    "    # Calculate Chi-square (χ²)\n",
    "    chi_square = np.mean(((test_true_cpu - test_means_cpu)**2) / (test_stds_cpu**2))\n",
    "    return test_means_cpu, test_stds_cpu, test_true_cpu, r2, epsilon, test_mse, chi_square\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PJGIkgQzA-K8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.002166\n",
      "0.8216975629329681\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate model\n",
    "test_means, test_stds, test_true, r2, epsilon, test_mse, chi_square = evaluate_model(trained_model, test_loader, device)\n",
    "\n",
    "print(f'Test MSE: {test_mse:.6f}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(test_true, test_means, yerr=test_stds, fmt='o', alpha=0.5, ecolor='gray', capsize=2, label='Predictions with Uncertainty')\n",
    "plt.plot([0, 0.5], [0, 0.5], color='red', linestyle='--', label='Perfect Prediction Line')  # Perfect prediction line\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel(r'Predicted $\\Omega_m$')\n",
    "plt.title('Training in ASTRID, Testing in ASTRID')\n",
    "\n",
    "# Add textbox with metrics\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\Omega_m$',\n",
    "    r'$R^2=%.2f$' % (r2,),\n",
    "    r'$\\epsilon=%.1f\\%%$' % (epsilon * 100,),\n",
    "    r'$\\chi^2=%.4f$' % (chi_square,)\n",
    "))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=9,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.text(0.75, 0.05, r'[0.3, 0.35, 0.35]', transform=plt.gca().transAxes,\n",
    "         fontsize=9, verticalalignment='bottom', bbox=props)\n",
    "\n",
    "plt.xlim(0.1, 0.5)\n",
    "plt.ylim(0, 0.6)\n",
    "plt.grid(True)\n",
    "#plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "com [~/.conda/envs/com/]",
   "language": "python",
   "name": "conda_com"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

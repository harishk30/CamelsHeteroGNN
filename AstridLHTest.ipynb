{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishk30/CamelsHetroGNN/blob/main/AstridLHTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Loading Data"
      ],
      "metadata": {
        "id": "Ep2eK29S_5qq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90wz8D6UuSs9",
        "outputId": "97b44c44-305c-4409-c4dc-08221048c041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlLkEWbmiyYj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5y4nEnfSfbv",
        "outputId": "6cb6aacd-ab6c-4162-8b06-991741715bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1stgVndkEQm"
      },
      "outputs": [],
      "source": [
        "catalog = '/content/drive/MyDrive/groups_090.hdf5'\n",
        "f = h5py.File(catalog, 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THXTiqZQmCO0"
      },
      "outputs": [],
      "source": [
        "M_star = f['Subhalo/SubhaloMassType'][:,4]*1e10\n",
        "pos  = f['Subhalo/SubhaloPos'][:]/1e3\n",
        "vel = f['Subhalo/SubhaloVel'][:]\n",
        "met = f['Subhalo/SubhaloStarMetallicity'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgJ7wxu6sLUM"
      },
      "outputs": [],
      "source": [
        "def load_and_filter_data(file, mass_threshold=2e8):\n",
        "    with h5py.File(file, 'r') as f:\n",
        "        positions = f['Subhalo/SubhaloPos'][:]/1e3  # Convert to Mpc/h\n",
        "        vel = f['Subhalo/SubhaloVel'][:]\n",
        "        metallicities = f['Subhalo/SubhaloStarMetallicity'][:]\n",
        "        masses = f['Subhalo/SubhaloMassType'][:,4]*1e10  # Stellar mass\n",
        "        omega_m = f['Header'].attrs['Omega0']\n",
        "\n",
        "    # Filter galaxies based on the stellar mass threshold\n",
        "    mask = masses > mass_threshold\n",
        "    positions = positions[mask]\n",
        "    vel = vel[mask]\n",
        "    metallicities = metallicities[mask]\n",
        "    masses = masses[mask]\n",
        "\n",
        "    return positions, vel, metallicities, masses, omega_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--oaAn3kDLtU"
      },
      "outputs": [],
      "source": [
        "def apply_periodic_boundary_conditions(positions, box_size):\n",
        "    # Wrap positions to the box size\n",
        "    positions = positions % box_size\n",
        "    return positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7VnGquYDPRe"
      },
      "outputs": [],
      "source": [
        "def minimum_image_distance(pos1, pos2, box_size):\n",
        "    # Calculate the minimum image distance between two points\n",
        "    delta = np.abs(pos1 - pos2)\n",
        "    delta = np.where(delta > 0.5 * box_size, box_size - delta, delta)\n",
        "    return np.sqrt((delta ** 2).sum(axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKOyScsvstrg"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import KDTree\n",
        "\n",
        "def distance(point1, point2):\n",
        "    return np.linalg.norm(point1 - point2)\n",
        "\n",
        "def create_edges_knn(points, k=6):\n",
        "    edges = []\n",
        "    edge_value = []\n",
        "\n",
        "    # Create a KDTree for efficient nearest neighbor search\n",
        "    point_tree = KDTree(points)\n",
        "\n",
        "    for i in range(len(points)):\n",
        "        # Query the k nearest neighbors for each point\n",
        "        _, neighbors = point_tree.query(points[i], k=k+1)\n",
        "\n",
        "        for j in neighbors[1:]:  # Skip the first neighbor because it's the point itself\n",
        "            # Add an edge between the point and its neighbor\n",
        "            edges.append([i, j])\n",
        "\n",
        "            # Compute the distance between the points as the edge value\n",
        "            edge_value.append(distance(points[i], points[j]))\n",
        "\n",
        "    return [edges, edge_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztRfbIqROn7N"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def min_distance(positions, box_size = 25):\n",
        "    min_distance = np.inf\n",
        "    max_distance = 0\n",
        "\n",
        "    # Iterate over all pairs of galaxies\n",
        "    for i in tqdm(range(len(positions))):\n",
        "        for j in range(i + 1, len(positions)):\n",
        "            dist = minimum_image_distance(positions[i], positions[j], box_size)\n",
        "            if dist < min_distance:\n",
        "                min_distance = dist\n",
        "            if dist > max_distance:\n",
        "                max_distance = dist\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Minimum distance: {min_distance} Mpc/h\")\n",
        "    print(f\"Maximum distance: {max_distance} Mpc/h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l40oy2_KRiWJ"
      },
      "outputs": [],
      "source": [
        "def minimum_image_distance_vectorized(positions, box_size = 25):\n",
        "    num_galaxies = positions.shape[0]\n",
        "\n",
        "    # Compute pairwise differences in each dimension\n",
        "    diff = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]\n",
        "\n",
        "    # Apply periodic boundary conditions\n",
        "    diff = np.abs(diff)\n",
        "    diff = np.where(diff > 0.5 * box_size, box_size - diff, diff)\n",
        "\n",
        "    # Compute the Euclidean distance\n",
        "    dist = np.sqrt(np.sum(diff ** 2, axis=-1))\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPQB3MDzDcid"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from scipy.spatial import cKDTree\n",
        "def create_edges_knn_pbc(points, box_size = 25, k=6):\n",
        "    tree = KDTree(points, boxsize=box_size)\n",
        "\n",
        "    edges = []\n",
        "    edge_values = []\n",
        "    '''\n",
        "    distances = minimum_image_distance_vectorized(points, box_size)\n",
        "    # Mask the diagonal (self-distances which are zero)\n",
        "    np.fill_diagonal(distances, np.inf)\n",
        "    # Get the minimum and maximum distances\n",
        "    min_distance = np.min(distances)\n",
        "    max_distance = np.max(np.triu(distances, k=1))\n",
        "    print(min_distance, max_distance)\n",
        "    '''\n",
        "\n",
        "    min_distance = np.inf\n",
        "    max_distance = 0\n",
        "    large_distance_count = 0\n",
        "\n",
        "    for i in tqdm(range(len(points)), desc=\"Processing points\"):\n",
        "        distances, neighbors = tree.query(points[i], k=k+1)\n",
        "        for j, tree_dist in zip(neighbors[1:], distances[1:]):\n",
        "            if j != i and j < len(points):\n",
        "                actual_distance = minimum_image_distance(points[i], points[j], box_size)\n",
        "                edges.append([i, j])\n",
        "                edge_values.append(actual_distance)\n",
        "                min_distance = min(min_distance, actual_distance)\n",
        "                max_distance = max(max_distance, actual_distance)\n",
        "    print(min_distance, max_distance)\n",
        "    return np.array(edges), np.array(edge_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5XiGu3wtkRA"
      },
      "outputs": [],
      "source": [
        "def create_points(positions, masses, vel, met):\n",
        "    point_features = []\n",
        "    for i, pos in enumerate(positions):\n",
        "        #point_features.append(list(pos) + list(vel[i]) + [masses[i]] + [met[i]])\n",
        "        point_features.append(list(pos) + [masses[i]] + [met[i]])\n",
        "    return point_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CE9AtwhuAKs"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "def create_graph(file_path, k_val=6):\n",
        "    positions, velocity, metallicities, masses, omega_m = load_and_filter_data(file_path)\n",
        "    edges, edge_values = create_edges_knn_pbc(positions, 25, k_val)\n",
        "    point_values = create_points(positions, masses, velocity, metallicities)\n",
        "\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    point_values = torch.tensor(point_values, dtype=torch.float)\n",
        "    edge_value = torch.tensor(edge_values, dtype=torch.float)\n",
        "\n",
        "    return [point_values, edge_index, edge_value, omega_m]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhxwN1mIub7M"
      },
      "outputs": [],
      "source": [
        "def turn_data(graph):\n",
        "    graph_data = Data(x=graph[0], edge_index=graph[1], edge_attr=graph[2], y = graph[3])\n",
        "    return graph_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(file_path, k_val=6):\n",
        "    graph = create_graph(file_path, k_val)\n",
        "    return turn_data(graph)"
      ],
      "metadata": {
        "id": "_PXmi7cFLFqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLywIItfwd0D",
        "outputId": "58a863f9-0dbc-4054-9dd5-ffba2a05ed59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing points: 100%|██████████| 709/709 [00:00<00:00, 3142.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.016440034 7.6689534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[709, 5], edge_index=[2, 4254], edge_attr=[4254], y=0.3862)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "create_data('/content/drive/MyDrive/groups_090.hdf5', 6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_normalization_params(data_list):\n",
        "    # Concatenate all node features and edge attributes\n",
        "    all_x = torch.cat([data.x for data in data_list], dim=0)\n",
        "    all_edge_attr = torch.cat([data.edge_attr for data in data_list], dim=0)\n",
        "\n",
        "    # Calculate mean and std for node features and edge attributes\n",
        "    x_mean, x_std = all_x.mean(dim=0), all_x.std(dim=0)\n",
        "    edge_attr_mean, edge_attr_std = all_edge_attr.mean(dim=0), all_edge_attr.std(dim=0)\n",
        "\n",
        "    return (x_mean, x_std), (edge_attr_mean, edge_attr_std)\n",
        "\n",
        "def normalize_dataset(data_list, x_params, edge_attr_params):\n",
        "    x_mean, x_std = x_params\n",
        "    edge_attr_mean, edge_attr_std = edge_attr_params\n",
        "\n",
        "    normalized_data_list = []\n",
        "    for data in data_list:\n",
        "        normalized_x = (data.x - x_mean) / (x_std + 1e-8)\n",
        "        normalized_edge_attr = (data.edge_attr - edge_attr_mean) / (edge_attr_std + 1e-8)\n",
        "\n",
        "        # Create a new Data object with normalized features and original y value\n",
        "        normalized_data = Data(x=normalized_x,\n",
        "                               edge_index=data.edge_index,\n",
        "                               edge_attr=normalized_edge_attr,\n",
        "                               y=data.y)  # Preserve the original y value\n",
        "\n",
        "        normalized_data_list.append(normalized_data)\n",
        "\n",
        "    return normalized_data_list"
      ],
      "metadata": {
        "id": "V9zLYGSy_cz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.data import DataLoader\n",
        "directory = ''\n",
        "def load_all_graphs(directory, k_val=6, box_size=25):\n",
        "    file_list = os.listdir(directory)\n",
        "    data_list = []\n",
        "    for file_name in tqdm(file_list, desc=\"Loading HDF5 files\"):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        graph_data = create_data(file_path, k_val)\n",
        "        data_list.append(graph_data)\n",
        "    return data_list\n",
        "\n",
        "# Load all graphs\n",
        "data_list = load_all_graphs(directory)\n",
        "\n",
        "# Calculate normalization parameters based on all graphs\n",
        "x_params, edge_attr_params = calculate_normalization_params(data_list)\n",
        "\n",
        "# Normalize the dataset using the calculated parameters\n",
        "normalized_data_list = normalize_dataset(data_list, x_params, edge_attr_params)\n",
        "\n",
        "# Calculate the lengths for the 70-15-15 split\n",
        "total_len = len(normalized_data_list)\n",
        "train_len = int(0.7 * total_len)\n",
        "val_len = int(0.15 * total_len)\n",
        "test_len = total_len - train_len - val_len  # Ensures all data is used\n",
        "\n",
        "# Perform the split\n",
        "train_data, val_data, test_data = random_split(normalized_data_list, [train_len, val_len, test_len])\n",
        "\n",
        "# Create DataLoaders for each split\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "qoAJQ_raMZ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "eD3eA9nTAAch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "class ComplexGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers=4, heads=4, dropout_rate=0.1):\n",
        "        super(ComplexGAT, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            GATConv(hidden_channels * heads, hidden_channels, heads=heads, concat=True, edge_dim=1)\n",
        "            for _ in range(num_layers - 2)\n",
        "        ])\n",
        "\n",
        "        self.conv_last = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, edge_dim=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.fc2 = nn.Linear(hidden_channels, 1)  # Output a single value for omega_m\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_attr))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        for conv in self.convs:\n",
        "            x = F.relu(conv(x, edge_index, edge_attr=edge_attr))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = self.conv_last(x, edge_index, edge_attr=edge_attr)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.squeeze()"
      ],
      "metadata": {
        "id": "mksges42ACLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=100, lr=0.001):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_true = []\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                out = model(data)\n",
        "                val_loss += criterion(out, data.y).item()\n",
        "                val_predictions.extend(out.cpu().numpy())\n",
        "                val_true.extend(data.y.cpu().numpy())\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_mse = mean_squared_error(val_true, val_predictions)\n",
        "        val_r2 = r2_score(val_true, val_predictions)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}, Val R2: {val_r2:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_true = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            test_predictions.extend(out.cpu().numpy())\n",
        "            test_true.extend(data.y.cpu().numpy())\n",
        "\n",
        "    test_mse = mean_squared_error(test_true, test_predictions)\n",
        "    test_r2 = r2_score(test_true, test_predictions)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}, Test R2: {test_r2:.4f}')\n",
        "    return test_predictions, test_true"
      ],
      "metadata": {
        "id": "_5UGNXhNAggG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize model\n",
        "in_channels = normalized_dataset[0].num_node_features\n",
        "hidden_channels = 64\n",
        "num_layers = 4\n",
        "heads = 4\n",
        "dropout_rate = 0.1\n",
        "\n",
        "model = ComplexGAT(in_channels, hidden_channels, num_layers, heads, dropout_rate)\n",
        "\n",
        "# Train model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=100, lr=0.001)"
      ],
      "metadata": {
        "id": "5mxL3nt4Avrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Model"
      ],
      "metadata": {
        "id": "R0jimGzhBHUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_predictions, test_true = evaluate_model(trained_model, test_loader, device)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(test_true, test_predictions, alpha=0.5)\n",
        "plt.plot([min(test_true), max(test_true)], [min(test_true), max(test_true)], 'r--', lw=2)\n",
        "plt.xlabel('True $\\Omega_{m}$')\n",
        "plt.ylabel('Predicted $\\Omega_{m}$')\n",
        "plt.title('True vs Predicted $\\Omega_{m}$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PJGIkgQzA-K8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}